{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"7e3a38735e86e8a6ac4d79df2bcce665827b172c","modified":1651740571987},{"_id":"source/categories/index.md","hash":"6623c5214c3774f6e0ce8494a72fce8ec851c4f1","modified":1651822901552},{"_id":"source/_posts/baidu-uie-sota.md","hash":"7b0e90e38469aa97a0efadc893a57d688248a0a8","modified":1654591294925},{"_id":"source/_posts/bert-3w.md","hash":"a0afe191421a5c47b4bb784dbd9d2379b49fd5eb","modified":1654585587506},{"_id":"source/_posts/begin-to-learn-nlp.md","hash":"531fe1001e4de85ac08cdb5893c3a46f9f07bc91","modified":1654678022364},{"_id":"source/_posts/contrastive-representation-learning.md","hash":"c651c70c320b3a681f75cde7f1ab57f5197ff6df","modified":1654595012670},{"_id":"source/_posts/cross-lingual-PLM.md","hash":"a21240bcb194cecd62db59d5471c013516fcecea","modified":1654585925510},{"_id":"source/_posts/galaxy-task-oriented-dialog.md","hash":"0ddc831bece7859e29be1c799de513e36c489123","modified":1654585951362},{"_id":"source/_posts/pretrain-model-yue-01.md","hash":"c72f31eaed5c1587e076d5099acb269da4bcbc8d","modified":1654671647496},{"_id":"source/_posts/pretrained-model.md","hash":"f79fc90c6fdd92bc266976c9edbff3a16409dbc7","modified":1655125115301},{"_id":"source/_posts/roformer.md","hash":"381a29a66c6f46759c766ab10961e188273b8696","modified":1656063499291},{"_id":"source/_posts/time-mention.md","hash":"5f0ba86dbce49f2fd191881ea57e63d5903b685b","modified":1654778491190},{"_id":"source/_posts/sentence-transformers.md","hash":"44113b325621bc77ead7bba3808ee203b585b0bc","modified":1654585629539},{"_id":"source/_posts/word2vec.md","hash":"93fbc82bd1095cb8d67f8d391ce39c067bb39bca","modified":1654778368711},{"_id":"source/guestbook/index.md","hash":"e0c2ee470c6b99beb36087ab42f229d952332cb2","modified":1651822690951},{"_id":"source/tags/index.md","hash":"56b4923115b1703ca3a0f1b03091fd170e387728","modified":1651822931098},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852606},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852516},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852516},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852584},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852584},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852585},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852602},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852604},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1651739852462},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1651739852463},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1651739852464},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1651739852471},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1651739852472},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1651739852477},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1651739852474},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1651739852476},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1651739852473},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1651739852477},{"_id":"themes/next/README.cn.md","hash":"b878b73f3fcdef47849453c94420871903d487b3","modified":1651739852478},{"_id":"themes/next/README.md","hash":"efcdc4b0ca791c3fc64afa28c8721e137f2d11ea","modified":1651739852479},{"_id":"themes/next/_config.yml","hash":"5ff16272344e55fe42f7c94fd75feee794b26301","modified":1652260099741},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1651739852482},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1651739852481},{"_id":"themes/next/package.json","hash":"3963ad558a24c78a3fd4ef23cf5f73f421854627","modified":1651739852540},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1651739852465},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"a0a82dbfabdef9a9d7c17a08ceebfb4052d98d81","modified":1651739852466},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1651739852467},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1651739852469},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1651739852482},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1651739852483},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1651739852484},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1651739852484},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1651739852485},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1651739852488},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1651739852488},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1651739852489},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1651739852490},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1651739852490},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1651739852491},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1651739852491},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1651739852492},{"_id":"themes/next/languages/zh-Hans.yml","hash":"1aa53ff7d04619114667381da6ce71dfcdcd9683","modified":1651752907739},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1651739852493},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1651739852493},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1651739852498},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1651739852537},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1651739852537},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1651739852538},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1651739852538},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1651739852538},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1651739852539},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1651739852540},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1651739852541},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1651739852718},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1651739852719},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1651739852720},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1651739852496},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1651739852497},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1651739852500},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1651739852502},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1651739852503},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1651739852539},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1651739852504},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9c7343fd470e0943ebd75f227a083a980816290b","modified":1651739852505},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1651739852505},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1651739852506},{"_id":"themes/next/layout/_partials/footer.swig","hash":"26e93336dc57a39590ba8dc80564a1d2ad5ff93b","modified":1651739852506},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1651739852507},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1651739852509},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1651739852508},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"053bf2b23c6c3fd9cc4a21445ca9d90322f9a05a","modified":1652778798790},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1651739852510},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1651739852514},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1651739852515},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1651739852517},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1651739852530},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1651739852530},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1651739852531},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1651739852531},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1651739852532},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1651739852532},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1651739852532},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1651739852542},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1651739852542},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1651739852542},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1651739852543},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1651739852544},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1651739852545},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1651739852545},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1651739852545},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1651739852605},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1651739852546},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1651739852609},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1651739852610},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1651739852610},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1651739852611},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1651739852612},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1651739852612},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1651739852612},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1651739852613},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1651739852613},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1651739852614},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1651739852614},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1651739852615},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1651739852615},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1651739852615},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1651739852616},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1651739852616},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1651739852617},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1651739852616},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1651739852508},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1651739852507},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1651739852510},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1651739852511},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1651739852511},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1651739852512},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1651739852513},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1651739852513},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1651739852514},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1651739852515},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1651739852516},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1651739852517},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1651739852519},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1651739852519},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1651739852520},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1651739852520},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1651739852522},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1651739852521},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1651739852523},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1651739852524},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1651739852524},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1651739852524},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1651739852525},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1651739852523},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1651739852525},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1651739852526},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1651739852526},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1651739852527},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1651739852528},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1651739852528},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1651739852528},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1651739852529},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1651739852529},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1651739852530},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1651739852535},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1651739852534},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1651739852536},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1651739852536},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1651739852583},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1651739852583},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1651739852584},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1651739852585},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1651739852601},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1651739852601},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1651739852602},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1651739852603},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1651739852618},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1651739852619},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1651739852618},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1651739852620},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1651739852624},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1651739852620},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1651739852626},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1651739852627},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1651739852629},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1651739852630},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1651739852629},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1651739852645},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1651739852640},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1651739852646},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1651739852646},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1651739852647},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1651739852658},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1651739852659},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1651739852661},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1651739852662},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1651739852664},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1651739852664},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1651739852665},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1651739852665},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1651739852665},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1651739852685},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1651739852687},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1651739852688},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1651739852689},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1651739852691},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1651739852692},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1651739852693},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1651739852694},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1651739852695},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1651739852695},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1651739852696},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1651739852697},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1651739852696},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1651739852697},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1651739852698},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1651739852698},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1651739852698},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1651739852699},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1651739852699},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1651739852700},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1651739852700},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1651739852701},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1651739852701},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1651739852702},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1651739852703},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1651739852705},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1651739852704},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1651739852713},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1651739852713},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1651739852716},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1651739852717},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1651739852533},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1651739852718},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1651739852533},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1651739852547},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1651739852547},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1651739852548},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1651739852548},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1651739852549},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1651739852556},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1651739852568},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1651739852580},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1651739852581},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1651739852580},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1651739852582},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1651739852582},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1651739852582},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1651739852586},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1651739852588},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1651739852590},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1651739852581},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1651739852592},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1651739852592},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1651739852593},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1651739852593},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1651739852594},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1651739852596},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1651739852596},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e695e58f714129ca292c2e54cd62c251aca7f7fe","modified":1651739852596},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1651739852597},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1651739852597},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1651739852598},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1651739852599},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1651739852599},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1651739852600},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"ad2dcedf393ed1f3f5afd2508d24969c916d02fc","modified":1651739852600},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1651739852600},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1651739852628},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1651739852634},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1651739852637},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1651739852640},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1651739852647},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1651739852648},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1651739852648},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1651739852649},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1651739852649},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1651739852650},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1651739852654},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1651739852656},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1651739852658},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1651739852663},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1651739852663},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1651739852666},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1651739852667},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1651739852667},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1651739852712},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1651739852712},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1651739852549},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1651739852550},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1651739852550},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1651739852551},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1651739852551},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1651739852552},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1651739852553},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1651739852553},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1651739852554},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1651739852554},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1651739852555},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1651739852555},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1651739852555},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1651739852556},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1651739852557},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1651739852557},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1651739852558},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1651739852558},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1651739852559},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1651739852559},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1651739852560},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1651739852560},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1651739852561},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1651739852561},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1651739852561},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1651739852562},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1651739852562},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1651739852563},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1651739852563},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1651739852563},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1651739852564},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1651739852565},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1651739852564},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1651739852565},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1651739852565},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1651739852566},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1651739852566},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1651739852567},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1651739852567},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1651739852575},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1651739852576},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1651739852576},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1651739852577},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1651739852578},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1651739852577},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1651739852578},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1651739852578},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1651739852579},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1651739852579},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1651739852569},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1651739852570},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1651739852569},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1651739852571},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1651739852571},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1651739852572},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1651739852573},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1651739852575},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1651739852574},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1651739852594},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1651739852595},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1651739852598},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1651739852632},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1651739852633},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1651739852632},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1651739852633},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1651739852634},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1651739852650},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1651739852651},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1651739852651},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1651739852651},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1651739852652},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1651739852653},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1651739852686},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1651739852636},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1651739852683},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1651739852684},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1651739852671},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1651739852675},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1651739852682},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1651739852715},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1651739852644},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1651739852679},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1651739852711},{"_id":"public/guestbook/index.html","hash":"7b341cd571ae1c8eeee60616edeadeb47d48dced","modified":1656385989888},{"_id":"public/tags/index.html","hash":"9b0cf6992e4f390a2ad94c950e0e4a16f14806b8","modified":1656385989888},{"_id":"public/categories/index.html","hash":"a638db80c0aeeeffe6b2d1778e70cd46976722e5","modified":1656385989888},{"_id":"public/archives/page/2/index.html","hash":"f08eb36eb6eafb29db2addacfbed6992bef27669","modified":1656385989888},{"_id":"public/archives/2021/index.html","hash":"cb082cdcc539f69e1fe95074a2c0bd2a8511f1d7","modified":1656385989888},{"_id":"public/archives/2021/01/index.html","hash":"25b1386fd68cfbe4e8f6793b84705a90dc59cb9a","modified":1656385989888},{"_id":"public/archives/2021/03/index.html","hash":"c0c1655cff8312b233701c2fd129c2797e128f77","modified":1656385989888},{"_id":"public/archives/2021/10/index.html","hash":"2c7236376ed45bdefd3767c4c14af6d0aff6d8c2","modified":1656385989888},{"_id":"public/archives/2021/11/index.html","hash":"89cb7d351d32222d7d8758a2813545c23bd2af05","modified":1656385989888},{"_id":"public/archives/2022/index.html","hash":"3f1bdaeccbaf008e293d5bfce356431c5aca4a46","modified":1656385989888},{"_id":"public/archives/2022/01/index.html","hash":"d1a962cf4cfc10d2fbd7a7760b9ef5e41ac39994","modified":1656385989888},{"_id":"public/archives/2022/03/index.html","hash":"e4324dc2dabc67bad9551b2e41696c9ad4a85dca","modified":1656385989888},{"_id":"public/archives/2022/05/index.html","hash":"eb10c9f057300746b25073cbd259b86cff055e98","modified":1656385989888},{"_id":"public/categories//index.html","hash":"4f5236bbd734d8e12e8081af86c879f9a0289139","modified":1656385989888},{"_id":"public/categories//index.html","hash":"c57d9d85420f4b9078ef5adf51534d901faa6868","modified":1656385989888},{"_id":"public/categories//index.html","hash":"e5f51ea24fa4a11b82fda41e0f5670cc517cab72","modified":1656385989888},{"_id":"public/categories//index.html","hash":"b730d7b363f847e39b6343712394e0c2de76dccb","modified":1656385989888},{"_id":"public/categories//index.html","hash":"4a3edf6f1acb88089c3ccf057a0e44fb45b0b7d3","modified":1656385989888},{"_id":"public/categories//index.html","hash":"082b8d43dbfdb9ed9fcfe12ebc7132891f369c5d","modified":1656385989888},{"_id":"public/categories/Sentence-Embedding/index.html","hash":"190f8296fd46786a7d3cdfbfb84d19d66f65011c","modified":1656385989888},{"_id":"public/categories//index.html","hash":"4ecb45e1b753957d11ddea7a4a2442766c9c01af","modified":1656385989888},{"_id":"public/tags/Information-Extraction/index.html","hash":"a335962debbfbf4a39c690b8f244e7484672b0d3","modified":1656385989888},{"_id":"public/tags/SOTA/index.html","hash":"df3a5e0bd5cd69bce9ba13db31e779643acdeabb","modified":1656385989888},{"_id":"public/tags//index.html","hash":"aeff7f5e026cd248a8f94789bf45ded3b615e448","modified":1656385989888},{"_id":"public/tags/Representation-Learning/index.html","hash":"76d6544f3db08b9f47816cd1b579eb3a14c74af0","modified":1656385989888},{"_id":"public/tags/PLM/index.html","hash":"6ea23a722a1f0f5bf3184e2df3363febf8dd0093","modified":1656385989888},{"_id":"public/tags/cross-lingual/index.html","hash":"393d42c5bdc72f00d488a040a7ab65f159442a55","modified":1656385989888},{"_id":"public/tags/PCM/index.html","hash":"aee8b6c333e84adf795788938de1671695dbb6b5","modified":1656385989888},{"_id":"public/tags/Dialog/index.html","hash":"86c86f585007ef9e90bb2fd41a9a071f004dd096","modified":1656385989888},{"_id":"public/tags//index.html","hash":"b81ef1a5d2a0ed5e681f85a0e7974fcc43955df1","modified":1656385989888},{"_id":"public/tags//index.html","hash":"19cd1508216fe684684e0bcdc4daaf3fc0fb770b","modified":1656385989888},{"_id":"public/tags/PM/index.html","hash":"daecf20a1e41e7db745e669da5cca022231c0446","modified":1656385989888},{"_id":"public/tags//index.html","hash":"d3eeb3d9981718ae6f9d697ad4d45b311714ba24","modified":1656385989888},{"_id":"public/tags/mention/index.html","hash":"46db1a914a5be7096671a9a67fe3156bbab5aa6a","modified":1656385989888},{"_id":"public/tags//index.html","hash":"5549624c79a882e197c8a91952672519746fb499","modified":1656385989888},{"_id":"public/tags//index.html","hash":"29cfe151ad358b589d353c71a439513fc2363f4d","modified":1656385989888},{"_id":"public/tags//index.html","hash":"2d05cfabe70e24dbfe2457d519477be09a329040","modified":1656385989888},{"_id":"public/tags/BERT/index.html","hash":"894cb968aaf1496cf2a1b8db914b0365c61d41c8","modified":1656385989888},{"_id":"public/2022/05/24/baidu-uie-sota/index.html","hash":"193a4959fdf1f3e92a88efa44eca1cabcb3fdf52","modified":1656385989888},{"_id":"public/2022/05/11/contrastive-representation-learning/index.html","hash":"ba1a97c2d380da1cec44deaff95fca98e35331d4","modified":1656385989888},{"_id":"public/2022/05/06/roformer/index.html","hash":"85dfde5bf91ae2e22b78af4481a82defafdd0d60","modified":1656385989888},{"_id":"public/2022/03/18/cross-lingual-PLM/index.html","hash":"96fc91358ea9f607e38de8bc08c2a776645fb9be","modified":1656385989888},{"_id":"public/2022/03/10/pretrain-model-yue-01/index.html","hash":"d80a9ca49114df2b9e3498f05a6e1a044ac9f189","modified":1656385989888},{"_id":"public/2022/01/15/sentence-transformers/index.html","hash":"57c3d4fe3cb37babf8ddd7e9a29285a8997454da","modified":1656385989888},{"_id":"public/2022/01/11/galaxy-task-oriented-dialog/index.html","hash":"d2c37fbcdaa6d80a3a71c447d2a4efdd67dfed18","modified":1656385989888},{"_id":"public/2021/11/15/pretrained-model/index.html","hash":"383cc09e7acb30860c1fe5ca60731d3621c4dd9b","modified":1656385989888},{"_id":"public/2021/10/19/word2vec/index.html","hash":"455cff3241e07857dba8c01f06b075fbe21fac9e","modified":1656385989888},{"_id":"public/2021/03/02/bert-3w/index.html","hash":"9bdb59ae17cf4f0f4c7abf65ce626e1d81e20ca2","modified":1656385989888},{"_id":"public/2021/01/06/time-mention/index.html","hash":"ac5d762de9b93506ba985229eed453b88fb32c58","modified":1656385989888},{"_id":"public/2021/01/05/begin-to-learn-nlp/index.html","hash":"fa6f208d8b3e95cc9b20212d96e3265022090dfd","modified":1656385989888},{"_id":"public/archives/index.html","hash":"6f2bc26d933cd91c3e1ba38efbbf11e79b7462bd","modified":1656385989888},{"_id":"public/index.html","hash":"09b21dab28519680a62efa8aa65df7b3dc24c02b","modified":1656385989888},{"_id":"public/page/2/index.html","hash":"9762174a609acd5576b953ec19edfcb55e29af52","modified":1656385989888},{"_id":"public/CNAME","hash":"7e3a38735e86e8a6ac4d79df2bcce665827b172c","modified":1656385989888},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1656385989888},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1656385989888},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1656385989888},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1656385989888},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1656385989888},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1656385989888},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1656385989888},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1656385989888},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1656385989888},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1656385989888},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1656385989888},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1656385989888},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1656385989888},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1656385989888},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1656385989888},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1656385989888},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1656385989888},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1656385989888},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1656385989888},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1656385989888},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1656385989888},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1656385989888},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1656385989888},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1656385989888},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1656385989888},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1656385989888},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1656385989888},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1656385989888},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1656385989888},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1656385989888},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1656385989888},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1656385989888},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1656385989888},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1656385989888},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1656385989888},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1656385989888},{"_id":"public/lib/fastclick/README.html","hash":"c07b353b4efa132290ec4479102a55d80ac6d300","modified":1656385989888},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1656385989888},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1656385989888},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1656385989888},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1656385989888},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1656385989888},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1656385989888},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1656385989888},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1656385989888},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1656385989888},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1656385989888},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1656385989888},{"_id":"public/css/main.css","hash":"b33bdaf25eeba3a8a050007425c2affe9d6bdc64","modified":1656385989888},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1656385989888},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1656385989888},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1656385989888},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1656385989888},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1656385989888},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1656385989888},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1656385989888},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1656385989888},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1656385989888},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1656385989888},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1656385989888},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1656385989888},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1656385989888},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1656385989888},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1656385989888},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1656385989888},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1656385989888},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1656385989888},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1656385989888},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1656385989888}],"Category":[{"name":"","_id":"cl4xlga6m0004q4uxacivhgvn"},{"name":"","_id":"cl4xlga6r000aq4uxbsjsh36j"},{"name":"","_id":"cl4xlga6v000fq4ux6og8arn4"},{"name":"","_id":"cl4xlga76000mq4ux0ptt2z56"},{"name":"","_id":"cl4xlga7c000sq4uxdhd22kov"},{"name":"","_id":"cl4xlga7g000yq4uxbx27ds6x"},{"name":"Sentence Embedding","_id":"cl4xlga7i0016q4uxh06bbuyv"},{"name":"","_id":"cl4xlga7j001bq4ux7dey6v00"}],"Data":[],"Page":[{"title":"guestbook","date":"2022-05-06T07:37:46.000Z","_content":"\n\n<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>","source":"guestbook/index.md","raw":"---\ntitle: guestbook\ndate: 2022-05-06 15:37:46\n---\n\n\n<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>","updated":"2022-05-06T07:38:10.951Z","path":"guestbook/index.html","comments":1,"layout":"page","_id":"cl4xlga6c0000q4uxfu2yfqq6","content":"<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>","site":{"data":{}},"excerpt":"","more":"<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>"},{"title":"tags","date":"2022-05-06T07:42:01.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2022-05-06 15:42:01\ntype: \"tags\"\n---\n","updated":"2022-05-06T07:42:11.098Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cl4xlga6k0002q4uxax1536jx","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2022-05-06T07:39:05.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2022-05-06 15:39:05\ntype: \"categories\"\n---\n","updated":"2022-05-06T07:41:41.552Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cl4xlga6o0006q4uxf6h1e4ki","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"UIE13SOTA","date":"2022-05-24T11:33:25.000Z","_content":"\nUIEUniversal Information Extraction13SOTAUIE\n\n<!--more-->\n\n### Task-specialized IE VS. Universial IE\n\nUIE\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\">\n\n12\n\n\n### Universial IE \nUIESELSSIUIESSI+Text->SEL\nUIE\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\">\n\n\n#### SEL\n\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\">\n\n\n#### SSI\n\nSSIschemapromptTextSchema PromptSEL\n1. [spot]  [text]\n2. [spot]  [asso]  [text]\n3. [spot]  [asso]  [text]\n4. [spot]  [asso]  [text]\n\n\n\n### UIE \n\nUIE+\n1. Text-to-Structure Pre-trainingtext-to-structSSITEXTSEL\n2. Structure Generation Pre-trainingSELSELrecordNoneNoneSEL\n3. Retrofitting Semantic RepresentationNoneTEXTTEXT\n\n\nteacher-forcing\n\n\n### \nUIEUIE+Prompt+\n\n\n\n### \n[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.","source":"_posts/baidu-uie-sota.md","raw":"---\ntitle: UIE13SOTA\ndate: 2022-05-24 19:33:25\ntags:\n- Information Extraction\n- SOTA\ncategories: \n- \n---\n\nUIEUniversal Information Extraction13SOTAUIE\n\n<!--more-->\n\n### Task-specialized IE VS. Universial IE\n\nUIE\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\">\n\n12\n\n\n### Universial IE \nUIESELSSIUIESSI+Text->SEL\nUIE\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\">\n\n\n#### SEL\n\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\">\n\n\n#### SSI\n\nSSIschemapromptTextSchema PromptSEL\n1. [spot]  [text]\n2. [spot]  [asso]  [text]\n3. [spot]  [asso]  [text]\n4. [spot]  [asso]  [text]\n\n\n\n### UIE \n\nUIE+\n1. Text-to-Structure Pre-trainingtext-to-structSSITEXTSEL\n2. Structure Generation Pre-trainingSELSELrecordNoneNoneSEL\n3. Retrofitting Semantic RepresentationNoneTEXTTEXT\n\n\nteacher-forcing\n\n\n### \nUIEUIE+Prompt+\n\n\n\n### \n[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.","slug":"baidu-uie-sota","published":1,"updated":"2022-06-07T08:41:34.925Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6g0001q4uxdduzdc52","content":"<p>UIEUniversal Information Extraction13SOTAUIE</p>\n<span id=\"more\"></span>\n<h3 id=\"Task-specialized-IE-VS-Universial-IE\"><a href=\"#Task-specialized-IE-VS-Universial-IE\" class=\"headerlink\" title=\"Task-specialized IE VS. Universial IE\"></a>Task-specialized IE VS. Universial IE</h3><p>UIE</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\"></p>\n<p>12</p>\n<h3 id=\"Universial-IE-\"><a href=\"#Universial-IE-\" class=\"headerlink\" title=\"Universial IE \"></a>Universial IE </h3><p>UIESELSSIUIESSI+Text-&gt;SEL<br>UIE<br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\"></p>\n<h4 id=\"SEL\"><a href=\"#SEL\" class=\"headerlink\" title=\"SEL\"></a>SEL</h4><p><br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\"></p>\n<h4 id=\"SSI\"><a href=\"#SSI\" class=\"headerlink\" title=\"SSI\"></a>SSI</h4><p>SSIschemapromptTextSchema PromptSEL</p>\n<ol>\n<li>[spot]  [text]</li>\n<li>[spot]  [asso]  [text]</li>\n<li>[spot]  [asso]  [text]</li>\n<li>[spot]  [asso]  [text]</li>\n</ol>\n<h3 id=\"UIE-\"><a href=\"#UIE-\" class=\"headerlink\" title=\"UIE \"></a>UIE </h3><p>UIE+</p>\n<ol>\n<li>Text-to-Structure Pre-trainingtext-to-structSSITEXTSEL</li>\n<li>Structure Generation Pre-trainingSELSELrecordNoneNoneSEL</li>\n<li>Retrofitting Semantic RepresentationNoneTEXTTEXT</li>\n</ol>\n<p>teacher-forcing</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>UIEUIE+Prompt+</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.</p>\n","site":{"data":{}},"excerpt":"<p>UIEUniversal Information Extraction13SOTAUIE</p>","more":"<h3 id=\"Task-specialized-IE-VS-Universial-IE\"><a href=\"#Task-specialized-IE-VS-Universial-IE\" class=\"headerlink\" title=\"Task-specialized IE VS. Universial IE\"></a>Task-specialized IE VS. Universial IE</h3><p>UIE</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\"></p>\n<p>12</p>\n<h3 id=\"Universial-IE-\"><a href=\"#Universial-IE-\" class=\"headerlink\" title=\"Universial IE \"></a>Universial IE </h3><p>UIESELSSIUIESSI+Text-&gt;SEL<br>UIE<br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\"></p>\n<h4 id=\"SEL\"><a href=\"#SEL\" class=\"headerlink\" title=\"SEL\"></a>SEL</h4><p><br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\"></p>\n<h4 id=\"SSI\"><a href=\"#SSI\" class=\"headerlink\" title=\"SSI\"></a>SSI</h4><p>SSIschemapromptTextSchema PromptSEL</p>\n<ol>\n<li>[spot]  [text]</li>\n<li>[spot]  [asso]  [text]</li>\n<li>[spot]  [asso]  [text]</li>\n<li>[spot]  [asso]  [text]</li>\n</ol>\n<h3 id=\"UIE-\"><a href=\"#UIE-\" class=\"headerlink\" title=\"UIE \"></a>UIE </h3><p>UIE+</p>\n<ol>\n<li>Text-to-Structure Pre-trainingtext-to-structSSITEXTSEL</li>\n<li>Structure Generation Pre-trainingSELSELrecordNoneNoneSEL</li>\n<li>Retrofitting Semantic RepresentationNoneTEXTTEXT</li>\n</ol>\n<p>teacher-forcing</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>UIEUIE+Prompt+</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.</p>"},{"title":"Learning NLP","date":"2021-01-05T09:29:16.000Z","_content":"\n\nNLP...\n<!--more-->\n\n\n\n\n### \n\n~\n\n1. https://github.com/d2l-ai/d2l-zh\n2. BERT\n3. \n4. Stanford EE364\n5. CMU https://www.stat.cmu.edu/~ryantibs/convexopt/\n6. https://www.langboat.com/academy/basics/0-basics\n\n\n\n\n### \n\n\n\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n\n\n\n\n\n### \n\n...\n","source":"_posts/begin-to-learn-nlp.md","raw":"---\ntitle: Learning NLP\ndate: 2021-01-05 17:29:16\ntags:\n- \ncategories: \n- \n---\n\n\nNLP...\n<!--more-->\n\n\n\n\n### \n\n~\n\n1. https://github.com/d2l-ai/d2l-zh\n2. BERT\n3. \n4. Stanford EE364\n5. CMU https://www.stat.cmu.edu/~ryantibs/convexopt/\n6. https://www.langboat.com/academy/basics/0-basics\n\n\n\n\n### \n\n\n\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n\n\n\n\n\n### \n\n...\n","slug":"begin-to-learn-nlp","published":1,"updated":"2022-06-08T08:47:02.364Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6l0003q4uxaami25xn","content":"<p>NLP<br><span id=\"more\"></span></p>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>~</p>\n<ol>\n<li><a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></li>\n<li>BERT</li>\n<li></li>\n<li>Stanford EE364</li>\n<li>CMU <a href=\"https://www.stat.cmu.edu/~ryantibs/convexopt/\">https://www.stat.cmu.edu/~ryantibs/convexopt/</a></li>\n<li><a href=\"https://www.langboat.com/academy/basics/0-basics\">https://www.langboat.com/academy/basics/0-basics</a></li>\n</ol>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n","site":{"data":{}},"excerpt":"<p>NLP<br>","more":"</p>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>~</p>\n<ol>\n<li><a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></li>\n<li>BERT</li>\n<li></li>\n<li>Stanford EE364</li>\n<li>CMU <a href=\"https://www.stat.cmu.edu/~ryantibs/convexopt/\">https://www.stat.cmu.edu/~ryantibs/convexopt/</a></li>\n<li><a href=\"https://www.langboat.com/academy/basics/0-basics\">https://www.langboat.com/academy/basics/0-basics</a></li>\n</ol>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>"},{"title":"","date":"2022-05-11T08:41:52.000Z","mathjax":true,"_content":"\n\nSimCSE[1]\n\n<!--more-->\n\n\n### sentence embedding\nNLPsentence embeddingloss\n\n\n### \n\nbatchbatch\n\n\n1. \n2. DropoutSimCSE\n3. Cutofftokenfeaturespan[2]\n4. Back-translation\n\n\n\n### loss\nlossloss\n\n#### Contrastive Loss[3]\n\n\n$$ \\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1) $$\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2) $$\n\n$W$$X_1$$X_2$$Y$0-101$L_S$loss$L_D$losslossloss3\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\">\n\n\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3) $$\n\nmarginloss$L_S$loss$L_D$marginmarginmargin\n\n\n#### NECNoise Contrastive EstimationLoss[5]\nNEC Noise Contrastive Estimation[4]NCEx$p(x)$$$p(x) = \\frac{e^{G(x)}}{Z}$$, $$Z=\\sum_x{e^{G(x)}}$$NEC$P(1|x)$$p(x)$\n$$ p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4) $$\n\n$$\\tilde{p(x)}$$$U(x)$loss\n$$ \\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5) $$\n\n\n$$ \\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)$$\n\n$$\\gamma - logU(x)$$NEC\n\n\n#### Triplet Loss[6]\nloss$(x,x^+,x^-)$\n$$ \\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)$$\n\n\n\n\n#### InfoNCE Loss[7]\nInfoNCENEC$X$$Y$$p(x,y)$$X$$Y$$p(x)$$p(y)$$X$$Y$$X$$Y$\n\n$$I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)$$\n\n\n$$p(x_{t+k}|c_t)$$$$c_t$$$$k$$$$x_{t+k}$$$$c_t$$$$x_{t+k}$$:\n\n\n$$P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)$$\n\n\n8\n$$I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)$$\n\n\n$$c_t$$$$x_{t+k}$$$$p(x_{t+k},c_t)$$$$\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$$NCE$$p(x_{t+k}|c_t)$$1$$p(x_{t+k})$$0NCE1NCE$$c_t$$$$x_{t+k}$$InfoNCE loss:\n\n\n$$  \\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11) $$\n\nInfoNCE\n\n#### SCL Loss[8]\nSCL loss2021fine-tunelosslossfine-tunelosslossSCL loss\n\n$$ \\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12) $$\n$$ \\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13) $$\n\n\n$$ \\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14) $$\n\n\n### \n\n[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.\n[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE, 2006, 2: 1735-1742.\n[4] Gutmann M, Hyvrinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.\n[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.\n[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.\n[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.\n[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.\n","source":"_posts/contrastive-representation-learning.md","raw":"---\ntitle: \ndate: 2022-05-11 16:41:52\ntags:\n- Representation Learning\ncategories: \n- \nmathjax: true\n---\n\n\nSimCSE[1]\n\n<!--more-->\n\n\n### sentence embedding\nNLPsentence embeddingloss\n\n\n### \n\nbatchbatch\n\n\n1. \n2. DropoutSimCSE\n3. Cutofftokenfeaturespan[2]\n4. Back-translation\n\n\n\n### loss\nlossloss\n\n#### Contrastive Loss[3]\n\n\n$$ \\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1) $$\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2) $$\n\n$W$$X_1$$X_2$$Y$0-101$L_S$loss$L_D$losslossloss3\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\">\n\n\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3) $$\n\nmarginloss$L_S$loss$L_D$marginmarginmargin\n\n\n#### NECNoise Contrastive EstimationLoss[5]\nNEC Noise Contrastive Estimation[4]NCEx$p(x)$$$p(x) = \\frac{e^{G(x)}}{Z}$$, $$Z=\\sum_x{e^{G(x)}}$$NEC$P(1|x)$$p(x)$\n$$ p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4) $$\n\n$$\\tilde{p(x)}$$$U(x)$loss\n$$ \\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5) $$\n\n\n$$ \\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)$$\n\n$$\\gamma - logU(x)$$NEC\n\n\n#### Triplet Loss[6]\nloss$(x,x^+,x^-)$\n$$ \\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)$$\n\n\n\n\n#### InfoNCE Loss[7]\nInfoNCENEC$X$$Y$$p(x,y)$$X$$Y$$p(x)$$p(y)$$X$$Y$$X$$Y$\n\n$$I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)$$\n\n\n$$p(x_{t+k}|c_t)$$$$c_t$$$$k$$$$x_{t+k}$$$$c_t$$$$x_{t+k}$$:\n\n\n$$P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)$$\n\n\n8\n$$I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)$$\n\n\n$$c_t$$$$x_{t+k}$$$$p(x_{t+k},c_t)$$$$\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$$NCE$$p(x_{t+k}|c_t)$$1$$p(x_{t+k})$$0NCE1NCE$$c_t$$$$x_{t+k}$$InfoNCE loss:\n\n\n$$  \\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11) $$\n\nInfoNCE\n\n#### SCL Loss[8]\nSCL loss2021fine-tunelosslossfine-tunelosslossSCL loss\n\n$$ \\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12) $$\n$$ \\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13) $$\n\n\n$$ \\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14) $$\n\n\n### \n\n[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.\n[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE, 2006, 2: 1735-1742.\n[4] Gutmann M, Hyvrinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.\n[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.\n[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.\n[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.\n[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.\n","slug":"contrastive-representation-learning","published":1,"updated":"2022-06-07T09:43:32.670Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6p0007q4ux5z0jezoz","content":"<p>SimCSE[1]</p>\n<span id=\"more\"></span>\n<h3 id=\"sentence-embedding\"><a href=\"#sentence-embedding\" class=\"headerlink\" title=\"sentence embedding\"></a>sentence embedding</h3><p>NLPsentence embeddingloss</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>batchbatch</p>\n<ol>\n<li></li>\n<li>DropoutSimCSE</li>\n<li>Cutofftokenfeaturespan[2]</li>\n<li>Back-translation</li>\n</ol>\n<h3 id=\"loss\"><a href=\"#loss\" class=\"headerlink\" title=\"loss\"></a>loss</h3><p>lossloss</p>\n<h4 id=\"Contrastive-Loss-3\"><a href=\"#Contrastive-Loss-3\" class=\"headerlink\" title=\"Contrastive Loss[3]\"></a>Contrastive Loss[3]</h4><p></p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1)</script><script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2)</script><p>$W$$X_1$$X_2$$Y$0-101$L_S$loss$L_D$losslossloss3</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\"></p>\n<script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3)</script><p>marginloss$L_S$loss$L_D$marginmarginmargin</p>\n<h4 id=\"NECNoise-Contrastive-EstimationLoss-5\"><a href=\"#NECNoise-Contrastive-EstimationLoss-5\" class=\"headerlink\" title=\"NECNoise Contrastive EstimationLoss[5]\"></a>NECNoise Contrastive EstimationLoss[5]</h4><p>NEC Noise Contrastive Estimation[4]NCEx$p(x)$<script type=\"math/tex\">p(x) = \\frac{e^{G(x)}}{Z}</script>, <script type=\"math/tex\">Z=\\sum_x{e^{G(x)}}</script>NEC$P(1|x)$$p(x)$</p>\n<script type=\"math/tex; mode=display\">p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4)</script><p><script type=\"math/tex\">\\tilde{p(x)}</script>$U(x)$loss</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5)</script><p></p>\n<script type=\"math/tex; mode=display\">\\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)</script><p><script type=\"math/tex\">\\gamma - logU(x)</script>NEC</p>\n<h4 id=\"Triplet-Loss-6\"><a href=\"#Triplet-Loss-6\" class=\"headerlink\" title=\"Triplet Loss[6]\"></a>Triplet Loss[6]</h4><p>loss$(x,x^+,x^-)$</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)</script><p></p>\n<h4 id=\"InfoNCE-Loss-7\"><a href=\"#InfoNCE-Loss-7\" class=\"headerlink\" title=\"InfoNCE Loss[7]\"></a>InfoNCE Loss[7]</h4><p>InfoNCENEC$X$$Y$$p(x,y)$$X$$Y$$p(x)$$p(y)$$X$$Y$$X$$Y$</p>\n<script type=\"math/tex; mode=display\">I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)</script><p><script type=\"math/tex\">p(x_{t+k}|c_t)</script><script type=\"math/tex\">c_t</script><script type=\"math/tex\">k</script><script type=\"math/tex\">x_{t+k}</script><script type=\"math/tex\">c_t</script><script type=\"math/tex\">x_{t+k}</script>:</p>\n<script type=\"math/tex; mode=display\">P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)</script><p>8</p>\n<script type=\"math/tex; mode=display\">I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)</script><p><script type=\"math/tex\">c_t</script><script type=\"math/tex\">x_{t+k}</script><script type=\"math/tex\">p(x_{t+k},c_t)</script><script type=\"math/tex\">\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}</script>NCE<script type=\"math/tex\">p(x_{t+k}|c_t)</script>1<script type=\"math/tex\">p(x_{t+k})</script>0NCE1NCE<script type=\"math/tex\">c_t</script><script type=\"math/tex\">x_{t+k}</script>InfoNCE loss:</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11)</script><p>InfoNCE</p>\n<h4 id=\"SCL-Loss-8\"><a href=\"#SCL-Loss-8\" class=\"headerlink\" title=\"SCL Loss[8]\"></a>SCL Loss[8]</h4><p>SCL loss2021fine-tunelosslossfine-tunelosslossSCL loss</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14)</script><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.<br>[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR06). IEEE, 2006, 2: 1735-1742.<br>[4] Gutmann M, Hyvrinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.<br>[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.<br>[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.<br>[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.<br>[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.</p>\n","site":{"data":{}},"excerpt":"<p>SimCSE[1]</p>","more":"<h3 id=\"sentence-embedding\"><a href=\"#sentence-embedding\" class=\"headerlink\" title=\"sentence embedding\"></a>sentence embedding</h3><p>NLPsentence embeddingloss</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>batchbatch</p>\n<ol>\n<li></li>\n<li>DropoutSimCSE</li>\n<li>Cutofftokenfeaturespan[2]</li>\n<li>Back-translation</li>\n</ol>\n<h3 id=\"loss\"><a href=\"#loss\" class=\"headerlink\" title=\"loss\"></a>loss</h3><p>lossloss</p>\n<h4 id=\"Contrastive-Loss-3\"><a href=\"#Contrastive-Loss-3\" class=\"headerlink\" title=\"Contrastive Loss[3]\"></a>Contrastive Loss[3]</h4><p></p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1)</script><script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2)</script><p>$W$$X_1$$X_2$$Y$0-101$L_S$loss$L_D$losslossloss3</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\"></p>\n<script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3)</script><p>marginloss$L_S$loss$L_D$marginmarginmargin</p>\n<h4 id=\"NECNoise-Contrastive-EstimationLoss-5\"><a href=\"#NECNoise-Contrastive-EstimationLoss-5\" class=\"headerlink\" title=\"NECNoise Contrastive EstimationLoss[5]\"></a>NECNoise Contrastive EstimationLoss[5]</h4><p>NEC Noise Contrastive Estimation[4]NCEx$p(x)$<script type=\"math/tex\">p(x) = \\frac{e^{G(x)}}{Z}</script>, <script type=\"math/tex\">Z=\\sum_x{e^{G(x)}}</script>NEC$P(1|x)$$p(x)$</p>\n<script type=\"math/tex; mode=display\">p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4)</script><p><script type=\"math/tex\">\\tilde{p(x)}</script>$U(x)$loss</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5)</script><p></p>\n<script type=\"math/tex; mode=display\">\\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)</script><p><script type=\"math/tex\">\\gamma - logU(x)</script>NEC</p>\n<h4 id=\"Triplet-Loss-6\"><a href=\"#Triplet-Loss-6\" class=\"headerlink\" title=\"Triplet Loss[6]\"></a>Triplet Loss[6]</h4><p>loss$(x,x^+,x^-)$</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)</script><p></p>\n<h4 id=\"InfoNCE-Loss-7\"><a href=\"#InfoNCE-Loss-7\" class=\"headerlink\" title=\"InfoNCE Loss[7]\"></a>InfoNCE Loss[7]</h4><p>InfoNCENEC$X$$Y$$p(x,y)$$X$$Y$$p(x)$$p(y)$$X$$Y$$X$$Y$</p>\n<script type=\"math/tex; mode=display\">I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)</script><p><script type=\"math/tex\">p(x_{t+k}|c_t)</script><script type=\"math/tex\">c_t</script><script type=\"math/tex\">k</script><script type=\"math/tex\">x_{t+k}</script><script type=\"math/tex\">c_t</script><script type=\"math/tex\">x_{t+k}</script>:</p>\n<script type=\"math/tex; mode=display\">P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)</script><p>8</p>\n<script type=\"math/tex; mode=display\">I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)</script><p><script type=\"math/tex\">c_t</script><script type=\"math/tex\">x_{t+k}</script><script type=\"math/tex\">p(x_{t+k},c_t)</script><script type=\"math/tex\">\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}</script>NCE<script type=\"math/tex\">p(x_{t+k}|c_t)</script>1<script type=\"math/tex\">p(x_{t+k})</script>0NCE1NCE<script type=\"math/tex\">c_t</script><script type=\"math/tex\">x_{t+k}</script>InfoNCE loss:</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11)</script><p>InfoNCE</p>\n<h4 id=\"SCL-Loss-8\"><a href=\"#SCL-Loss-8\" class=\"headerlink\" title=\"SCL Loss[8]\"></a>SCL Loss[8]</h4><p>SCL loss2021fine-tunelosslossfine-tunelosslossSCL loss</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14)</script><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.<br>[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR06). IEEE, 2006, 2: 1735-1742.<br>[4] Gutmann M, Hyvrinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.<br>[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.<br>[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.<br>[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.<br>[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.</p>"},{"title":"","date":"2022-03-18T11:30:17.000Z","mathjax":true,"_content":"\n\n<!--more-->\n\n\n### \n\nbertbertmbertmbertbertmbert104110kmbertmbertXLMXLM-RLaBSEInfoXLMXLM-EVECOERNIE-MencoderMASSmBART\n\n\n#### XLM[1]\nfocebook2019XLMXLMbertMLM+TLMCLM+MLMTLMMLMCLMXLM\n\n\n#### XLM-R[2]\nfocebook2020100XLM-RXLM-RXLMRoBertaXLM-RCommonCrawlmbertwikiXLM-RmbertXNLI\n\n\n#### LaBSE[3]\nGoogle2020LaBSELaBSEbi-encoderbertLaBSE109MLM+TLMLaBSETatoeba83.7%65.5%Tatoeba30+LaBSELaBSEzero-shotbert CommonCrawlwikiCommonCrawlCDSLaBSE\n\n\n\n#### XLM-E[4]\nXLM-EELECTRAMRTDTRTDmaskmaskmaskmask\n\n\n#### InfoXLM[5]\nInfoXLMcross-lingual contrastXLCOInfoXLMXLCOMMLMTLMMMLMTLMMMLMmaskmaskmaskInfoNCEloss$$L_{MMLM}$$TLM$$(c_1,c_2)$$$$x_1$$$$c_1$$maskmask$$c1$$$$x_1$$$$c_2$$$$x_1|c_1$$MMLMmaskloss$$L_{TLM}$$XLCOloss$$L_{XLCO}$$InfoXLMloss$$L = L_{MMLM} + L_{TLM} + L_{XLCO}$$\n\n\n#### ERNIE-M[6]\nERNIE-MERNIEERNIE-MERNIE-MCAMLMBTMLMMMLMTLM\n\n\nCAMLMBTMLMCAMLMTLMMMLMCAMLMCAMLMmaskMMLMTLM\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\">\n\n\nBLMLM\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\">\n\nERNIE-MXNLIERINE-MInfoXML\n\n\n### \n\n[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.\n[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.\n[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.\n[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.\n[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.\n[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.","source":"_posts/cross-lingual-PLM.md","raw":"---\ntitle: \ndate: 2022-03-18 19:30:17\ntags:\n- PLM\n- cross-lingual\ncategories: \n- \nmathjax: true\n---\n\n\n<!--more-->\n\n\n### \n\nbertbertmbertmbertbertmbert104110kmbertmbertXLMXLM-RLaBSEInfoXLMXLM-EVECOERNIE-MencoderMASSmBART\n\n\n#### XLM[1]\nfocebook2019XLMXLMbertMLM+TLMCLM+MLMTLMMLMCLMXLM\n\n\n#### XLM-R[2]\nfocebook2020100XLM-RXLM-RXLMRoBertaXLM-RCommonCrawlmbertwikiXLM-RmbertXNLI\n\n\n#### LaBSE[3]\nGoogle2020LaBSELaBSEbi-encoderbertLaBSE109MLM+TLMLaBSETatoeba83.7%65.5%Tatoeba30+LaBSELaBSEzero-shotbert CommonCrawlwikiCommonCrawlCDSLaBSE\n\n\n\n#### XLM-E[4]\nXLM-EELECTRAMRTDTRTDmaskmaskmaskmask\n\n\n#### InfoXLM[5]\nInfoXLMcross-lingual contrastXLCOInfoXLMXLCOMMLMTLMMMLMTLMMMLMmaskmaskmaskInfoNCEloss$$L_{MMLM}$$TLM$$(c_1,c_2)$$$$x_1$$$$c_1$$maskmask$$c1$$$$x_1$$$$c_2$$$$x_1|c_1$$MMLMmaskloss$$L_{TLM}$$XLCOloss$$L_{XLCO}$$InfoXLMloss$$L = L_{MMLM} + L_{TLM} + L_{XLCO}$$\n\n\n#### ERNIE-M[6]\nERNIE-MERNIEERNIE-MERNIE-MCAMLMBTMLMMMLMTLM\n\n\nCAMLMBTMLMCAMLMTLMMMLMCAMLMCAMLMmaskMMLMTLM\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\">\n\n\nBLMLM\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\">\n\nERNIE-MXNLIERINE-MInfoXML\n\n\n### \n\n[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.\n[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.\n[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.\n[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.\n[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.\n[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.","slug":"cross-lingual-PLM","published":1,"updated":"2022-06-07T07:12:05.510Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6q0008q4uxfjuh6b38","content":"<p><br><span id=\"more\"></span></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>bertbertmbertmbertbertmbert104110kmbertmbertXLMXLM-RLaBSEInfoXLMXLM-EVECOERNIE-MencoderMASSmBART</p>\n<h4 id=\"XLM-1\"><a href=\"#XLM-1\" class=\"headerlink\" title=\"XLM[1]\"></a>XLM[1]</h4><p>focebook2019XLMXLMbertMLM+TLMCLM+MLMTLMMLMCLMXLM</p>\n<h4 id=\"XLM-R-2\"><a href=\"#XLM-R-2\" class=\"headerlink\" title=\"XLM-R[2]\"></a>XLM-R[2]</h4><p>focebook2020100XLM-RXLM-RXLMRoBertaXLM-RCommonCrawlmbertwikiXLM-RmbertXNLI</p>\n<h4 id=\"LaBSE-3\"><a href=\"#LaBSE-3\" class=\"headerlink\" title=\"LaBSE[3]\"></a>LaBSE[3]</h4><p>Google2020LaBSELaBSEbi-encoderbertLaBSE109MLM+TLMLaBSETatoeba83.7%65.5%Tatoeba30+LaBSELaBSEzero-shotbert CommonCrawlwikiCommonCrawlCDSLaBSE</p>\n<h4 id=\"XLM-E-4\"><a href=\"#XLM-E-4\" class=\"headerlink\" title=\"XLM-E[4]\"></a>XLM-E[4]</h4><p>XLM-EELECTRAMRTDTRTDmaskmaskmaskmask</p>\n<h4 id=\"InfoXLM-5\"><a href=\"#InfoXLM-5\" class=\"headerlink\" title=\"InfoXLM[5]\"></a>InfoXLM[5]</h4><p>InfoXLMcross-lingual contrastXLCOInfoXLMXLCOMMLMTLMMMLMTLMMMLMmaskmaskmaskInfoNCEloss<script type=\"math/tex\">L_{MMLM}</script>TLM<script type=\"math/tex\">(c_1,c_2)</script><script type=\"math/tex\">x_1</script><script type=\"math/tex\">c_1</script>maskmask<script type=\"math/tex\">c1</script><script type=\"math/tex\">x_1</script><script type=\"math/tex\">c_2</script><script type=\"math/tex\">x_1|c_1</script>MMLMmaskloss<script type=\"math/tex\">L_{TLM}</script>XLCOloss<script type=\"math/tex\">L_{XLCO}</script>InfoXLMloss<script type=\"math/tex\">L = L_{MMLM} + L_{TLM} + L_{XLCO}</script></p>\n<h4 id=\"ERNIE-M-6\"><a href=\"#ERNIE-M-6\" class=\"headerlink\" title=\"ERNIE-M[6]\"></a>ERNIE-M[6]</h4><p>ERNIE-MERNIEERNIE-MERNIE-MCAMLMBTMLMMMLMTLM</p>\n<p>CAMLMBTMLMCAMLMTLMMMLMCAMLMCAMLMmaskMMLMTLM<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\"></p>\n<p>BLMLM<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\"></p>\n<p>ERNIE-MXNLIERINE-MInfoXML</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.<br>[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.<br>[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.<br>[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.<br>[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.<br>[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.</p>\n","site":{"data":{}},"excerpt":"<p><br>","more":"</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>bertbertmbertmbertbertmbert104110kmbertmbertXLMXLM-RLaBSEInfoXLMXLM-EVECOERNIE-MencoderMASSmBART</p>\n<h4 id=\"XLM-1\"><a href=\"#XLM-1\" class=\"headerlink\" title=\"XLM[1]\"></a>XLM[1]</h4><p>focebook2019XLMXLMbertMLM+TLMCLM+MLMTLMMLMCLMXLM</p>\n<h4 id=\"XLM-R-2\"><a href=\"#XLM-R-2\" class=\"headerlink\" title=\"XLM-R[2]\"></a>XLM-R[2]</h4><p>focebook2020100XLM-RXLM-RXLMRoBertaXLM-RCommonCrawlmbertwikiXLM-RmbertXNLI</p>\n<h4 id=\"LaBSE-3\"><a href=\"#LaBSE-3\" class=\"headerlink\" title=\"LaBSE[3]\"></a>LaBSE[3]</h4><p>Google2020LaBSELaBSEbi-encoderbertLaBSE109MLM+TLMLaBSETatoeba83.7%65.5%Tatoeba30+LaBSELaBSEzero-shotbert CommonCrawlwikiCommonCrawlCDSLaBSE</p>\n<h4 id=\"XLM-E-4\"><a href=\"#XLM-E-4\" class=\"headerlink\" title=\"XLM-E[4]\"></a>XLM-E[4]</h4><p>XLM-EELECTRAMRTDTRTDmaskmaskmaskmask</p>\n<h4 id=\"InfoXLM-5\"><a href=\"#InfoXLM-5\" class=\"headerlink\" title=\"InfoXLM[5]\"></a>InfoXLM[5]</h4><p>InfoXLMcross-lingual contrastXLCOInfoXLMXLCOMMLMTLMMMLMTLMMMLMmaskmaskmaskInfoNCEloss<script type=\"math/tex\">L_{MMLM}</script>TLM<script type=\"math/tex\">(c_1,c_2)</script><script type=\"math/tex\">x_1</script><script type=\"math/tex\">c_1</script>maskmask<script type=\"math/tex\">c1</script><script type=\"math/tex\">x_1</script><script type=\"math/tex\">c_2</script><script type=\"math/tex\">x_1|c_1</script>MMLMmaskloss<script type=\"math/tex\">L_{TLM}</script>XLCOloss<script type=\"math/tex\">L_{XLCO}</script>InfoXLMloss<script type=\"math/tex\">L = L_{MMLM} + L_{TLM} + L_{XLCO}</script></p>\n<h4 id=\"ERNIE-M-6\"><a href=\"#ERNIE-M-6\" class=\"headerlink\" title=\"ERNIE-M[6]\"></a>ERNIE-M[6]</h4><p>ERNIE-MERNIEERNIE-MERNIE-MCAMLMBTMLMMMLMTLM</p>\n<p>CAMLMBTMLMCAMLMTLMMMLMCAMLMCAMLMmaskMMLMTLM<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\"></p>\n<p>BLMLM<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\"></p>\n<p>ERNIE-MXNLIERINE-MInfoXML</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.<br>[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.<br>[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.<br>[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.<br>[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.<br>[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.</p>"},{"title":"GALAXY","date":"2022-01-11T08:43:21.000Z","_content":"\nGALAXY[1]Pre-trained Conversation ModelPCMIn-CarMultiWOZ2.0MultiWOZ2.1SOTA\n\n<!--more-->\n\n### \n123pipelineGALAXYUniLM\n\n\n### PCM\n\n1. \n2. \n3. \n\n\n### GALAXY\n\n1. UniDAUnDial\n2. UniLMLossKL\n\nGALAXY:\n<img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\">\n\ncontextcontextconextresponsecontextresponsecontextKLKLcontextresponseGALAXY\n\n\n### \n[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.","source":"_posts/galaxy-task-oriented-dialog.md","raw":"---\ntitle: GALAXY\ndate: 2022-01-11 16:43:21\ntags:\n- PCM\n- Dialog\ncategories: \n- \n---\n\nGALAXY[1]Pre-trained Conversation ModelPCMIn-CarMultiWOZ2.0MultiWOZ2.1SOTA\n\n<!--more-->\n\n### \n123pipelineGALAXYUniLM\n\n\n### PCM\n\n1. \n2. \n3. \n\n\n### GALAXY\n\n1. UniDAUnDial\n2. UniLMLossKL\n\nGALAXY:\n<img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\">\n\ncontextcontextconextresponsecontextresponsecontextKLKLcontextresponseGALAXY\n\n\n### \n[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.","slug":"galaxy-task-oriented-dialog","published":1,"updated":"2022-06-07T07:12:31.362Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6r0009q4ux270ib8fz","content":"<p>GALAXY[1]Pre-trained Conversation ModelPCMIn-CarMultiWOZ2.0MultiWOZ2.1SOTA</p>\n<span id=\"more\"></span>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>123pipelineGALAXYUniLM</p>\n<h3 id=\"PCM\"><a href=\"#PCM\" class=\"headerlink\" title=\"PCM\"></a>PCM</h3><ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<h3 id=\"GALAXY\"><a href=\"#GALAXY\" class=\"headerlink\" title=\"GALAXY\"></a>GALAXY</h3><ol>\n<li>UniDAUnDial</li>\n<li>UniLMLossKL</li>\n</ol>\n<p>GALAXY:<br><img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\"></p>\n<p>contextcontextconextresponsecontextresponsecontextKLKLcontextresponseGALAXY</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.</p>\n","site":{"data":{}},"excerpt":"<p>GALAXY[1]Pre-trained Conversation ModelPCMIn-CarMultiWOZ2.0MultiWOZ2.1SOTA</p>","more":"<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>123pipelineGALAXYUniLM</p>\n<h3 id=\"PCM\"><a href=\"#PCM\" class=\"headerlink\" title=\"PCM\"></a>PCM</h3><ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<h3 id=\"GALAXY\"><a href=\"#GALAXY\" class=\"headerlink\" title=\"GALAXY\"></a>GALAXY</h3><ol>\n<li>UniDAUnDial</li>\n<li>UniLMLossKL</li>\n</ol>\n<p>GALAXY:<br><img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\"></p>\n<p>contextcontextconextresponsecontextresponsecontextKLKLcontextresponseGALAXY</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.</p>"},{"title":"","date":"2022-03-10T03:13:48.000Z","_content":"\n\n<!--more-->\n\n\n### \nWikipediaWikipedia[2]20220501wiki88Mhuggingfacex-tech/cantonese-mandarin-translation[3][4]0.0\n1. http://www.hkcna.hk/index.jsp?channel=2803\n2. https://weibo.com/u/6227634537?refer_flag=1005055013_\n3. https://subanana.com/ AI\n4. https://commonvoice.mozilla.org/zh-CN/datasets\n\n\n### \n\n1. https://app.diffbot.com/\n2. https://commoncrawl.org/the-data/get-started/\n3. https://webz.io/\n4. https://www.parsehub.com/\n5. https://www.scrapingbee.com/\n\n\n### \nwikiGENSIM[5]wikiwiki\n\n```python\nfrom gensim.corpora import WikiCorpus\n\nif __name__ == '__main__':\n    output = open('wiki.txt', 'w', encoding='utf8')\n    wiki = WikiCorpus(\"wiki-xxx.xml.bz2\", dictionary={})\n    for text in wiki.get_texts():\n        output.write(\" \".join(text) + \"\\n\")\n    output.close()\t\t \n```\n\nwikihttps://github.com/Quelisa/data_cleaner.git\n\n### \n\nbig data\n\n\nCINO (Chinese minority PLM)[1]LaBSE,CINOLaBSE\n\n\n### \n[1] https://github.com/ymcui/Chinese-Minority-PLM\n[2] https://dumps.wikimedia.org/zh_yuewiki/\n[3] https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\n[4] https://hkcc.eduhk.hk/v1/introduction.html\n[5] https://radimrehurek.com/gensim/apiref.html#api-reference","source":"_posts/pretrain-model-yue-01.md","raw":"---\ntitle: \ndate: 2022-03-10 11:13:48\ntags:\n- \n- \ncategories: \n- \n---\n\n\n<!--more-->\n\n\n### \nWikipediaWikipedia[2]20220501wiki88Mhuggingfacex-tech/cantonese-mandarin-translation[3][4]0.0\n1. http://www.hkcna.hk/index.jsp?channel=2803\n2. https://weibo.com/u/6227634537?refer_flag=1005055013_\n3. https://subanana.com/ AI\n4. https://commonvoice.mozilla.org/zh-CN/datasets\n\n\n### \n\n1. https://app.diffbot.com/\n2. https://commoncrawl.org/the-data/get-started/\n3. https://webz.io/\n4. https://www.parsehub.com/\n5. https://www.scrapingbee.com/\n\n\n### \nwikiGENSIM[5]wikiwiki\n\n```python\nfrom gensim.corpora import WikiCorpus\n\nif __name__ == '__main__':\n    output = open('wiki.txt', 'w', encoding='utf8')\n    wiki = WikiCorpus(\"wiki-xxx.xml.bz2\", dictionary={})\n    for text in wiki.get_texts():\n        output.write(\" \".join(text) + \"\\n\")\n    output.close()\t\t \n```\n\nwikihttps://github.com/Quelisa/data_cleaner.git\n\n### \n\nbig data\n\n\nCINO (Chinese minority PLM)[1]LaBSE,CINOLaBSE\n\n\n### \n[1] https://github.com/ymcui/Chinese-Minority-PLM\n[2] https://dumps.wikimedia.org/zh_yuewiki/\n[3] https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\n[4] https://hkcc.eduhk.hk/v1/introduction.html\n[5] https://radimrehurek.com/gensim/apiref.html#api-reference","slug":"pretrain-model-yue-01","published":1,"updated":"2022-06-08T07:00:47.496Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6s000cq4uxh0vy08dp","content":"<p><br><span id=\"more\"></span></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>WikipediaWikipedia[2]20220501wiki88Mhuggingfacex-tech/cantonese-mandarin-translation[3][4]0.0</p>\n<ol>\n<li><a href=\"http://www.hkcna.hk/index.jsp?channel=2803\">http://www.hkcna.hk/index.jsp?channel=2803</a></li>\n<li><a href=\"https://weibo.com/u/6227634537?refer_flag=1005055013_\">https://weibo.com/u/6227634537?refer_flag=1005055013_</a></li>\n<li><a href=\"https://subanana.com/\">https://subanana.com/</a> AI</li>\n<li><a href=\"https://commonvoice.mozilla.org/zh-CN/datasets\">https://commonvoice.mozilla.org/zh-CN/datasets</a></li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<ol>\n<li><a href=\"https://app.diffbot.com/\">https://app.diffbot.com/</a></li>\n<li><a href=\"https://commoncrawl.org/the-data/get-started/\">https://commoncrawl.org/the-data/get-started/</a></li>\n<li><a href=\"https://webz.io/\">https://webz.io/</a></li>\n<li><a href=\"https://www.parsehub.com/\">https://www.parsehub.com/</a></li>\n<li><a href=\"https://www.scrapingbee.com/\">https://www.scrapingbee.com/</a></li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>wikiGENSIM[5]wikiwiki</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> gensim.corpora <span class=\"keyword\">import</span> WikiCorpus</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    output = <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;wiki.txt&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, encoding=<span class=\"string\">&#x27;utf8&#x27;</span>)</span><br><span class=\"line\">    wiki = WikiCorpus(<span class=\"string\">&quot;wiki-xxx.xml.bz2&quot;</span>, dictionary=&#123;&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> wiki.get_texts():</span><br><span class=\"line\">        output.write(<span class=\"string\">&quot; &quot;</span>.join(text) + <span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">    output.close()\t\t </span><br></pre></td></tr></table></figure>\n<p>wiki<a href=\"https://github.com/Quelisa/data_cleaner.git\">https://github.com/Quelisa/data_cleaner.git</a></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>big data</p>\n<p>CINO (Chinese minority PLM)[1]LaBSE,CINOLaBSE</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] <a href=\"https://github.com/ymcui/Chinese-Minority-PLM\">https://github.com/ymcui/Chinese-Minority-PLM</a><br>[2] <a href=\"https://dumps.wikimedia.org/zh_yuewiki/\">https://dumps.wikimedia.org/zh_yuewiki/</a><br>[3] <a href=\"https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\">https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations</a><br>[4] <a href=\"https://hkcc.eduhk.hk/v1/introduction.html\">https://hkcc.eduhk.hk/v1/introduction.html</a><br>[5] <a href=\"https://radimrehurek.com/gensim/apiref.html#api-reference\">https://radimrehurek.com/gensim/apiref.html#api-reference</a></p>\n","site":{"data":{}},"excerpt":"<p><br>","more":"</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>WikipediaWikipedia[2]20220501wiki88Mhuggingfacex-tech/cantonese-mandarin-translation[3][4]0.0</p>\n<ol>\n<li><a href=\"http://www.hkcna.hk/index.jsp?channel=2803\">http://www.hkcna.hk/index.jsp?channel=2803</a></li>\n<li><a href=\"https://weibo.com/u/6227634537?refer_flag=1005055013_\">https://weibo.com/u/6227634537?refer_flag=1005055013_</a></li>\n<li><a href=\"https://subanana.com/\">https://subanana.com/</a> AI</li>\n<li><a href=\"https://commonvoice.mozilla.org/zh-CN/datasets\">https://commonvoice.mozilla.org/zh-CN/datasets</a></li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<ol>\n<li><a href=\"https://app.diffbot.com/\">https://app.diffbot.com/</a></li>\n<li><a href=\"https://commoncrawl.org/the-data/get-started/\">https://commoncrawl.org/the-data/get-started/</a></li>\n<li><a href=\"https://webz.io/\">https://webz.io/</a></li>\n<li><a href=\"https://www.parsehub.com/\">https://www.parsehub.com/</a></li>\n<li><a href=\"https://www.scrapingbee.com/\">https://www.scrapingbee.com/</a></li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>wikiGENSIM[5]wikiwiki</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> gensim.corpora <span class=\"keyword\">import</span> WikiCorpus</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    output = <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;wiki.txt&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, encoding=<span class=\"string\">&#x27;utf8&#x27;</span>)</span><br><span class=\"line\">    wiki = WikiCorpus(<span class=\"string\">&quot;wiki-xxx.xml.bz2&quot;</span>, dictionary=&#123;&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> wiki.get_texts():</span><br><span class=\"line\">        output.write(<span class=\"string\">&quot; &quot;</span>.join(text) + <span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">    output.close()\t\t </span><br></pre></td></tr></table></figure>\n<p>wiki<a href=\"https://github.com/Quelisa/data_cleaner.git\">https://github.com/Quelisa/data_cleaner.git</a></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>big data</p>\n<p>CINO (Chinese minority PLM)[1]LaBSE,CINOLaBSE</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] <a href=\"https://github.com/ymcui/Chinese-Minority-PLM\">https://github.com/ymcui/Chinese-Minority-PLM</a><br>[2] <a href=\"https://dumps.wikimedia.org/zh_yuewiki/\">https://dumps.wikimedia.org/zh_yuewiki/</a><br>[3] <a href=\"https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\">https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations</a><br>[4] <a href=\"https://hkcc.eduhk.hk/v1/introduction.html\">https://hkcc.eduhk.hk/v1/introduction.html</a><br>[5] <a href=\"https://radimrehurek.com/gensim/apiref.html#api-reference\">https://radimrehurek.com/gensim/apiref.html#api-reference</a></p>"},{"title":"","date":"2021-11-15T08:40:46.000Z","mathjax":true,"_content":"\nGoogle2017TransformerBERTGPT~\n\n\n<!--more-->\n\n\n### ELMo\nCembeddingLSTMSOTA\n\n\n### BERTRoBERTaALBERTELECTRAXLNetMacBERT\n1. BETRTransformerencodeMLMNSPMLM\n2. RoBERTaBERTtokenbatchstepNSPMLM\n3. ALBERTBETRBERTNSP\n4. ELECTRABERTRTD[MASK]\n5. XLNetTransformer-XLtransformer[MASK]\n6. MacBERTBERT[MASK]-\n\n\n\n### DistilBERTTinyBERT\n\n\n\n\n1. DistilBERTTriple LossBERT-baseBERTBERTMLMMLM\n\n2. TinyBERT\n\n\n\n### Transformer-XLReformerLongformerBigBird\nTransformer$O(n^2)$trick\n\n\n1. Transformer-XL\n2. ReformerTransformerTransformer$QK$softmaxnTransformer\n3. Longformer4096\n4. BigBirdTransformer\n\n\n\n### BARTUniLMT5GPT-3\n\n\n\n1. BARTseq2seq tranformerBARTlossBARTNoiseencoderdecoderdeocoderdecoderencoderdecoderBARTfinetuneencoderdecoderBART\n2. UniLMattention maskseq2seqLMmasktokenLMmasktokenseq-to-seqs1masks1s1s2masks2s1\n3. T5NLUNLGPromptNLPText-to-Text\n4. GPT-3\n\n\n\n\n### Vision Transformer\nVision TransformerCVNLPTransformerembedding+encode+MLP headembeddingTransformertoken[num_token, token_dim][H, W, C]TransformerEmbedding\n\n\n\n### UNITERCLIPUNIMO\nTransformerimagenetNLPmaskCVNLP\n1. UNITERTransformerencoderembeddingUNITEREmbedderImage EmbedderFaster-RCNNROI feature7normalized top/left/bottom/right coordinates, width, height, and area.FCLNText EmbedderBERTsegmentTransformerEncoderMLMMRM(Mask Region Model, )ITMImage Text Match, WRAWord Region Alignment, \n2. CLIPzero-shot learning\n3. UNIMO\n\n","source":"_posts/pretrained-model.md","raw":"---\ntitle: \ndate: 2021-11-15 16:40:46\ntags:\n- PM\ncategories: \n- \nmathjax: true\n---\n\nGoogle2017TransformerBERTGPT~\n\n\n<!--more-->\n\n\n### ELMo\nCembeddingLSTMSOTA\n\n\n### BERTRoBERTaALBERTELECTRAXLNetMacBERT\n1. BETRTransformerencodeMLMNSPMLM\n2. RoBERTaBERTtokenbatchstepNSPMLM\n3. ALBERTBETRBERTNSP\n4. ELECTRABERTRTD[MASK]\n5. XLNetTransformer-XLtransformer[MASK]\n6. MacBERTBERT[MASK]-\n\n\n\n### DistilBERTTinyBERT\n\n\n\n\n1. DistilBERTTriple LossBERT-baseBERTBERTMLMMLM\n\n2. TinyBERT\n\n\n\n### Transformer-XLReformerLongformerBigBird\nTransformer$O(n^2)$trick\n\n\n1. Transformer-XL\n2. ReformerTransformerTransformer$QK$softmaxnTransformer\n3. Longformer4096\n4. BigBirdTransformer\n\n\n\n### BARTUniLMT5GPT-3\n\n\n\n1. BARTseq2seq tranformerBARTlossBARTNoiseencoderdecoderdeocoderdecoderencoderdecoderBARTfinetuneencoderdecoderBART\n2. UniLMattention maskseq2seqLMmasktokenLMmasktokenseq-to-seqs1masks1s1s2masks2s1\n3. T5NLUNLGPromptNLPText-to-Text\n4. GPT-3\n\n\n\n\n### Vision Transformer\nVision TransformerCVNLPTransformerembedding+encode+MLP headembeddingTransformertoken[num_token, token_dim][H, W, C]TransformerEmbedding\n\n\n\n### UNITERCLIPUNIMO\nTransformerimagenetNLPmaskCVNLP\n1. UNITERTransformerencoderembeddingUNITEREmbedderImage EmbedderFaster-RCNNROI feature7normalized top/left/bottom/right coordinates, width, height, and area.FCLNText EmbedderBERTsegmentTransformerEncoderMLMMRM(Mask Region Model, )ITMImage Text Match, WRAWord Region Alignment, \n2. CLIPzero-shot learning\n3. UNIMO\n\n","slug":"pretrained-model","published":1,"updated":"2022-06-13T12:58:35.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6t000dq4ux7tb487v4","content":"<p>Google2017TransformerBERTGPT~</p>\n<span id=\"more\"></span>\n<h3 id=\"ELMo\"><a href=\"#ELMo\" class=\"headerlink\" title=\"ELMo\"></a>ELMo</h3><p>CembeddingLSTMSOTA</p>\n<h3 id=\"BERTRoBERTaALBERTELECTRAXLNetMacBERT\"><a href=\"#BERTRoBERTaALBERTELECTRAXLNetMacBERT\" class=\"headerlink\" title=\"BERTRoBERTaALBERTELECTRAXLNetMacBERT\"></a>BERTRoBERTaALBERTELECTRAXLNetMacBERT</h3><ol>\n<li>BETRTransformerencodeMLMNSPMLM</li>\n<li>RoBERTaBERTtokenbatchstepNSPMLM</li>\n<li>ALBERTBETRBERTNSP</li>\n<li>ELECTRABERTRTD[MASK]</li>\n<li>XLNetTransformer-XLtransformer[MASK]</li>\n<li>MacBERTBERT[MASK]-</li>\n</ol>\n<h3 id=\"DistilBERTTinyBERT\"><a href=\"#DistilBERTTinyBERT\" class=\"headerlink\" title=\"DistilBERTTinyBERT\"></a>DistilBERTTinyBERT</h3><p></p>\n<ol>\n<li><p>DistilBERTTriple LossBERT-baseBERTBERTMLMMLM</p>\n</li>\n<li><p>TinyBERT</p>\n</li>\n</ol>\n<h3 id=\"Transformer-XLReformerLongformerBigBird\"><a href=\"#Transformer-XLReformerLongformerBigBird\" class=\"headerlink\" title=\"Transformer-XLReformerLongformerBigBird\"></a>Transformer-XLReformerLongformerBigBird</h3><p>Transformer$O(n^2)$trick</p>\n<ol>\n<li>Transformer-XL</li>\n<li>ReformerTransformerTransformer$QK$softmaxnTransformer</li>\n<li>Longformer4096</li>\n<li>BigBirdTransformer</li>\n</ol>\n<h3 id=\"BARTUniLMT5GPT-3\"><a href=\"#BARTUniLMT5GPT-3\" class=\"headerlink\" title=\"BARTUniLMT5GPT-3\"></a>BARTUniLMT5GPT-3</h3><p></p>\n<ol>\n<li>BARTseq2seq tranformerBARTlossBARTNoiseencoderdecoderdeocoderdecoderencoderdecoderBARTfinetuneencoderdecoderBART</li>\n<li>UniLMattention maskseq2seqLMmasktokenLMmasktokenseq-to-seqs1masks1s1s2masks2s1</li>\n<li>T5NLUNLGPromptNLPText-to-Text</li>\n<li>GPT-3</li>\n</ol>\n<h3 id=\"Vision-Transformer\"><a href=\"#Vision-Transformer\" class=\"headerlink\" title=\"Vision Transformer\"></a>Vision Transformer</h3><p>Vision TransformerCVNLPTransformerembedding+encode+MLP headembeddingTransformertoken[num_token, token_dim][H, W, C]TransformerEmbedding</p>\n<h3 id=\"UNITERCLIPUNIMO\"><a href=\"#UNITERCLIPUNIMO\" class=\"headerlink\" title=\"UNITERCLIPUNIMO\"></a>UNITERCLIPUNIMO</h3><p>TransformerimagenetNLPmaskCVNLP</p>\n<ol>\n<li>UNITERTransformerencoderembeddingUNITEREmbedderImage EmbedderFaster-RCNNROI feature7normalized top/left/bottom/right coordinates, width, height, and area.FCLNText EmbedderBERTsegmentTransformerEncoderMLMMRM(Mask Region Model, )ITMImage Text Match, WRAWord Region Alignment, </li>\n<li>CLIPzero-shot learning</li>\n<li>UNIMO</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>Google2017TransformerBERTGPT~</p>","more":"<h3 id=\"ELMo\"><a href=\"#ELMo\" class=\"headerlink\" title=\"ELMo\"></a>ELMo</h3><p>CembeddingLSTMSOTA</p>\n<h3 id=\"BERTRoBERTaALBERTELECTRAXLNetMacBERT\"><a href=\"#BERTRoBERTaALBERTELECTRAXLNetMacBERT\" class=\"headerlink\" title=\"BERTRoBERTaALBERTELECTRAXLNetMacBERT\"></a>BERTRoBERTaALBERTELECTRAXLNetMacBERT</h3><ol>\n<li>BETRTransformerencodeMLMNSPMLM</li>\n<li>RoBERTaBERTtokenbatchstepNSPMLM</li>\n<li>ALBERTBETRBERTNSP</li>\n<li>ELECTRABERTRTD[MASK]</li>\n<li>XLNetTransformer-XLtransformer[MASK]</li>\n<li>MacBERTBERT[MASK]-</li>\n</ol>\n<h3 id=\"DistilBERTTinyBERT\"><a href=\"#DistilBERTTinyBERT\" class=\"headerlink\" title=\"DistilBERTTinyBERT\"></a>DistilBERTTinyBERT</h3><p></p>\n<ol>\n<li><p>DistilBERTTriple LossBERT-baseBERTBERTMLMMLM</p>\n</li>\n<li><p>TinyBERT</p>\n</li>\n</ol>\n<h3 id=\"Transformer-XLReformerLongformerBigBird\"><a href=\"#Transformer-XLReformerLongformerBigBird\" class=\"headerlink\" title=\"Transformer-XLReformerLongformerBigBird\"></a>Transformer-XLReformerLongformerBigBird</h3><p>Transformer$O(n^2)$trick</p>\n<ol>\n<li>Transformer-XL</li>\n<li>ReformerTransformerTransformer$QK$softmaxnTransformer</li>\n<li>Longformer4096</li>\n<li>BigBirdTransformer</li>\n</ol>\n<h3 id=\"BARTUniLMT5GPT-3\"><a href=\"#BARTUniLMT5GPT-3\" class=\"headerlink\" title=\"BARTUniLMT5GPT-3\"></a>BARTUniLMT5GPT-3</h3><p></p>\n<ol>\n<li>BARTseq2seq tranformerBARTlossBARTNoiseencoderdecoderdeocoderdecoderencoderdecoderBARTfinetuneencoderdecoderBART</li>\n<li>UniLMattention maskseq2seqLMmasktokenLMmasktokenseq-to-seqs1masks1s1s2masks2s1</li>\n<li>T5NLUNLGPromptNLPText-to-Text</li>\n<li>GPT-3</li>\n</ol>\n<h3 id=\"Vision-Transformer\"><a href=\"#Vision-Transformer\" class=\"headerlink\" title=\"Vision Transformer\"></a>Vision Transformer</h3><p>Vision TransformerCVNLPTransformerembedding+encode+MLP headembeddingTransformertoken[num_token, token_dim][H, W, C]TransformerEmbedding</p>\n<h3 id=\"UNITERCLIPUNIMO\"><a href=\"#UNITERCLIPUNIMO\" class=\"headerlink\" title=\"UNITERCLIPUNIMO\"></a>UNITERCLIPUNIMO</h3><p>TransformerimagenetNLPmaskCVNLP</p>\n<ol>\n<li>UNITERTransformerencoderembeddingUNITEREmbedderImage EmbedderFaster-RCNNROI feature7normalized top/left/bottom/right coordinates, width, height, and area.FCLNText EmbedderBERTsegmentTransformerEncoderMLMMRM(Mask Region Model, )ITMImage Text Match, WRAWord Region Alignment, </li>\n<li>CLIPzero-shot learning</li>\n<li>UNIMO</li>\n</ol>"},{"title":"RoFormer","date":"2022-05-06T07:51:19.000Z","mathjax":true,"_content":"\n\nRoFormerV2[1]RoFormerRotary Position EmbeddingRoPE[2]RoFormer\n<!--more-->\n\n~\n\n\n### \nTransformer[1]idattentiontransformer512512*dimattentionRoFormerRoFoermerRoPE\n\n\n### RoPE\nRoPEattentionqkmn$$f(q,m)$$$$f(k,n)$$(m-n)$$g(q,k,m-n)$$<f(q,m), f(k,n)> = g(q,k,m-n)$$$$f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}$$RoPERoPESinusoidal[3]RoPE\n<img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\">\n\n\nRoPEAttentionattentionRoFormer[4]RoPERoFormer\n\n\n### RoFormerV2\n[5]RoFormerV2RoformerV2RoFormerV2RoFormerV2GLUERoFormerV2bias30G280GRoFormerV2\n\n\n### \n[1] https://github.com/ZhuiyiTechnology/roformer-v2\n[2] https://spaces.ac.cn/archives/8265\n[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.\n[5] https://kexue.fm/archives/8998","source":"_posts/roformer.md","raw":"---\ntitle: RoFormer\ndate: 2022-05-06 15:51:19\ntags:\n- PLM\ncategories: \n- \nmathjax: true\n---\n\n\nRoFormerV2[1]RoFormerRotary Position EmbeddingRoPE[2]RoFormer\n<!--more-->\n\n~\n\n\n### \nTransformer[1]idattentiontransformer512512*dimattentionRoFormerRoFoermerRoPE\n\n\n### RoPE\nRoPEattentionqkmn$$f(q,m)$$$$f(k,n)$$(m-n)$$g(q,k,m-n)$$<f(q,m), f(k,n)> = g(q,k,m-n)$$$$f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}$$RoPERoPESinusoidal[3]RoPE\n<img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\">\n\n\nRoPEAttentionattentionRoFormer[4]RoPERoFormer\n\n\n### RoFormerV2\n[5]RoFormerV2RoformerV2RoFormerV2RoFormerV2GLUERoFormerV2bias30G280GRoFormerV2\n\n\n### \n[1] https://github.com/ZhuiyiTechnology/roformer-v2\n[2] https://spaces.ac.cn/archives/8265\n[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.\n[5] https://kexue.fm/archives/8998","slug":"roformer","published":1,"updated":"2022-06-24T09:38:19.291Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6z000hq4uxafn90o86","content":"<p>RoFormerV2[1]RoFormerRotary Position EmbeddingRoPE[2]RoFormer<br><span id=\"more\"></span></p>\n<p>~</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Transformer[1]idattentiontransformer512512*dimattentionRoFormerRoFoermerRoPE</p>\n<h3 id=\"RoPE\"><a href=\"#RoPE\" class=\"headerlink\" title=\"RoPE\"></a>RoPE</h3><p>RoPEattentionqkmn<script type=\"math/tex\">f(q,m)</script><script type=\"math/tex\">f(k,n)</script>(m-n)<script type=\"math/tex\">g(q,k,m-n)$$<f(q,m), f(k,n)> = g(q,k,m-n)</script><script type=\"math/tex\">f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}</script>RoPERoPESinusoidal[3]RoPE<br><img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\"></p>\n<p>RoPEAttentionattentionRoFormer[4]RoPERoFormer</p>\n<h3 id=\"RoFormerV2\"><a href=\"#RoFormerV2\" class=\"headerlink\" title=\"RoFormerV2\"></a>RoFormerV2</h3><p>[5]RoFormerV2RoformerV2RoFormerV2RoFormerV2GLUERoFormerV2bias30G280GRoFormerV2</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] <a href=\"https://github.com/ZhuiyiTechnology/roformer-v2\">https://github.com/ZhuiyiTechnology/roformer-v2</a><br>[2] <a href=\"https://spaces.ac.cn/archives/8265\">https://spaces.ac.cn/archives/8265</a><br>[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.<br>[5] <a href=\"https://kexue.fm/archives/8998\">https://kexue.fm/archives/8998</a></p>\n","site":{"data":{}},"excerpt":"<p>RoFormerV2[1]RoFormerRotary Position EmbeddingRoPE[2]RoFormer<br>","more":"</p>\n<p>~</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Transformer[1]idattentiontransformer512512*dimattentionRoFormerRoFoermerRoPE</p>\n<h3 id=\"RoPE\"><a href=\"#RoPE\" class=\"headerlink\" title=\"RoPE\"></a>RoPE</h3><p>RoPEattentionqkmn<script type=\"math/tex\">f(q,m)</script><script type=\"math/tex\">f(k,n)</script>(m-n)<script type=\"math/tex\">g(q,k,m-n)$$<f(q,m), f(k,n)> = g(q,k,m-n)</script><script type=\"math/tex\">f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}</script>RoPERoPESinusoidal[3]RoPE<br><img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\"></p>\n<p>RoPEAttentionattentionRoFormer[4]RoPERoFormer</p>\n<h3 id=\"RoFormerV2\"><a href=\"#RoFormerV2\" class=\"headerlink\" title=\"RoFormerV2\"></a>RoFormerV2</h3><p>[5]RoFormerV2RoformerV2RoFormerV2RoFormerV2GLUERoFormerV2bias30G280GRoFormerV2</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] <a href=\"https://github.com/ZhuiyiTechnology/roformer-v2\">https://github.com/ZhuiyiTechnology/roformer-v2</a><br>[2] <a href=\"https://spaces.ac.cn/archives/8265\">https://spaces.ac.cn/archives/8265</a><br>[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.<br>[5] <a href=\"https://kexue.fm/archives/8998\">https://kexue.fm/archives/8998</a></p>"},{"title":"","date":"2022-01-15T08:41:14.000Z","mathjax":true,"_content":"\nsentence embeddingsentence embeddingsentence-transformer[1]\n\n<!--more-->\n\n\n### \nBERT[CLS]\n\n\n#### Sentence-BERT[2]\nBERTSentence-BERTSBERTembeddingembeddingBERT\n\nSBERTTriplet loss\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\">\n\n#### BERT-flow[3]  BERT-whitening[4]\nBERT-flowBERTfine-tuneBERTGloVeBERTBERTBERTBERTBERTBERTEmbeddingsflowBERT-flow\n\n\nBERT-whiteningBERTflowcosinecosineBERTflowflow0\n\n\n#### Constrastive Tension[5]\nConstrastive TensionCTSTStransformer\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\">\nCT\n\n\n#### SimCSE[6]\nSimCSEYYDSSimCSESimCSEInfoNCE lossdropoutdropout0batch\n\n\nSimCSE\n\n\n#### TSDAE[7]\nTSDAETransformer\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\">\nencoderdecoderTSDAECTSimCSEBERT-flow\n\n\n\n#### LaBSE[8]\nLaBSE109BERT\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\">\n\nlossInfoNCE lossmarginLaBSEnTPUbatch8batchLaBSE\n\n\n### \nBERTBERTBERTBERT\n\n### \n[1] https://www.sbert.net/\n[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.\n[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.\n[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.\n[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.\n[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.\n[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.","source":"_posts/sentence-transformers.md","raw":"---\ntitle: \ndate: 2022-01-15 16:41:14\ntags:\n- \n- Representation Learning\ncategories: \n- Sentence Embedding\nmathjax: true\n---\n\nsentence embeddingsentence embeddingsentence-transformer[1]\n\n<!--more-->\n\n\n### \nBERT[CLS]\n\n\n#### Sentence-BERT[2]\nBERTSentence-BERTSBERTembeddingembeddingBERT\n\nSBERTTriplet loss\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\">\n\n#### BERT-flow[3]  BERT-whitening[4]\nBERT-flowBERTfine-tuneBERTGloVeBERTBERTBERTBERTBERTBERTEmbeddingsflowBERT-flow\n\n\nBERT-whiteningBERTflowcosinecosineBERTflowflow0\n\n\n#### Constrastive Tension[5]\nConstrastive TensionCTSTStransformer\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\">\nCT\n\n\n#### SimCSE[6]\nSimCSEYYDSSimCSESimCSEInfoNCE lossdropoutdropout0batch\n\n\nSimCSE\n\n\n#### TSDAE[7]\nTSDAETransformer\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\">\nencoderdecoderTSDAECTSimCSEBERT-flow\n\n\n\n#### LaBSE[8]\nLaBSE109BERT\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\">\n\nlossInfoNCE lossmarginLaBSEnTPUbatch8batchLaBSE\n\n\n### \nBERTBERTBERTBERT\n\n### \n[1] https://www.sbert.net/\n[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.\n[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.\n[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.\n[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.\n[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.\n[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.","slug":"sentence-transformers","published":1,"updated":"2022-06-07T07:07:09.539Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga72000jq4ux7xq4fllg","content":"<p>sentence embeddingsentence embeddingsentence-transformer[1]</p>\n<span id=\"more\"></span>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>BERT[CLS]</p>\n<h4 id=\"Sentence-BERT-2\"><a href=\"#Sentence-BERT-2\" class=\"headerlink\" title=\"Sentence-BERT[2]\"></a>Sentence-BERT[2]</h4><p>BERTSentence-BERTSBERTembeddingembeddingBERT</p>\n<p>SBERTTriplet loss<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\"></p>\n<h4 id=\"BERT-flow-3--BERT-whitening-4\"><a href=\"#BERT-flow-3--BERT-whitening-4\" class=\"headerlink\" title=\"BERT-flow[3]  BERT-whitening[4]\"></a>BERT-flow[3]  BERT-whitening[4]</h4><p>BERT-flowBERTfine-tuneBERTGloVeBERTBERTBERTBERTBERTBERTEmbeddingsflowBERT-flow</p>\n<p>BERT-whiteningBERTflowcosinecosineBERTflowflow0</p>\n<h4 id=\"Constrastive-Tension-5\"><a href=\"#Constrastive-Tension-5\" class=\"headerlink\" title=\"Constrastive Tension[5]\"></a>Constrastive Tension[5]</h4><p>Constrastive TensionCTSTStransformer<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\"><br>CT</p>\n<h4 id=\"SimCSE-6\"><a href=\"#SimCSE-6\" class=\"headerlink\" title=\"SimCSE[6]\"></a>SimCSE[6]</h4><p>SimCSEYYDSSimCSESimCSEInfoNCE lossdropoutdropout0batch</p>\n<p>SimCSE</p>\n<h4 id=\"TSDAE-7\"><a href=\"#TSDAE-7\" class=\"headerlink\" title=\"TSDAE[7]\"></a>TSDAE[7]</h4><p>TSDAETransformer<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\"><br>encoderdecoderTSDAECTSimCSEBERT-flow</p>\n<h4 id=\"LaBSE-8\"><a href=\"#LaBSE-8\" class=\"headerlink\" title=\"LaBSE[8]\"></a>LaBSE[8]</h4><p>LaBSE109BERT<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\"></p>\n<p>lossInfoNCE lossmarginLaBSEnTPUbatch8batchLaBSE</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>BERTBERTBERTBERT</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] <a href=\"https://www.sbert.net/\">https://www.sbert.net/</a><br>[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.<br>[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.<br>[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.<br>[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.<br>[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.<br>[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.</p>\n","site":{"data":{}},"excerpt":"<p>sentence embeddingsentence embeddingsentence-transformer[1]</p>","more":"<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>BERT[CLS]</p>\n<h4 id=\"Sentence-BERT-2\"><a href=\"#Sentence-BERT-2\" class=\"headerlink\" title=\"Sentence-BERT[2]\"></a>Sentence-BERT[2]</h4><p>BERTSentence-BERTSBERTembeddingembeddingBERT</p>\n<p>SBERTTriplet loss<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\"></p>\n<h4 id=\"BERT-flow-3--BERT-whitening-4\"><a href=\"#BERT-flow-3--BERT-whitening-4\" class=\"headerlink\" title=\"BERT-flow[3]  BERT-whitening[4]\"></a>BERT-flow[3]  BERT-whitening[4]</h4><p>BERT-flowBERTfine-tuneBERTGloVeBERTBERTBERTBERTBERTBERTEmbeddingsflowBERT-flow</p>\n<p>BERT-whiteningBERTflowcosinecosineBERTflowflow0</p>\n<h4 id=\"Constrastive-Tension-5\"><a href=\"#Constrastive-Tension-5\" class=\"headerlink\" title=\"Constrastive Tension[5]\"></a>Constrastive Tension[5]</h4><p>Constrastive TensionCTSTStransformer<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\"><br>CT</p>\n<h4 id=\"SimCSE-6\"><a href=\"#SimCSE-6\" class=\"headerlink\" title=\"SimCSE[6]\"></a>SimCSE[6]</h4><p>SimCSEYYDSSimCSESimCSEInfoNCE lossdropoutdropout0batch</p>\n<p>SimCSE</p>\n<h4 id=\"TSDAE-7\"><a href=\"#TSDAE-7\" class=\"headerlink\" title=\"TSDAE[7]\"></a>TSDAE[7]</h4><p>TSDAETransformer<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\"><br>encoderdecoderTSDAECTSimCSEBERT-flow</p>\n<h4 id=\"LaBSE-8\"><a href=\"#LaBSE-8\" class=\"headerlink\" title=\"LaBSE[8]\"></a>LaBSE[8]</h4><p>LaBSE109BERT<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\"></p>\n<p>lossInfoNCE lossmarginLaBSEnTPUbatch8batchLaBSE</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>BERTBERTBERTBERT</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] <a href=\"https://www.sbert.net/\">https://www.sbert.net/</a><br>[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.<br>[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.<br>[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.<br>[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.<br>[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.<br>[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.</p>"},{"title":"mention","date":"2021-01-06T08:40:04.000Z","_content":"\nmention\n<!--more-->\n\n\n### mention\nmentionmentionmention2021512mention2021512\n\n\n### \ndatetimeduration\n1. date202151\n2. time1258\n3. duration52\n\n\n### mention\n1. \n2. 22928\n3. \n4. 3153\n\n\n### mention\nmentionmentionmention\n\n(1) 1  \nuser2   \nbot 20215614:00:00  \n\n(2) 2  \nbot:   \nuser:   \nbot: 202158  \n\n(3) 3  \nbot:   \nuser: 1011  \nbot: 20215610:00:00-20215611:30:00  \n\n(4) 4  \nbot:   \nuser: 2021562021512  \nbot: 202156-2021512  \n\n(5) 5  \nuser: 15  \nbot: 20215514:15:00  \n\n(6) 6  \nbot:   \nuser:   \nbot: 202158  \n\n(7) 7  \nbot  \nuser  \nbot: 20215600:00:00-20216600:00:00  \n\n(8) 8  \nbot:   \nuser: 10  \nbot: 10:00:00  \n\n(9) 9  \nbot:   \nuser:   \nbot: keep  \n\n\nmentionmention\n\n1. mentiontimedateduration\n2. mentioncycle_timecycle_datecycle_duration\n3. mention+shift_time+shift_date+shift_duration\n4. mention+cycle_shift_time+cycle_shift_date+cycle_shift_duration\n\n\n### \n\nmentionmentionmentiondatetimeduration202156date202156202157\n\n### TODO\n\n\n\n","source":"_posts/time-mention.md","raw":"---\ntitle: mention\ndate: 2021-01-06 16:40:04\ntags:\n- mention\n- \ncategories: \n- \n---\n\nmention\n<!--more-->\n\n\n### mention\nmentionmentionmention2021512mention2021512\n\n\n### \ndatetimeduration\n1. date202151\n2. time1258\n3. duration52\n\n\n### mention\n1. \n2. 22928\n3. \n4. 3153\n\n\n### mention\nmentionmentionmention\n\n(1) 1  \nuser2   \nbot 20215614:00:00  \n\n(2) 2  \nbot:   \nuser:   \nbot: 202158  \n\n(3) 3  \nbot:   \nuser: 1011  \nbot: 20215610:00:00-20215611:30:00  \n\n(4) 4  \nbot:   \nuser: 2021562021512  \nbot: 202156-2021512  \n\n(5) 5  \nuser: 15  \nbot: 20215514:15:00  \n\n(6) 6  \nbot:   \nuser:   \nbot: 202158  \n\n(7) 7  \nbot  \nuser  \nbot: 20215600:00:00-20216600:00:00  \n\n(8) 8  \nbot:   \nuser: 10  \nbot: 10:00:00  \n\n(9) 9  \nbot:   \nuser:   \nbot: keep  \n\n\nmentionmention\n\n1. mentiontimedateduration\n2. mentioncycle_timecycle_datecycle_duration\n3. mention+shift_time+shift_date+shift_duration\n4. mention+cycle_shift_time+cycle_shift_date+cycle_shift_duration\n\n\n### \n\nmentionmentionmentiondatetimeduration202156date202156202157\n\n### TODO\n\n\n\n","slug":"time-mention","published":1,"updated":"2022-06-09T12:41:31.190Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga77000oq4ux8dnb8802","content":"<p>mention<br><span id=\"more\"></span></p>\n<h3 id=\"mention\"><a href=\"#mention\" class=\"headerlink\" title=\"mention\"></a>mention</h3><p>mentionmentionmention2021512mention2021512</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>datetimeduration</p>\n<ol>\n<li>date202151</li>\n<li>time1258</li>\n<li>duration52</li>\n</ol>\n<h3 id=\"mention\"><a href=\"#mention\" class=\"headerlink\" title=\"mention\"></a>mention</h3><ol>\n<li></li>\n<li>22928</li>\n<li></li>\n<li>3153</li>\n</ol>\n<h3 id=\"mention\"><a href=\"#mention\" class=\"headerlink\" title=\"mention\"></a>mention</h3><p>mentionmentionmention</p>\n<p>(1) 1<br>user2<br>bot 20215614:00:00  </p>\n<p>(2) 2<br>bot: <br>user: <br>bot: 202158  </p>\n<p>(3) 3<br>bot: <br>user: 1011<br>bot: 20215610:00:00-20215611:30:00  </p>\n<p>(4) 4<br>bot: <br>user: 2021562021512<br>bot: 202156-2021512  </p>\n<p>(5) 5<br>user: 15<br>bot: 20215514:15:00  </p>\n<p>(6) 6<br>bot: <br>user: <br>bot: 202158  </p>\n<p>(7) 7<br>bot<br>user<br>bot: 20215600:00:00-20216600:00:00  </p>\n<p>(8) 8<br>bot: <br>user: 10<br>bot: 10:00:00  </p>\n<p>(9) 9<br>bot: <br>user: <br>bot: keep  </p>\n<p>mentionmention</p>\n<ol>\n<li>mentiontimedateduration</li>\n<li>mentioncycle_timecycle_datecycle_duration</li>\n<li>mention+shift_time+shift_date+shift_duration</li>\n<li>mention+cycle_shift_time+cycle_shift_date+cycle_shift_duration</li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>mentionmentionmentiondatetimeduration202156date202156202157</p>\n<h3 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h3><p></p>\n","site":{"data":{}},"excerpt":"<p>mention<br>","more":"</p>\n<h3 id=\"mention\"><a href=\"#mention\" class=\"headerlink\" title=\"mention\"></a>mention</h3><p>mentionmentionmention2021512mention2021512</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>datetimeduration</p>\n<ol>\n<li>date202151</li>\n<li>time1258</li>\n<li>duration52</li>\n</ol>\n<h3 id=\"mention\"><a href=\"#mention\" class=\"headerlink\" title=\"mention\"></a>mention</h3><ol>\n<li></li>\n<li>22928</li>\n<li></li>\n<li>3153</li>\n</ol>\n<h3 id=\"mention\"><a href=\"#mention\" class=\"headerlink\" title=\"mention\"></a>mention</h3><p>mentionmentionmention</p>\n<p>(1) 1<br>user2<br>bot 20215614:00:00  </p>\n<p>(2) 2<br>bot: <br>user: <br>bot: 202158  </p>\n<p>(3) 3<br>bot: <br>user: 1011<br>bot: 20215610:00:00-20215611:30:00  </p>\n<p>(4) 4<br>bot: <br>user: 2021562021512<br>bot: 202156-2021512  </p>\n<p>(5) 5<br>user: 15<br>bot: 20215514:15:00  </p>\n<p>(6) 6<br>bot: <br>user: <br>bot: 202158  </p>\n<p>(7) 7<br>bot<br>user<br>bot: 20215600:00:00-20216600:00:00  </p>\n<p>(8) 8<br>bot: <br>user: 10<br>bot: 10:00:00  </p>\n<p>(9) 9<br>bot: <br>user: <br>bot: keep  </p>\n<p>mentionmention</p>\n<ol>\n<li>mentiontimedateduration</li>\n<li>mentioncycle_timecycle_datecycle_duration</li>\n<li>mention+shift_time+shift_date+shift_duration</li>\n<li>mention+cycle_shift_time+cycle_shift_date+cycle_shift_duration</li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>mentionmentionmentiondatetimeduration202156date202156202157</p>\n<h3 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h3><p></p>"},{"title":"Word2vec & GloVe","date":"2021-10-19T08:42:15.000Z","mathjax":true,"_content":"\nWord2vecMikolov2013GloVeWord2vecBERT\n\n<!--more-->\n\n### \none-hotone-hotWord2vec[1]GloVe[2]\n\n### Word2vec\nWord2vecword2vecCBOWSkip-gram\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\">\n\n\n\n\n#### CBOW\nthemanloveshissonloves2themanhimsonloves\n\n$$P(``love\"|``the\",``man\",``his\",``son\")$$\n\n\nCBOW\n1. 5$w_t$$\\mathbb{V}$\n2. $E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$\n$$ \\pmb{v}_{w_i}=E \\pmb{e}_{w_i} $$\n$$w_i$$$$E$$$$E$$$$\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$$$w_t$$$$\\mathcal{c}_t$$$$w_t$$\n$$\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m$$\n3. $$E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }$$$$\\pmb{v}_{w_i}^{'}$$$$E'$$$$w_i$$$$w_t$$\n$$P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}$$\n\n\n\nCBOW$E$$E'$\n\n\n\n#### Skip-gram\nSkip-gramCBOW$P(w_{t+j}|w_t)$$$j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}$$$k=2$\n1. $w_t$$E$\n2. $$w_t$$$$\\pmb{v}_{w_t}=E^T_{w_t}$$;\n3. $$\\pmb{v}_{w_t}$$$$E'$$\n$$P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }$$\n$$c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$\n\n\n\n#### \n\nCBOWSkip-gram$\\theta = \\{E, {E'}\\}$$T$$w_1 w_2 \\cdots w_T$CBOW\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)$$\n\n\n$$\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}$$\n\nSkip-gram\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t) $$\n\n\n[3]$P(D=1|w,c)$$c$$w$\n\n$$P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\n\n\n$$P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\nCBOW$$\\{w_t,c\\}$$$$w_t$$$$c$$lossSkip-gram$$\\{w_t,w_{t+j}\\}$$K$w_t$\n\n\n\n### GloVe\nGloVeword2vec\n\n\nGlove-$M$$M_{w,c}$$w$$c$GloVeM$w$$c$$(w,c)$\n\n$$M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}$$\n\n$d_{i}(w,c)$$i$$w$$c$$W$$M$\n\n$$\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}$$\n\n$$\\pmb{v}_w^T$$$$\\pmb{v}_c^{'}$$$w$$c$$b_w$$b_c$Glove\n\n$$\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2$$\n\n\n$f(M_{w,c})$$(w,c)$Glove\n\n$$f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}$$\n\n\n$$M_{w,c}$$$$\\theta$$$$f(W_{w,c})$$$$M_{w,c}$$11$$\\alpha$$\n\n\n### \n[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.\n[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.","source":"_posts/word2vec.md","raw":"---\ntitle: Word2vec & GloVe\ndate: 2021-10-19 16:42:15\ntags:\n- \n- \ncategories: \n- \nmathjax: true\n---\n\nWord2vecMikolov2013GloVeWord2vecBERT\n\n<!--more-->\n\n### \none-hotone-hotWord2vec[1]GloVe[2]\n\n### Word2vec\nWord2vecword2vecCBOWSkip-gram\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\">\n\n\n\n\n#### CBOW\nthemanloveshissonloves2themanhimsonloves\n\n$$P(``love\"|``the\",``man\",``his\",``son\")$$\n\n\nCBOW\n1. 5$w_t$$\\mathbb{V}$\n2. $E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$\n$$ \\pmb{v}_{w_i}=E \\pmb{e}_{w_i} $$\n$$w_i$$$$E$$$$E$$$$\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$$$w_t$$$$\\mathcal{c}_t$$$$w_t$$\n$$\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m$$\n3. $$E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }$$$$\\pmb{v}_{w_i}^{'}$$$$E'$$$$w_i$$$$w_t$$\n$$P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}$$\n\n\n\nCBOW$E$$E'$\n\n\n\n#### Skip-gram\nSkip-gramCBOW$P(w_{t+j}|w_t)$$$j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}$$$k=2$\n1. $w_t$$E$\n2. $$w_t$$$$\\pmb{v}_{w_t}=E^T_{w_t}$$;\n3. $$\\pmb{v}_{w_t}$$$$E'$$\n$$P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }$$\n$$c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$\n\n\n\n#### \n\nCBOWSkip-gram$\\theta = \\{E, {E'}\\}$$T$$w_1 w_2 \\cdots w_T$CBOW\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)$$\n\n\n$$\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}$$\n\nSkip-gram\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t) $$\n\n\n[3]$P(D=1|w,c)$$c$$w$\n\n$$P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\n\n\n$$P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\nCBOW$$\\{w_t,c\\}$$$$w_t$$$$c$$lossSkip-gram$$\\{w_t,w_{t+j}\\}$$K$w_t$\n\n\n\n### GloVe\nGloVeword2vec\n\n\nGlove-$M$$M_{w,c}$$w$$c$GloVeM$w$$c$$(w,c)$\n\n$$M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}$$\n\n$d_{i}(w,c)$$i$$w$$c$$W$$M$\n\n$$\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}$$\n\n$$\\pmb{v}_w^T$$$$\\pmb{v}_c^{'}$$$w$$c$$b_w$$b_c$Glove\n\n$$\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2$$\n\n\n$f(M_{w,c})$$(w,c)$Glove\n\n$$f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}$$\n\n\n$$M_{w,c}$$$$\\theta$$$$f(W_{w,c})$$$$M_{w,c}$$11$$\\alpha$$\n\n\n### \n[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.\n[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.","slug":"word2vec","published":1,"updated":"2022-06-09T12:39:28.711Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga7a000qq4ux64mg6y51","content":"<p>Word2vecMikolov2013GloVeWord2vecBERT</p>\n<span id=\"more\"></span>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>one-hotone-hotWord2vec[1]GloVe[2]</p>\n<h3 id=\"Word2vec\"><a href=\"#Word2vec\" class=\"headerlink\" title=\"Word2vec\"></a>Word2vec</h3><p>Word2vecword2vecCBOWSkip-gram<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\"></p>\n<p></p>\n<h4 id=\"CBOW\"><a href=\"#CBOW\" class=\"headerlink\" title=\"CBOW\"></a>CBOW</h4><p>themanloveshissonloves2themanhimsonloves</p>\n<script type=\"math/tex; mode=display\">P(``love\"|``the\",``man\",``his\",``son\")</script><p>CBOW</p>\n<ol>\n<li>5$w_t$$\\mathbb{V}$</li>\n<li>$E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$<script type=\"math/tex; mode=display\">\\pmb{v}_{w_i}=E \\pmb{e}_{w_i}</script><script type=\"math/tex\">w_i</script><script type=\"math/tex\">E</script><script type=\"math/tex\">E</script><script type=\"math/tex\">\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script><script type=\"math/tex\">w_t</script><script type=\"math/tex\">\\mathcal{c}_t</script><script type=\"math/tex\">w_t</script><script type=\"math/tex; mode=display\">\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m</script></li>\n<li><script type=\"math/tex\">E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }</script><script type=\"math/tex\">\\pmb{v}_{w_i}^{'}</script><script type=\"math/tex\">E'</script><script type=\"math/tex\">w_i</script><script type=\"math/tex\">w_t</script><script type=\"math/tex; mode=display\">P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}</script></li>\n</ol>\n<p>CBOW$E$$E$</p>\n<h4 id=\"Skip-gram\"><a href=\"#Skip-gram\" class=\"headerlink\" title=\"Skip-gram\"></a>Skip-gram</h4><p>Skip-gramCBOW$P(w_{t+j}|w_t)$<script type=\"math/tex\">j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}</script>$k=2$</p>\n<ol>\n<li>$w_t$$E$</li>\n<li><script type=\"math/tex\">w_t</script><script type=\"math/tex\">\\pmb{v}_{w_t}=E^T_{w_t}</script>;</li>\n<li><script type=\"math/tex\">\\pmb{v}_{w_t}</script><script type=\"math/tex\">E'</script><script type=\"math/tex; mode=display\">P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }</script><script type=\"math/tex\">c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script></li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>CBOWSkip-gram$\\theta = {E, {E}}$$T$$w_1 w_2 \\cdots w_T$CBOW</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)</script><p><script type=\"math/tex\">\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}</script></p>\n<p>Skip-gram</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t)</script><p>[3]$P(D=1|w,c)$$c$$w$</p>\n<script type=\"math/tex; mode=display\">P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p></p>\n<script type=\"math/tex; mode=display\">P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p>CBOW<script type=\"math/tex\">\\{w_t,c\\}</script><script type=\"math/tex\">w_t</script><script type=\"math/tex\">c</script>lossSkip-gram<script type=\"math/tex\">\\{w_t,w_{t+j}\\}</script>K$w_t$</p>\n<h3 id=\"GloVe\"><a href=\"#GloVe\" class=\"headerlink\" title=\"GloVe\"></a>GloVe</h3><p>GloVeword2vec</p>\n<p>Glove-$M$$M_{w,c}$$w$$c$GloVeM$w$$c$$(w,c)$</p>\n<script type=\"math/tex; mode=display\">M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}</script><p>$d_{i}(w,c)$$i$$w$$c$$W$$M$</p>\n<script type=\"math/tex; mode=display\">\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}</script><p><script type=\"math/tex\">\\pmb{v}_w^T</script><script type=\"math/tex\">\\pmb{v}_c^{'}</script>$w$$c$$b_w$$b_c$Glove</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2</script><p>$f(M_{w,c})$$(w,c)$Glove</p>\n<script type=\"math/tex; mode=display\">f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}</script><p><script type=\"math/tex\">M_{w,c}</script><script type=\"math/tex\">\\theta</script><script type=\"math/tex\">f(W_{w,c})</script><script type=\"math/tex\">M_{w,c}</script>11<script type=\"math/tex\">\\alpha</script></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.<br>[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.<br>[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.</p>\n","site":{"data":{}},"excerpt":"<p>Word2vecMikolov2013GloVeWord2vecBERT</p>","more":"<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>one-hotone-hotWord2vec[1]GloVe[2]</p>\n<h3 id=\"Word2vec\"><a href=\"#Word2vec\" class=\"headerlink\" title=\"Word2vec\"></a>Word2vec</h3><p>Word2vecword2vecCBOWSkip-gram<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\"></p>\n<p></p>\n<h4 id=\"CBOW\"><a href=\"#CBOW\" class=\"headerlink\" title=\"CBOW\"></a>CBOW</h4><p>themanloveshissonloves2themanhimsonloves</p>\n<script type=\"math/tex; mode=display\">P(``love\"|``the\",``man\",``his\",``son\")</script><p>CBOW</p>\n<ol>\n<li>5$w_t$$\\mathbb{V}$</li>\n<li>$E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$<script type=\"math/tex; mode=display\">\\pmb{v}_{w_i}=E \\pmb{e}_{w_i}</script><script type=\"math/tex\">w_i</script><script type=\"math/tex\">E</script><script type=\"math/tex\">E</script><script type=\"math/tex\">\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script><script type=\"math/tex\">w_t</script><script type=\"math/tex\">\\mathcal{c}_t</script><script type=\"math/tex\">w_t</script><script type=\"math/tex; mode=display\">\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m</script></li>\n<li><script type=\"math/tex\">E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }</script><script type=\"math/tex\">\\pmb{v}_{w_i}^{'}</script><script type=\"math/tex\">E'</script><script type=\"math/tex\">w_i</script><script type=\"math/tex\">w_t</script><script type=\"math/tex; mode=display\">P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}</script></li>\n</ol>\n<p>CBOW$E$$E$</p>\n<h4 id=\"Skip-gram\"><a href=\"#Skip-gram\" class=\"headerlink\" title=\"Skip-gram\"></a>Skip-gram</h4><p>Skip-gramCBOW$P(w_{t+j}|w_t)$<script type=\"math/tex\">j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}</script>$k=2$</p>\n<ol>\n<li>$w_t$$E$</li>\n<li><script type=\"math/tex\">w_t</script><script type=\"math/tex\">\\pmb{v}_{w_t}=E^T_{w_t}</script>;</li>\n<li><script type=\"math/tex\">\\pmb{v}_{w_t}</script><script type=\"math/tex\">E'</script><script type=\"math/tex; mode=display\">P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }</script><script type=\"math/tex\">c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script></li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>CBOWSkip-gram$\\theta = {E, {E}}$$T$$w_1 w_2 \\cdots w_T$CBOW</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)</script><p><script type=\"math/tex\">\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}</script></p>\n<p>Skip-gram</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t)</script><p>[3]$P(D=1|w,c)$$c$$w$</p>\n<script type=\"math/tex; mode=display\">P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p></p>\n<script type=\"math/tex; mode=display\">P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p>CBOW<script type=\"math/tex\">\\{w_t,c\\}</script><script type=\"math/tex\">w_t</script><script type=\"math/tex\">c</script>lossSkip-gram<script type=\"math/tex\">\\{w_t,w_{t+j}\\}</script>K$w_t$</p>\n<h3 id=\"GloVe\"><a href=\"#GloVe\" class=\"headerlink\" title=\"GloVe\"></a>GloVe</h3><p>GloVeword2vec</p>\n<p>Glove-$M$$M_{w,c}$$w$$c$GloVeM$w$$c$$(w,c)$</p>\n<script type=\"math/tex; mode=display\">M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}</script><p>$d_{i}(w,c)$$i$$w$$c$$W$$M$</p>\n<script type=\"math/tex; mode=display\">\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}</script><p><script type=\"math/tex\">\\pmb{v}_w^T</script><script type=\"math/tex\">\\pmb{v}_c^{'}</script>$w$$c$$b_w$$b_c$Glove</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2</script><p>$f(M_{w,c})$$(w,c)$Glove</p>\n<script type=\"math/tex; mode=display\">f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}</script><p><script type=\"math/tex\">M_{w,c}</script><script type=\"math/tex\">\\theta</script><script type=\"math/tex\">f(W_{w,c})</script><script type=\"math/tex\">M_{w,c}</script>11<script type=\"math/tex\">\\alpha</script></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.<br>[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.<br>[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.</p>"},{"title":"BERT ","date":"2021-03-02T08:40:29.000Z","mathjax":true,"_content":"\nBERTNLPTransformer[1]BERT[2]CNNRNNBERTTransformerNLP\n<!--more-->\n\n\n### TransformerCNNRNN\nRNNCNNtransformer\n\n\nCNNRNNRNNCNNTransformerTransformerCNNRNNmulti-head attentionself-attentionTransformer\n\n\n#### (1) CNN[3]\nCNN\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\">\n\n\nCNN$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $\n\n$$ (f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)$$\n\n$ \\bf{x} $$f$$g$\n\n$$ (f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2) $$\n\n$f$$(a,b)$$g$$(i-a, j-b)$:\n\n$$ (f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3) $$\n\n\n\n\npaddingstride240  240105  5200  200RGBCNN(Transposed)/(Dilated/Atrous)(Deformable)\n\n\nCNN\n\n\nLeNetAlexNetVGGNiNGoogLeNetResNetDenseNet\n\n#### (2) RNN[3]\nnn$x_t$$t$$n-1$$n$,\n$$ P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4) $$\n\n\n$$h_{t-1}$$$$t-1$$$$x_t$$$$h_{t-1}$$$$t$$\n$$ h_{t} = f(x_t, h_{t-1}) \\qquad (5) $$\n\n\nRNN\n$t$$$X_t \\in \\mathbb{R}^{n \\times d}$$$$H_t \\in \\mathbb{R}^{n \\times h}$$$t$$$H_{t-1}$$$$W_{hh} \\in \\mathbb{R}^{h \\times n}$$\n\n\n$$H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6) $$\n\n5$t$\n\n$$O_t = H_t W_{hq} + b_q \\qquad (7) $$\n\n\n$$W_{xh} \\in \\mathbb{R}^{d \\times h}$$$$W_{hh} \\in \\mathbb{R}^{h \\times h}$$$$b_h \\in \\mathbb{R}^{1 \\times h}$$$$W_{qh} \\in \\mathbb{R}^{q \\times h}$$$$b_q \\in \\mathbb{R}^{1 \\times q}$$RNN\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\">\n\n\nRNNRNN\n\n\n#### (3) Transformer\n\n> 108[3]\n\n\n\n\nquerykeyvalue\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\">\n\n$\\pmb{q} \\in \\mathbb{R}^q$$m$-$$(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k$$$$\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v$$$f$\n\n$$ f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7) $$\n\n$\\pmb{q}$$\\pmb{k}_i$$\\alpha$softmax\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8) $$\n\n\naadditive attentionscaled dot-product attentionq  Rq k  Rk\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9) $$\n\n$$\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}$$$$\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}$$$$\\pmb{W}_v \\in \\mathbb{R}_h$$9$h$tanh\n\n$d$0$d$1$\\sqrt{d}$\n\n$$ \\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10) $$\n\n$n$$m$-$d$$v$$$\\pmb{Q} \\in \\mathbb{R}^{n \\times d}$$$$\\pmb{K} \\in \\mathbb{R}^{m \\times d}$$$$\\pmb{V} \\in \\mathbb{R}^{m \\times v}$$\n\n$$ softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11) $$\n\n\nTransformerTransformerTransformer\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\">\n\nTransformerencoder-decoderbertencoderencoderembeddingN=6encodeencodedecoderN=6decodeencodeencodemaskmask\n\n\nTransformerCNNRNNTransformerTransformer$sine$$cosine$\n\n$$ PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12) $$\n$$ PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13) $$\n\n$pos$$i$$d_{model}$$2\\pi$$10000\\cdot 2\\pi$[Transformer](https://spaces.ac.cn/archives/8130)[Sinusoidal](https://spaces.ac.cn/archives/8231)\n\n### \nCNN\n\n\n\n### TransformerBERT\nBERTTransformerencodeembeddingtoke_typeBERT[CLS][SEP][CLS]embeddingCLSfine-tune[SEP]BERT\n\n\nBERTMasked Language ModelmaskBERTTransformerMLMBERTNext Sequence PredictionBABERTBERT\n\n\n### BERT\n- BERTMLM\n- \n- (NLU)(NLG)\n- BERT[MASK]\n\n\n### \n[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] https://github.com/d2l-ai/d2l-zh","source":"_posts/bert-3w.md","raw":"---\ntitle: BERT \ndate: 2021-03-02 16:40:29\ntags:\n- BERT\ncategories: \n- \nmathjax: true\n---\n\nBERTNLPTransformer[1]BERT[2]CNNRNNBERTTransformerNLP\n<!--more-->\n\n\n### TransformerCNNRNN\nRNNCNNtransformer\n\n\nCNNRNNRNNCNNTransformerTransformerCNNRNNmulti-head attentionself-attentionTransformer\n\n\n#### (1) CNN[3]\nCNN\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\">\n\n\nCNN$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $\n\n$$ (f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)$$\n\n$ \\bf{x} $$f$$g$\n\n$$ (f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2) $$\n\n$f$$(a,b)$$g$$(i-a, j-b)$:\n\n$$ (f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3) $$\n\n\n\n\npaddingstride240  240105  5200  200RGBCNN(Transposed)/(Dilated/Atrous)(Deformable)\n\n\nCNN\n\n\nLeNetAlexNetVGGNiNGoogLeNetResNetDenseNet\n\n#### (2) RNN[3]\nnn$x_t$$t$$n-1$$n$,\n$$ P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4) $$\n\n\n$$h_{t-1}$$$$t-1$$$$x_t$$$$h_{t-1}$$$$t$$\n$$ h_{t} = f(x_t, h_{t-1}) \\qquad (5) $$\n\n\nRNN\n$t$$$X_t \\in \\mathbb{R}^{n \\times d}$$$$H_t \\in \\mathbb{R}^{n \\times h}$$$t$$$H_{t-1}$$$$W_{hh} \\in \\mathbb{R}^{h \\times n}$$\n\n\n$$H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6) $$\n\n5$t$\n\n$$O_t = H_t W_{hq} + b_q \\qquad (7) $$\n\n\n$$W_{xh} \\in \\mathbb{R}^{d \\times h}$$$$W_{hh} \\in \\mathbb{R}^{h \\times h}$$$$b_h \\in \\mathbb{R}^{1 \\times h}$$$$W_{qh} \\in \\mathbb{R}^{q \\times h}$$$$b_q \\in \\mathbb{R}^{1 \\times q}$$RNN\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\">\n\n\nRNNRNN\n\n\n#### (3) Transformer\n\n> 108[3]\n\n\n\n\nquerykeyvalue\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\">\n\n$\\pmb{q} \\in \\mathbb{R}^q$$m$-$$(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k$$$$\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v$$$f$\n\n$$ f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7) $$\n\n$\\pmb{q}$$\\pmb{k}_i$$\\alpha$softmax\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8) $$\n\n\naadditive attentionscaled dot-product attentionq  Rq k  Rk\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9) $$\n\n$$\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}$$$$\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}$$$$\\pmb{W}_v \\in \\mathbb{R}_h$$9$h$tanh\n\n$d$0$d$1$\\sqrt{d}$\n\n$$ \\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10) $$\n\n$n$$m$-$d$$v$$$\\pmb{Q} \\in \\mathbb{R}^{n \\times d}$$$$\\pmb{K} \\in \\mathbb{R}^{m \\times d}$$$$\\pmb{V} \\in \\mathbb{R}^{m \\times v}$$\n\n$$ softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11) $$\n\n\nTransformerTransformerTransformer\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\">\n\nTransformerencoder-decoderbertencoderencoderembeddingN=6encodeencodedecoderN=6decodeencodeencodemaskmask\n\n\nTransformerCNNRNNTransformerTransformer$sine$$cosine$\n\n$$ PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12) $$\n$$ PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13) $$\n\n$pos$$i$$d_{model}$$2\\pi$$10000\\cdot 2\\pi$[Transformer](https://spaces.ac.cn/archives/8130)[Sinusoidal](https://spaces.ac.cn/archives/8231)\n\n### \nCNN\n\n\n\n### TransformerBERT\nBERTTransformerencodeembeddingtoke_typeBERT[CLS][SEP][CLS]embeddingCLSfine-tune[SEP]BERT\n\n\nBERTMasked Language ModelmaskBERTTransformerMLMBERTNext Sequence PredictionBABERTBERT\n\n\n### BERT\n- BERTMLM\n- \n- (NLU)(NLG)\n- BERT[MASK]\n\n\n### \n[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] https://github.com/d2l-ai/d2l-zh","slug":"bert-3w","published":1,"updated":"2022-06-07T07:06:27.506Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga7w0020q4uxcn91gosx","content":"<p>BERTNLPTransformer[1]BERT[2]CNNRNNBERTTransformerNLP<br><span id=\"more\"></span></p>\n<h3 id=\"TransformerCNNRNN\"><a href=\"#TransformerCNNRNN\" class=\"headerlink\" title=\"TransformerCNNRNN\"></a>TransformerCNNRNN</h3><p>RNNCNNtransformer</p>\n<p>CNNRNNRNNCNNTransformerTransformerCNNRNNmulti-head attentionself-attentionTransformer</p>\n<h4 id=\"1-CNN-3\"><a href=\"#1-CNN-3\" class=\"headerlink\" title=\"(1) CNN[3]\"></a>(1) CNN[3]</h4><p>CNN<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\"></p>\n<p>CNN$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $</p>\n<script type=\"math/tex; mode=display\">(f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)</script><p>$ \\bf{x} $$f$$g$</p>\n<script type=\"math/tex; mode=display\">(f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2)</script><p>$f$$(a,b)$$g$$(i-a, j-b)$:</p>\n<script type=\"math/tex; mode=display\">(f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3)</script><p></p>\n<p>paddingstride240  240105  5200  200RGBCNN(Transposed)/(Dilated/Atrous)(Deformable)</p>\n<p>CNN</p>\n<p>LeNetAlexNetVGGNiNGoogLeNetResNetDenseNet</p>\n<h4 id=\"2-RNN-3\"><a href=\"#2-RNN-3\" class=\"headerlink\" title=\"(2) RNN[3]\"></a>(2) RNN[3]</h4><p>nn$x_t$$t$$n-1$$n$,</p>\n<script type=\"math/tex; mode=display\">P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4)</script><p><script type=\"math/tex\">h_{t-1}</script><script type=\"math/tex\">t-1</script><script type=\"math/tex\">x_t</script><script type=\"math/tex\">h_{t-1}</script><script type=\"math/tex\">t</script></p>\n<script type=\"math/tex; mode=display\">h_{t} = f(x_t, h_{t-1}) \\qquad (5)</script><p>RNN<br>$t$<script type=\"math/tex\">X_t \\in \\mathbb{R}^{n \\times d}</script><script type=\"math/tex\">H_t \\in \\mathbb{R}^{n \\times h}</script>$t$<script type=\"math/tex\">H_{t-1}</script><script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times n}</script></p>\n<script type=\"math/tex; mode=display\">H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6)</script><p>5$t$</p>\n<script type=\"math/tex; mode=display\">O_t = H_t W_{hq} + b_q \\qquad (7)</script><p><script type=\"math/tex\">W_{xh} \\in \\mathbb{R}^{d \\times h}</script><script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times h}</script><script type=\"math/tex\">b_h \\in \\mathbb{R}^{1 \\times h}</script><script type=\"math/tex\">W_{qh} \\in \\mathbb{R}^{q \\times h}</script><script type=\"math/tex\">b_q \\in \\mathbb{R}^{1 \\times q}</script>RNN</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\"></p>\n<p>RNNRNN</p>\n<h4 id=\"3-Transformer\"><a href=\"#3-Transformer\" class=\"headerlink\" title=\"(3) Transformer\"></a>(3) Transformer</h4><p></p>\n<blockquote>\n<p>108[3]</p>\n</blockquote>\n<p></p>\n<p>querykeyvalue<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\"></p>\n<p>$\\pmb{q} \\in \\mathbb{R}^q$$m$-<script type=\"math/tex\">(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k</script><script type=\"math/tex\">\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v</script>$f$</p>\n<script type=\"math/tex; mode=display\">f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7)</script><p>$\\pmb{q}$$\\pmb{k}_i$$\\alpha$softmax</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8)</script><p>aadditive attentionscaled dot-product attentionq  Rq k  Rk</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9)</script><p><script type=\"math/tex\">\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}</script><script type=\"math/tex\">\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}</script><script type=\"math/tex\">\\pmb{W}_v \\in \\mathbb{R}_h</script>9$h$tanh</p>\n<p>$d$0$d$1$\\sqrt{d}$</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10)</script><p>$n$$m$-$d$$v$<script type=\"math/tex\">\\pmb{Q} \\in \\mathbb{R}^{n \\times d}</script><script type=\"math/tex\">\\pmb{K} \\in \\mathbb{R}^{m \\times d}</script><script type=\"math/tex\">\\pmb{V} \\in \\mathbb{R}^{m \\times v}</script></p>\n<script type=\"math/tex; mode=display\">softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11)</script><p>TransformerTransformerTransformer<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\"></p>\n<p>Transformerencoder-decoderbertencoderencoderembeddingN=6encodeencodedecoderN=6decodeencodeencodemaskmask</p>\n<p>TransformerCNNRNNTransformerTransformer$sine$$cosine$</p>\n<script type=\"math/tex; mode=display\">PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12)</script><script type=\"math/tex; mode=display\">PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13)</script><p>$pos$$i$$d_{model}$$2\\pi$$10000\\cdot 2\\pi$<a href=\"https://spaces.ac.cn/archives/8130\">Transformer</a><a href=\"https://spaces.ac.cn/archives/8231\">Sinusoidal</a></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>CNN</p>\n<h3 id=\"TransformerBERT\"><a href=\"#TransformerBERT\" class=\"headerlink\" title=\"TransformerBERT\"></a>TransformerBERT</h3><p>BERTTransformerencodeembeddingtoke_typeBERT[CLS][SEP][CLS]embeddingCLSfine-tune[SEP]BERT</p>\n<p>BERTMasked Language ModelmaskBERTTransformerMLMBERTNext Sequence PredictionBABERTBERT</p>\n<h3 id=\"BERT\"><a href=\"#BERT\" class=\"headerlink\" title=\"BERT\"></a>BERT</h3><ul>\n<li>BERTMLM</li>\n<li></li>\n<li>(NLU)(NLG)</li>\n<li>BERT[MASK]</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.<br>[3] <a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></p>\n","site":{"data":{}},"excerpt":"<p>BERTNLPTransformer[1]BERT[2]CNNRNNBERTTransformerNLP<br>","more":"</p>\n<h3 id=\"TransformerCNNRNN\"><a href=\"#TransformerCNNRNN\" class=\"headerlink\" title=\"TransformerCNNRNN\"></a>TransformerCNNRNN</h3><p>RNNCNNtransformer</p>\n<p>CNNRNNRNNCNNTransformerTransformerCNNRNNmulti-head attentionself-attentionTransformer</p>\n<h4 id=\"1-CNN-3\"><a href=\"#1-CNN-3\" class=\"headerlink\" title=\"(1) CNN[3]\"></a>(1) CNN[3]</h4><p>CNN<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\"></p>\n<p>CNN$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $</p>\n<script type=\"math/tex; mode=display\">(f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)</script><p>$ \\bf{x} $$f$$g$</p>\n<script type=\"math/tex; mode=display\">(f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2)</script><p>$f$$(a,b)$$g$$(i-a, j-b)$:</p>\n<script type=\"math/tex; mode=display\">(f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3)</script><p></p>\n<p>paddingstride240  240105  5200  200RGBCNN(Transposed)/(Dilated/Atrous)(Deformable)</p>\n<p>CNN</p>\n<p>LeNetAlexNetVGGNiNGoogLeNetResNetDenseNet</p>\n<h4 id=\"2-RNN-3\"><a href=\"#2-RNN-3\" class=\"headerlink\" title=\"(2) RNN[3]\"></a>(2) RNN[3]</h4><p>nn$x_t$$t$$n-1$$n$,</p>\n<script type=\"math/tex; mode=display\">P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4)</script><p><script type=\"math/tex\">h_{t-1}</script><script type=\"math/tex\">t-1</script><script type=\"math/tex\">x_t</script><script type=\"math/tex\">h_{t-1}</script><script type=\"math/tex\">t</script></p>\n<script type=\"math/tex; mode=display\">h_{t} = f(x_t, h_{t-1}) \\qquad (5)</script><p>RNN<br>$t$<script type=\"math/tex\">X_t \\in \\mathbb{R}^{n \\times d}</script><script type=\"math/tex\">H_t \\in \\mathbb{R}^{n \\times h}</script>$t$<script type=\"math/tex\">H_{t-1}</script><script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times n}</script></p>\n<script type=\"math/tex; mode=display\">H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6)</script><p>5$t$</p>\n<script type=\"math/tex; mode=display\">O_t = H_t W_{hq} + b_q \\qquad (7)</script><p><script type=\"math/tex\">W_{xh} \\in \\mathbb{R}^{d \\times h}</script><script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times h}</script><script type=\"math/tex\">b_h \\in \\mathbb{R}^{1 \\times h}</script><script type=\"math/tex\">W_{qh} \\in \\mathbb{R}^{q \\times h}</script><script type=\"math/tex\">b_q \\in \\mathbb{R}^{1 \\times q}</script>RNN</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\"></p>\n<p>RNNRNN</p>\n<h4 id=\"3-Transformer\"><a href=\"#3-Transformer\" class=\"headerlink\" title=\"(3) Transformer\"></a>(3) Transformer</h4><p></p>\n<blockquote>\n<p>108[3]</p>\n</blockquote>\n<p></p>\n<p>querykeyvalue<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\"></p>\n<p>$\\pmb{q} \\in \\mathbb{R}^q$$m$-<script type=\"math/tex\">(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k</script><script type=\"math/tex\">\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v</script>$f$</p>\n<script type=\"math/tex; mode=display\">f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7)</script><p>$\\pmb{q}$$\\pmb{k}_i$$\\alpha$softmax</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8)</script><p>aadditive attentionscaled dot-product attentionq  Rq k  Rk</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9)</script><p><script type=\"math/tex\">\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}</script><script type=\"math/tex\">\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}</script><script type=\"math/tex\">\\pmb{W}_v \\in \\mathbb{R}_h</script>9$h$tanh</p>\n<p>$d$0$d$1$\\sqrt{d}$</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10)</script><p>$n$$m$-$d$$v$<script type=\"math/tex\">\\pmb{Q} \\in \\mathbb{R}^{n \\times d}</script><script type=\"math/tex\">\\pmb{K} \\in \\mathbb{R}^{m \\times d}</script><script type=\"math/tex\">\\pmb{V} \\in \\mathbb{R}^{m \\times v}</script></p>\n<script type=\"math/tex; mode=display\">softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11)</script><p>TransformerTransformerTransformer<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\"></p>\n<p>Transformerencoder-decoderbertencoderencoderembeddingN=6encodeencodedecoderN=6decodeencodeencodemaskmask</p>\n<p>TransformerCNNRNNTransformerTransformer$sine$$cosine$</p>\n<script type=\"math/tex; mode=display\">PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12)</script><script type=\"math/tex; mode=display\">PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13)</script><p>$pos$$i$$d_{model}$$2\\pi$$10000\\cdot 2\\pi$<a href=\"https://spaces.ac.cn/archives/8130\">Transformer</a><a href=\"https://spaces.ac.cn/archives/8231\">Sinusoidal</a></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>CNN</p>\n<h3 id=\"TransformerBERT\"><a href=\"#TransformerBERT\" class=\"headerlink\" title=\"TransformerBERT\"></a>TransformerBERT</h3><p>BERTTransformerencodeembeddingtoke_typeBERT[CLS][SEP][CLS]embeddingCLSfine-tune[SEP]BERT</p>\n<p>BERTMasked Language ModelmaskBERTTransformerMLMBERTNext Sequence PredictionBABERTBERT</p>\n<h3 id=\"BERT\"><a href=\"#BERT\" class=\"headerlink\" title=\"BERT\"></a>BERT</h3><ul>\n<li>BERTMLM</li>\n<li></li>\n<li>(NLU)(NLG)</li>\n<li>BERT[MASK]</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.<br>[3] <a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cl4xlga6g0001q4uxdduzdc52","category_id":"cl4xlga6m0004q4uxacivhgvn","_id":"cl4xlga6u000eq4ux0w4a8owe"},{"post_id":"cl4xlga6l0003q4uxaami25xn","category_id":"cl4xlga6r000aq4uxbsjsh36j","_id":"cl4xlga76000lq4uxh4pl1za7"},{"post_id":"cl4xlga6p0007q4ux5z0jezoz","category_id":"cl4xlga6v000fq4ux6og8arn4","_id":"cl4xlga7c000rq4ux8l9hac2e"},{"post_id":"cl4xlga6q0008q4uxfjuh6b38","category_id":"cl4xlga76000mq4ux0ptt2z56","_id":"cl4xlga7e000vq4ux8btu3n41"},{"post_id":"cl4xlga6r0009q4ux270ib8fz","category_id":"cl4xlga7c000sq4uxdhd22kov","_id":"cl4xlga7g000zq4uxeb768l9f"},{"post_id":"cl4xlga6s000cq4uxh0vy08dp","category_id":"cl4xlga76000mq4ux0ptt2z56","_id":"cl4xlga7h0012q4ux528o6cyv"},{"post_id":"cl4xlga6t000dq4ux7tb487v4","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7i0017q4ux9dzm3a7b"},{"post_id":"cl4xlga6z000hq4uxafn90o86","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7j001aq4ux8za11yqr"},{"post_id":"cl4xlga72000jq4ux7xq4fllg","category_id":"cl4xlga7i0016q4uxh06bbuyv","_id":"cl4xlga7k001eq4ux1qbahc5a"},{"post_id":"cl4xlga77000oq4ux8dnb8802","category_id":"cl4xlga7j001bq4ux7dey6v00","_id":"cl4xlga7l001iq4ux5de14b5g"},{"post_id":"cl4xlga7a000qq4ux64mg6y51","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7m001mq4ux21m64pro"},{"post_id":"cl4xlga7w0020q4uxcn91gosx","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7x0022q4ux10sacm4j"}],"PostTag":[{"post_id":"cl4xlga6g0001q4uxdduzdc52","tag_id":"cl4xlga6n0005q4uxchwl2jgt","_id":"cl4xlga71000iq4ux9gwp7p5d"},{"post_id":"cl4xlga6g0001q4uxdduzdc52","tag_id":"cl4xlga6r000bq4ux9ndl1eob","_id":"cl4xlga75000kq4uxale95skx"},{"post_id":"cl4xlga6l0003q4uxaami25xn","tag_id":"cl4xlga6x000gq4ux48x4ccu2","_id":"cl4xlga79000pq4ux96yk06fi"},{"post_id":"cl4xlga6p0007q4ux5z0jezoz","tag_id":"cl4xlga76000nq4ux5p7hc675","_id":"cl4xlga7d000uq4ux2k3r8brf"},{"post_id":"cl4xlga6q0008q4uxfjuh6b38","tag_id":"cl4xlga7c000tq4ux519k162k","_id":"cl4xlga7h0011q4uxcv0lcf1e"},{"post_id":"cl4xlga6q0008q4uxfjuh6b38","tag_id":"cl4xlga7e000xq4uxgrvlc14f","_id":"cl4xlga7h0013q4uxbcf37pdc"},{"post_id":"cl4xlga6r0009q4ux270ib8fz","tag_id":"cl4xlga7g0010q4ux7xb8f7iz","_id":"cl4xlga7j0019q4uxfwgmcca1"},{"post_id":"cl4xlga6r0009q4ux270ib8fz","tag_id":"cl4xlga7h0015q4ux1g56b8g4","_id":"cl4xlga7j001cq4ux0br019sc"},{"post_id":"cl4xlga6s000cq4uxh0vy08dp","tag_id":"cl4xlga7j0018q4uxfj98304o","_id":"cl4xlga7l001hq4ux6pa09x0n"},{"post_id":"cl4xlga6s000cq4uxh0vy08dp","tag_id":"cl4xlga7j001dq4uxbune1x1u","_id":"cl4xlga7l001jq4uxh204bt82"},{"post_id":"cl4xlga6t000dq4ux7tb487v4","tag_id":"cl4xlga7k001gq4uxd6y4dxaa","_id":"cl4xlga7l001lq4uxhbhz87zk"},{"post_id":"cl4xlga6z000hq4uxafn90o86","tag_id":"cl4xlga7c000tq4ux519k162k","_id":"cl4xlga7m001oq4ux294tftt2"},{"post_id":"cl4xlga72000jq4ux7xq4fllg","tag_id":"cl4xlga7m001nq4uxggtugnif","_id":"cl4xlga7n001rq4ux2ndofp60"},{"post_id":"cl4xlga72000jq4ux7xq4fllg","tag_id":"cl4xlga76000nq4ux5p7hc675","_id":"cl4xlga7n001sq4ux7zfy0zzq"},{"post_id":"cl4xlga77000oq4ux8dnb8802","tag_id":"cl4xlga7n001qq4uxa2t0fcmo","_id":"cl4xlga7o001vq4uxcslyfdmf"},{"post_id":"cl4xlga77000oq4ux8dnb8802","tag_id":"cl4xlga7n001tq4uxfq304bvl","_id":"cl4xlga7o001wq4ux1hw8eo65"},{"post_id":"cl4xlga7a000qq4ux64mg6y51","tag_id":"cl4xlga7n001uq4ux6mgxa7rz","_id":"cl4xlga7s001yq4ux1ws56dqy"},{"post_id":"cl4xlga7a000qq4ux64mg6y51","tag_id":"cl4xlga7o001xq4ux3obr814n","_id":"cl4xlga7t001zq4uxbocu8gt9"},{"post_id":"cl4xlga7w0020q4uxcn91gosx","tag_id":"cl4xlga7x0021q4uxajtdgkxn","_id":"cl4xlga7x0023q4uxa8c974a4"}],"Tag":[{"name":"Information Extraction","_id":"cl4xlga6n0005q4uxchwl2jgt"},{"name":"SOTA","_id":"cl4xlga6r000bq4ux9ndl1eob"},{"name":"","_id":"cl4xlga6x000gq4ux48x4ccu2"},{"name":"Representation Learning","_id":"cl4xlga76000nq4ux5p7hc675"},{"name":"PLM","_id":"cl4xlga7c000tq4ux519k162k"},{"name":"cross-lingual","_id":"cl4xlga7e000xq4uxgrvlc14f"},{"name":"PCM","_id":"cl4xlga7g0010q4ux7xb8f7iz"},{"name":"Dialog","_id":"cl4xlga7h0015q4ux1g56b8g4"},{"name":"","_id":"cl4xlga7j0018q4uxfj98304o"},{"name":"","_id":"cl4xlga7j001dq4uxbune1x1u"},{"name":"PM","_id":"cl4xlga7k001gq4uxd6y4dxaa"},{"name":"","_id":"cl4xlga7m001nq4uxggtugnif"},{"name":"mention","_id":"cl4xlga7n001qq4uxa2t0fcmo"},{"name":"","_id":"cl4xlga7n001tq4uxfq304bvl"},{"name":"","_id":"cl4xlga7n001uq4ux6mgxa7rz"},{"name":"","_id":"cl4xlga7o001xq4ux3obr814n"},{"name":"BERT","_id":"cl4xlga7x0021q4uxajtdgkxn"}]}}