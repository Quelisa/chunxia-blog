{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"7e3a38735e86e8a6ac4d79df2bcce665827b172c","modified":1651740571987},{"_id":"source/categories/index.md","hash":"6623c5214c3774f6e0ce8494a72fce8ec851c4f1","modified":1651822901552},{"_id":"source/_posts/baidu-uie-sota.md","hash":"7b0e90e38469aa97a0efadc893a57d688248a0a8","modified":1654591294925},{"_id":"source/_posts/bert-3w.md","hash":"a0afe191421a5c47b4bb784dbd9d2379b49fd5eb","modified":1654585587506},{"_id":"source/_posts/begin-to-learn-nlp.md","hash":"531fe1001e4de85ac08cdb5893c3a46f9f07bc91","modified":1654678022364},{"_id":"source/_posts/contrastive-representation-learning.md","hash":"c651c70c320b3a681f75cde7f1ab57f5197ff6df","modified":1654595012670},{"_id":"source/_posts/cross-lingual-PLM.md","hash":"a21240bcb194cecd62db59d5471c013516fcecea","modified":1654585925510},{"_id":"source/_posts/galaxy-task-oriented-dialog.md","hash":"0ddc831bece7859e29be1c799de513e36c489123","modified":1654585951362},{"_id":"source/_posts/pretrain-model-yue-01.md","hash":"c72f31eaed5c1587e076d5099acb269da4bcbc8d","modified":1654671647496},{"_id":"source/_posts/pretrained-model.md","hash":"f79fc90c6fdd92bc266976c9edbff3a16409dbc7","modified":1655125115301},{"_id":"source/_posts/roformer.md","hash":"381a29a66c6f46759c766ab10961e188273b8696","modified":1656063499291},{"_id":"source/_posts/time-mention.md","hash":"5f0ba86dbce49f2fd191881ea57e63d5903b685b","modified":1654778491190},{"_id":"source/_posts/sentence-transformers.md","hash":"44113b325621bc77ead7bba3808ee203b585b0bc","modified":1654585629539},{"_id":"source/_posts/word2vec.md","hash":"93fbc82bd1095cb8d67f8d391ce39c067bb39bca","modified":1654778368711},{"_id":"source/guestbook/index.md","hash":"e0c2ee470c6b99beb36087ab42f229d952332cb2","modified":1651822690951},{"_id":"source/tags/index.md","hash":"56b4923115b1703ca3a0f1b03091fd170e387728","modified":1651822931098},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852606},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852516},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852516},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852584},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852584},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852585},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852602},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651739852604},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1651739852462},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1651739852463},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1651739852464},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1651739852471},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1651739852472},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1651739852477},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1651739852474},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1651739852476},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1651739852473},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1651739852477},{"_id":"themes/next/README.cn.md","hash":"b878b73f3fcdef47849453c94420871903d487b3","modified":1651739852478},{"_id":"themes/next/README.md","hash":"efcdc4b0ca791c3fc64afa28c8721e137f2d11ea","modified":1651739852479},{"_id":"themes/next/_config.yml","hash":"5ff16272344e55fe42f7c94fd75feee794b26301","modified":1652260099741},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1651739852482},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1651739852481},{"_id":"themes/next/package.json","hash":"3963ad558a24c78a3fd4ef23cf5f73f421854627","modified":1651739852540},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1651739852465},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"a0a82dbfabdef9a9d7c17a08ceebfb4052d98d81","modified":1651739852466},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1651739852467},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1651739852469},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1651739852482},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1651739852483},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1651739852484},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1651739852484},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1651739852485},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1651739852488},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1651739852488},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1651739852489},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1651739852490},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1651739852490},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1651739852491},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1651739852491},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1651739852492},{"_id":"themes/next/languages/zh-Hans.yml","hash":"1aa53ff7d04619114667381da6ce71dfcdcd9683","modified":1651752907739},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1651739852493},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1651739852493},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1651739852498},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1651739852537},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1651739852537},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1651739852538},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1651739852538},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1651739852538},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1651739852539},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1651739852540},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1651739852541},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1651739852718},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1651739852719},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1651739852720},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1651739852496},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1651739852497},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1651739852500},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1651739852502},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1651739852503},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1651739852539},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1651739852504},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9c7343fd470e0943ebd75f227a083a980816290b","modified":1651739852505},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1651739852505},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1651739852506},{"_id":"themes/next/layout/_partials/footer.swig","hash":"26e93336dc57a39590ba8dc80564a1d2ad5ff93b","modified":1651739852506},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1651739852507},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1651739852509},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1651739852508},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"053bf2b23c6c3fd9cc4a21445ca9d90322f9a05a","modified":1652778798790},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1651739852510},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1651739852514},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1651739852515},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1651739852517},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1651739852530},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1651739852530},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1651739852531},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1651739852531},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1651739852532},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1651739852532},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1651739852532},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1651739852542},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1651739852542},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1651739852542},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1651739852543},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1651739852544},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1651739852545},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1651739852545},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1651739852545},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1651739852605},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1651739852546},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1651739852609},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1651739852610},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1651739852610},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1651739852611},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1651739852612},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1651739852612},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1651739852612},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1651739852613},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1651739852613},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1651739852614},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1651739852614},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1651739852615},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1651739852615},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1651739852615},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1651739852616},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1651739852616},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1651739852617},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1651739852616},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1651739852508},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1651739852507},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1651739852510},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1651739852511},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1651739852511},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1651739852512},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1651739852513},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1651739852513},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1651739852514},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1651739852515},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1651739852516},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1651739852517},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1651739852519},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1651739852519},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1651739852520},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1651739852520},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1651739852522},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1651739852521},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1651739852523},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1651739852524},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1651739852524},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1651739852524},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1651739852525},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1651739852523},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1651739852525},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1651739852526},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1651739852526},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1651739852527},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1651739852528},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1651739852528},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1651739852528},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1651739852529},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1651739852529},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1651739852530},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1651739852535},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1651739852534},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1651739852536},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1651739852536},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1651739852583},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1651739852583},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1651739852584},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1651739852585},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1651739852601},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1651739852601},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1651739852602},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1651739852603},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1651739852618},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1651739852619},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1651739852618},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1651739852620},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1651739852624},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1651739852620},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1651739852626},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1651739852627},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1651739852629},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1651739852630},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1651739852629},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1651739852645},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1651739852640},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1651739852646},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1651739852646},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1651739852647},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1651739852658},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1651739852659},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1651739852661},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1651739852662},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1651739852664},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1651739852664},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1651739852665},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1651739852665},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1651739852665},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1651739852685},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1651739852687},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1651739852688},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1651739852689},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1651739852691},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1651739852692},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1651739852693},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1651739852694},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1651739852695},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1651739852695},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1651739852696},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1651739852697},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1651739852696},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1651739852697},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1651739852698},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1651739852698},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1651739852698},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1651739852699},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1651739852699},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1651739852700},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1651739852700},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1651739852701},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1651739852701},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1651739852702},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1651739852703},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1651739852705},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1651739852704},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1651739852713},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1651739852713},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1651739852716},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1651739852717},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1651739852533},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1651739852718},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1651739852533},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1651739852547},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1651739852547},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1651739852548},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1651739852548},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1651739852549},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1651739852556},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1651739852568},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1651739852580},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1651739852581},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1651739852580},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1651739852582},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1651739852582},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1651739852582},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1651739852586},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1651739852588},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1651739852590},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1651739852581},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1651739852592},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1651739852592},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1651739852593},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1651739852593},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1651739852594},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1651739852596},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1651739852596},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e695e58f714129ca292c2e54cd62c251aca7f7fe","modified":1651739852596},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1651739852597},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1651739852597},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1651739852598},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1651739852599},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1651739852599},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1651739852600},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"ad2dcedf393ed1f3f5afd2508d24969c916d02fc","modified":1651739852600},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1651739852600},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1651739852628},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1651739852634},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1651739852637},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1651739852640},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1651739852647},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1651739852648},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1651739852648},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1651739852649},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1651739852649},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1651739852650},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1651739852654},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1651739852656},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1651739852658},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1651739852663},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1651739852663},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1651739852666},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1651739852667},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1651739852667},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1651739852712},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1651739852712},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1651739852549},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1651739852550},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1651739852550},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1651739852551},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1651739852551},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1651739852552},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1651739852553},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1651739852553},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1651739852554},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1651739852554},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1651739852555},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1651739852555},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1651739852555},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1651739852556},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1651739852557},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1651739852557},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1651739852558},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1651739852558},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1651739852559},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1651739852559},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1651739852560},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1651739852560},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1651739852561},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1651739852561},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1651739852561},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1651739852562},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1651739852562},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1651739852563},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1651739852563},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1651739852563},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1651739852564},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1651739852565},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1651739852564},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1651739852565},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1651739852565},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1651739852566},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1651739852566},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1651739852567},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1651739852567},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1651739852575},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1651739852576},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1651739852576},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1651739852577},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1651739852578},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1651739852577},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1651739852578},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1651739852578},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1651739852579},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1651739852579},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1651739852569},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1651739852570},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1651739852569},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1651739852571},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1651739852571},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1651739852572},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1651739852573},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1651739852575},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1651739852574},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1651739852594},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1651739852595},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1651739852598},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1651739852632},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1651739852633},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1651739852632},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1651739852633},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1651739852634},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1651739852650},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1651739852651},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1651739852651},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1651739852651},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1651739852652},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1651739852653},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1651739852686},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1651739852636},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1651739852683},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1651739852684},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1651739852671},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1651739852675},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1651739852682},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1651739852715},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1651739852644},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1651739852679},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1651739852711},{"_id":"public/guestbook/index.html","hash":"7b341cd571ae1c8eeee60616edeadeb47d48dced","modified":1656385989888},{"_id":"public/tags/index.html","hash":"9b0cf6992e4f390a2ad94c950e0e4a16f14806b8","modified":1656385989888},{"_id":"public/categories/index.html","hash":"a638db80c0aeeeffe6b2d1778e70cd46976722e5","modified":1656385989888},{"_id":"public/archives/page/2/index.html","hash":"f08eb36eb6eafb29db2addacfbed6992bef27669","modified":1656385989888},{"_id":"public/archives/2021/index.html","hash":"cb082cdcc539f69e1fe95074a2c0bd2a8511f1d7","modified":1656385989888},{"_id":"public/archives/2021/01/index.html","hash":"25b1386fd68cfbe4e8f6793b84705a90dc59cb9a","modified":1656385989888},{"_id":"public/archives/2021/03/index.html","hash":"c0c1655cff8312b233701c2fd129c2797e128f77","modified":1656385989888},{"_id":"public/archives/2021/10/index.html","hash":"2c7236376ed45bdefd3767c4c14af6d0aff6d8c2","modified":1656385989888},{"_id":"public/archives/2021/11/index.html","hash":"89cb7d351d32222d7d8758a2813545c23bd2af05","modified":1656385989888},{"_id":"public/archives/2022/index.html","hash":"3f1bdaeccbaf008e293d5bfce356431c5aca4a46","modified":1656385989888},{"_id":"public/archives/2022/01/index.html","hash":"d1a962cf4cfc10d2fbd7a7760b9ef5e41ac39994","modified":1656385989888},{"_id":"public/archives/2022/03/index.html","hash":"e4324dc2dabc67bad9551b2e41696c9ad4a85dca","modified":1656385989888},{"_id":"public/archives/2022/05/index.html","hash":"eb10c9f057300746b25073cbd259b86cff055e98","modified":1656385989888},{"_id":"public/categories/信息抽取/index.html","hash":"4f5236bbd734d8e12e8081af86c879f9a0289139","modified":1656385989888},{"_id":"public/categories/自然语言处理/index.html","hash":"c57d9d85420f4b9078ef5adf51534d901faa6868","modified":1656385989888},{"_id":"public/categories/对比学习/index.html","hash":"e5f51ea24fa4a11b82fda41e0f5670cc517cab72","modified":1656385989888},{"_id":"public/categories/跨语言模型/index.html","hash":"b730d7b363f847e39b6343712394e0c2de76dccb","modified":1656385989888},{"_id":"public/categories/任务型对话/index.html","hash":"4a3edf6f1acb88089c3ccf057a0e44fb45b0b7d3","modified":1656385989888},{"_id":"public/categories/预训练语言模型/index.html","hash":"082b8d43dbfdb9ed9fcfe12ebc7132891f369c5d","modified":1656385989888},{"_id":"public/categories/Sentence-Embedding/index.html","hash":"190f8296fd46786a7d3cdfbfb84d19d66f65011c","modified":1656385989888},{"_id":"public/categories/实体识别/index.html","hash":"4ecb45e1b753957d11ddea7a4a2442766c9c01af","modified":1656385989888},{"_id":"public/tags/Information-Extraction/index.html","hash":"a335962debbfbf4a39c690b8f244e7484672b0d3","modified":1656385989888},{"_id":"public/tags/SOTA/index.html","hash":"df3a5e0bd5cd69bce9ba13db31e779643acdeabb","modified":1656385989888},{"_id":"public/tags/学习心得/index.html","hash":"aeff7f5e026cd248a8f94789bf45ded3b615e448","modified":1656385989888},{"_id":"public/tags/Representation-Learning/index.html","hash":"76d6544f3db08b9f47816cd1b579eb3a14c74af0","modified":1656385989888},{"_id":"public/tags/PLM/index.html","hash":"6ea23a722a1f0f5bf3184e2df3363febf8dd0093","modified":1656385989888},{"_id":"public/tags/cross-lingual/index.html","hash":"393d42c5bdc72f00d488a040a7ab65f159442a55","modified":1656385989888},{"_id":"public/tags/PCM/index.html","hash":"aee8b6c333e84adf795788938de1671695dbb6b5","modified":1656385989888},{"_id":"public/tags/Dialog/index.html","hash":"86c86f585007ef9e90bb2fd41a9a071f004dd096","modified":1656385989888},{"_id":"public/tags/粤语/index.html","hash":"b81ef1a5d2a0ed5e681f85a0e7974fcc43955df1","modified":1656385989888},{"_id":"public/tags/数据收集与清洗/index.html","hash":"19cd1508216fe684684e0bcdc4daaf3fc0fb770b","modified":1656385989888},{"_id":"public/tags/PM/index.html","hash":"daecf20a1e41e7db745e669da5cca022231c0446","modified":1656385989888},{"_id":"public/tags/文本匹配/index.html","hash":"d3eeb3d9981718ae6f9d697ad4d45b311714ba24","modified":1656385989888},{"_id":"public/tags/mention/index.html","hash":"46db1a914a5be7096671a9a67fe3156bbab5aa6a","modified":1656385989888},{"_id":"public/tags/时间实体/index.html","hash":"5549624c79a882e197c8a91952672519746fb499","modified":1656385989888},{"_id":"public/tags/静态词向量/index.html","hash":"29cfe151ad358b589d353c71a439513fc2363f4d","modified":1656385989888},{"_id":"public/tags/负采样/index.html","hash":"2d05cfabe70e24dbfe2457d519477be09a329040","modified":1656385989888},{"_id":"public/tags/BERT/index.html","hash":"894cb968aaf1496cf2a1b8db914b0365c61d41c8","modified":1656385989888},{"_id":"public/2022/05/24/baidu-uie-sota/index.html","hash":"193a4959fdf1f3e92a88efa44eca1cabcb3fdf52","modified":1656385989888},{"_id":"public/2022/05/11/contrastive-representation-learning/index.html","hash":"ba1a97c2d380da1cec44deaff95fca98e35331d4","modified":1656385989888},{"_id":"public/2022/05/06/roformer/index.html","hash":"85dfde5bf91ae2e22b78af4481a82defafdd0d60","modified":1656385989888},{"_id":"public/2022/03/18/cross-lingual-PLM/index.html","hash":"96fc91358ea9f607e38de8bc08c2a776645fb9be","modified":1656385989888},{"_id":"public/2022/03/10/pretrain-model-yue-01/index.html","hash":"d80a9ca49114df2b9e3498f05a6e1a044ac9f189","modified":1656385989888},{"_id":"public/2022/01/15/sentence-transformers/index.html","hash":"57c3d4fe3cb37babf8ddd7e9a29285a8997454da","modified":1656385989888},{"_id":"public/2022/01/11/galaxy-task-oriented-dialog/index.html","hash":"d2c37fbcdaa6d80a3a71c447d2a4efdd67dfed18","modified":1656385989888},{"_id":"public/2021/11/15/pretrained-model/index.html","hash":"383cc09e7acb30860c1fe5ca60731d3621c4dd9b","modified":1656385989888},{"_id":"public/2021/10/19/word2vec/index.html","hash":"455cff3241e07857dba8c01f06b075fbe21fac9e","modified":1656385989888},{"_id":"public/2021/03/02/bert-3w/index.html","hash":"9bdb59ae17cf4f0f4c7abf65ce626e1d81e20ca2","modified":1656385989888},{"_id":"public/2021/01/06/time-mention/index.html","hash":"ac5d762de9b93506ba985229eed453b88fb32c58","modified":1656385989888},{"_id":"public/2021/01/05/begin-to-learn-nlp/index.html","hash":"fa6f208d8b3e95cc9b20212d96e3265022090dfd","modified":1656385989888},{"_id":"public/archives/index.html","hash":"6f2bc26d933cd91c3e1ba38efbbf11e79b7462bd","modified":1656385989888},{"_id":"public/index.html","hash":"09b21dab28519680a62efa8aa65df7b3dc24c02b","modified":1656385989888},{"_id":"public/page/2/index.html","hash":"9762174a609acd5576b953ec19edfcb55e29af52","modified":1656385989888},{"_id":"public/CNAME","hash":"7e3a38735e86e8a6ac4d79df2bcce665827b172c","modified":1656385989888},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1656385989888},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1656385989888},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1656385989888},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1656385989888},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1656385989888},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1656385989888},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1656385989888},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1656385989888},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1656385989888},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1656385989888},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1656385989888},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1656385989888},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1656385989888},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1656385989888},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1656385989888},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1656385989888},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1656385989888},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1656385989888},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1656385989888},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1656385989888},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1656385989888},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1656385989888},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1656385989888},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1656385989888},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1656385989888},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1656385989888},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1656385989888},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1656385989888},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1656385989888},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1656385989888},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1656385989888},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1656385989888},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1656385989888},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1656385989888},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1656385989888},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1656385989888},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1656385989888},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1656385989888},{"_id":"public/lib/fastclick/README.html","hash":"c07b353b4efa132290ec4479102a55d80ac6d300","modified":1656385989888},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1656385989888},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1656385989888},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1656385989888},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1656385989888},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1656385989888},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1656385989888},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1656385989888},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1656385989888},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1656385989888},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1656385989888},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1656385989888},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1656385989888},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1656385989888},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1656385989888},{"_id":"public/css/main.css","hash":"b33bdaf25eeba3a8a050007425c2affe9d6bdc64","modified":1656385989888},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1656385989888},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1656385989888},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1656385989888},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1656385989888},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1656385989888},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1656385989888},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1656385989888},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1656385989888},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1656385989888},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1656385989888},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1656385989888},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1656385989888},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1656385989888},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1656385989888},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1656385989888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1656385989888},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1656385989888},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1656385989888},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1656385989888},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1656385989888},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1656385989888}],"Category":[{"name":"信息抽取","_id":"cl4xlga6m0004q4uxacivhgvn"},{"name":"自然语言处理","_id":"cl4xlga6r000aq4uxbsjsh36j"},{"name":"对比学习","_id":"cl4xlga6v000fq4ux6og8arn4"},{"name":"跨语言模型","_id":"cl4xlga76000mq4ux0ptt2z56"},{"name":"任务型对话","_id":"cl4xlga7c000sq4uxdhd22kov"},{"name":"预训练语言模型","_id":"cl4xlga7g000yq4uxbx27ds6x"},{"name":"Sentence Embedding","_id":"cl4xlga7i0016q4uxh06bbuyv"},{"name":"实体识别","_id":"cl4xlga7j001bq4ux7dey6v00"}],"Data":[],"Page":[{"title":"guestbook","date":"2022-05-06T07:37:46.000Z","_content":"\n\n<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>","source":"guestbook/index.md","raw":"---\ntitle: guestbook\ndate: 2022-05-06 15:37:46\n---\n\n\n<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>","updated":"2022-05-06T07:38:10.951Z","path":"guestbook/index.html","comments":1,"layout":"page","_id":"cl4xlga6c0000q4uxfu2yfqq6","content":"<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>","site":{"data":{}},"excerpt":"","more":"<div class=\"ds-recent-visitors\" data-num-items=\"28\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"></div>"},{"title":"tags","date":"2022-05-06T07:42:01.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2022-05-06 15:42:01\ntype: \"tags\"\n---\n","updated":"2022-05-06T07:42:11.098Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cl4xlga6k0002q4uxax1536jx","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2022-05-06T07:39:05.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2022-05-06 15:39:05\ntype: \"categories\"\n---\n","updated":"2022-05-06T07:41:41.552Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cl4xlga6o0006q4uxf6h1e4ki","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"信息抽取新范式：百度UIE刷新13个数据集SOTA","date":"2022-05-24T11:33:25.000Z","_content":"\n百度最近提出的统一文本到结构生成的框架UIE（Universal Information Extraction），将实体、关系、事件和情感四大抽取任务统一建模，并在13个数据集上达到了SOTA。令人振奋的不仅是效果上的提升，还有对于信息抽取革命性的统一！下面看看UIE是怎么做的吧！\n\n<!--more-->\n\n### Task-specialized IE VS. Universial IE\n\n目前大多数的信息抽取都是分任务进行的，针对一个任务建立一个模型进行任务学习，这给信息抽取带来了很大的复杂度，百度提出一个统一文本到结构生成的框架UIE，将实体信息抽取、关系抽取、事件抽取和敏感性抽取统一建模，实现一个模型抽取多种信息。下图是这两种信息抽取方式比较直观的对比。\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\">\n\n综上可以将所有的信息抽取进行统一建模为两个步骤：（1）抽取点，可以是实体、事件等；（2）建立联系，将收取到的点信息之间建立关联。\n\n\n### Universial IE 架构\nUIE是如何统一建模的呢？首先，因为之前的信息抽取都是分任务的，每一个任务都有自己的输出表示，要建立一个统一的模型在结构输出上首先要有一个结构规范，为此论文中提出了一种结构化抽取语言（SEL），对信息结果进行统一管理。其次，虽然是统一的信息抽取模型，但是我们希望它能够自适应的输出所需要的任务的结果，而不是给出所有的结果，因此论文中提出了在此基础上提出了结构化模式提示器（SSI）来控制对不同任务的生成需求。综上就是UIE结构的核心，它可以表达为：SSI+Text->SEL，\nUIE架构图如下：\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\">\n\n\n#### SEL：结构化抽取语言\n\n举例如图所示：\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\">\n\n\n#### SSI：结构化模式提示器\n\nSSI的本质一个基于schema的prompt机制，用于控制不同的生成需求：在Text前拼接上相应的Schema Prompt，输出相应的SEL结构语言。例如：\n1. 实体抽取：[spot] 实体类别 [text]\n2. 关系抽取：[spot] 实体类别 [asso] 关系类别 [text]\n3. 事件抽取：[spot] 事件类别 [asso] 论元类别 [text]\n4. 观点抽取：[spot] 评价维度 [asso] 观点类别 [text]\n\n\n\n### UIE 训练\n\nUIE采用预训练+微调的方式进行训练，预训练主要针对：\n1. Text-to-Structure Pre-training：构建基础的文本到结构的映射能力，使用text-to-struct的平行语料（SSI，TEXT，SEL）进行训练；\n2. Structure Generation Pre-training：训练具备SEL语言的结构化能力，使用包含SEL语法的record数据（None，None，SEL）进行训练；\n3. Retrofitting Semantic Representation：训练具备基础的语义编码能力，使用原始文本（None，TEXT，TEXT）进行训练。\n\n\n微调阶段为解决自回归teacher-forcing的暴露偏差，构建了拒识噪声注入的模型微调机制。\n\n\n### 展望\n少样本实验可以发现，大规模异构监督预训练可以学习通用的信息抽取能力，使模型具有更好小样本学习能力。UIE的提出只是信息抽取统一建模的开始，论文的作者也表明未来希望将类似指代消解等任务也在信息抽取中进行统一建模学习。我感觉UIE本质上还是走的大规模预训练训练+Prompt+多任务微调的范式，但是不得不承认这种范式确实已经成为一种主流。\n\n\n\n### 参考文献\n[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.","source":"_posts/baidu-uie-sota.md","raw":"---\ntitle: 信息抽取新范式：百度UIE刷新13个数据集SOTA\ndate: 2022-05-24 19:33:25\ntags:\n- Information Extraction\n- SOTA\ncategories: \n- 信息抽取\n---\n\n百度最近提出的统一文本到结构生成的框架UIE（Universal Information Extraction），将实体、关系、事件和情感四大抽取任务统一建模，并在13个数据集上达到了SOTA。令人振奋的不仅是效果上的提升，还有对于信息抽取革命性的统一！下面看看UIE是怎么做的吧！\n\n<!--more-->\n\n### Task-specialized IE VS. Universial IE\n\n目前大多数的信息抽取都是分任务进行的，针对一个任务建立一个模型进行任务学习，这给信息抽取带来了很大的复杂度，百度提出一个统一文本到结构生成的框架UIE，将实体信息抽取、关系抽取、事件抽取和敏感性抽取统一建模，实现一个模型抽取多种信息。下图是这两种信息抽取方式比较直观的对比。\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\">\n\n综上可以将所有的信息抽取进行统一建模为两个步骤：（1）抽取点，可以是实体、事件等；（2）建立联系，将收取到的点信息之间建立关联。\n\n\n### Universial IE 架构\nUIE是如何统一建模的呢？首先，因为之前的信息抽取都是分任务的，每一个任务都有自己的输出表示，要建立一个统一的模型在结构输出上首先要有一个结构规范，为此论文中提出了一种结构化抽取语言（SEL），对信息结果进行统一管理。其次，虽然是统一的信息抽取模型，但是我们希望它能够自适应的输出所需要的任务的结果，而不是给出所有的结果，因此论文中提出了在此基础上提出了结构化模式提示器（SSI）来控制对不同任务的生成需求。综上就是UIE结构的核心，它可以表达为：SSI+Text->SEL，\nUIE架构图如下：\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\">\n\n\n#### SEL：结构化抽取语言\n\n举例如图所示：\n<img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\">\n\n\n#### SSI：结构化模式提示器\n\nSSI的本质一个基于schema的prompt机制，用于控制不同的生成需求：在Text前拼接上相应的Schema Prompt，输出相应的SEL结构语言。例如：\n1. 实体抽取：[spot] 实体类别 [text]\n2. 关系抽取：[spot] 实体类别 [asso] 关系类别 [text]\n3. 事件抽取：[spot] 事件类别 [asso] 论元类别 [text]\n4. 观点抽取：[spot] 评价维度 [asso] 观点类别 [text]\n\n\n\n### UIE 训练\n\nUIE采用预训练+微调的方式进行训练，预训练主要针对：\n1. Text-to-Structure Pre-training：构建基础的文本到结构的映射能力，使用text-to-struct的平行语料（SSI，TEXT，SEL）进行训练；\n2. Structure Generation Pre-training：训练具备SEL语言的结构化能力，使用包含SEL语法的record数据（None，None，SEL）进行训练；\n3. Retrofitting Semantic Representation：训练具备基础的语义编码能力，使用原始文本（None，TEXT，TEXT）进行训练。\n\n\n微调阶段为解决自回归teacher-forcing的暴露偏差，构建了拒识噪声注入的模型微调机制。\n\n\n### 展望\n少样本实验可以发现，大规模异构监督预训练可以学习通用的信息抽取能力，使模型具有更好小样本学习能力。UIE的提出只是信息抽取统一建模的开始，论文的作者也表明未来希望将类似指代消解等任务也在信息抽取中进行统一建模学习。我感觉UIE本质上还是走的大规模预训练训练+Prompt+多任务微调的范式，但是不得不承认这种范式确实已经成为一种主流。\n\n\n\n### 参考文献\n[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.","slug":"baidu-uie-sota","published":1,"updated":"2022-06-07T08:41:34.925Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6g0001q4uxdduzdc52","content":"<p>百度最近提出的统一文本到结构生成的框架UIE（Universal Information Extraction），将实体、关系、事件和情感四大抽取任务统一建模，并在13个数据集上达到了SOTA。令人振奋的不仅是效果上的提升，还有对于信息抽取革命性的统一！下面看看UIE是怎么做的吧！</p>\n<span id=\"more\"></span>\n<h3 id=\"Task-specialized-IE-VS-Universial-IE\"><a href=\"#Task-specialized-IE-VS-Universial-IE\" class=\"headerlink\" title=\"Task-specialized IE VS. Universial IE\"></a>Task-specialized IE VS. Universial IE</h3><p>目前大多数的信息抽取都是分任务进行的，针对一个任务建立一个模型进行任务学习，这给信息抽取带来了很大的复杂度，百度提出一个统一文本到结构生成的框架UIE，将实体信息抽取、关系抽取、事件抽取和敏感性抽取统一建模，实现一个模型抽取多种信息。下图是这两种信息抽取方式比较直观的对比。</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\"></p>\n<p>综上可以将所有的信息抽取进行统一建模为两个步骤：（1）抽取点，可以是实体、事件等；（2）建立联系，将收取到的点信息之间建立关联。</p>\n<h3 id=\"Universial-IE-架构\"><a href=\"#Universial-IE-架构\" class=\"headerlink\" title=\"Universial IE 架构\"></a>Universial IE 架构</h3><p>UIE是如何统一建模的呢？首先，因为之前的信息抽取都是分任务的，每一个任务都有自己的输出表示，要建立一个统一的模型在结构输出上首先要有一个结构规范，为此论文中提出了一种结构化抽取语言（SEL），对信息结果进行统一管理。其次，虽然是统一的信息抽取模型，但是我们希望它能够自适应的输出所需要的任务的结果，而不是给出所有的结果，因此论文中提出了在此基础上提出了结构化模式提示器（SSI）来控制对不同任务的生成需求。综上就是UIE结构的核心，它可以表达为：SSI+Text-&gt;SEL，<br>UIE架构图如下：<br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\"></p>\n<h4 id=\"SEL：结构化抽取语言\"><a href=\"#SEL：结构化抽取语言\" class=\"headerlink\" title=\"SEL：结构化抽取语言\"></a>SEL：结构化抽取语言</h4><p>举例如图所示：<br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\"></p>\n<h4 id=\"SSI：结构化模式提示器\"><a href=\"#SSI：结构化模式提示器\" class=\"headerlink\" title=\"SSI：结构化模式提示器\"></a>SSI：结构化模式提示器</h4><p>SSI的本质一个基于schema的prompt机制，用于控制不同的生成需求：在Text前拼接上相应的Schema Prompt，输出相应的SEL结构语言。例如：</p>\n<ol>\n<li>实体抽取：[spot] 实体类别 [text]</li>\n<li>关系抽取：[spot] 实体类别 [asso] 关系类别 [text]</li>\n<li>事件抽取：[spot] 事件类别 [asso] 论元类别 [text]</li>\n<li>观点抽取：[spot] 评价维度 [asso] 观点类别 [text]</li>\n</ol>\n<h3 id=\"UIE-训练\"><a href=\"#UIE-训练\" class=\"headerlink\" title=\"UIE 训练\"></a>UIE 训练</h3><p>UIE采用预训练+微调的方式进行训练，预训练主要针对：</p>\n<ol>\n<li>Text-to-Structure Pre-training：构建基础的文本到结构的映射能力，使用text-to-struct的平行语料（SSI，TEXT，SEL）进行训练；</li>\n<li>Structure Generation Pre-training：训练具备SEL语言的结构化能力，使用包含SEL语法的record数据（None，None，SEL）进行训练；</li>\n<li>Retrofitting Semantic Representation：训练具备基础的语义编码能力，使用原始文本（None，TEXT，TEXT）进行训练。</li>\n</ol>\n<p>微调阶段为解决自回归teacher-forcing的暴露偏差，构建了拒识噪声注入的模型微调机制。</p>\n<h3 id=\"展望\"><a href=\"#展望\" class=\"headerlink\" title=\"展望\"></a>展望</h3><p>少样本实验可以发现，大规模异构监督预训练可以学习通用的信息抽取能力，使模型具有更好小样本学习能力。UIE的提出只是信息抽取统一建模的开始，论文的作者也表明未来希望将类似指代消解等任务也在信息抽取中进行统一建模学习。我感觉UIE本质上还是走的大规模预训练训练+Prompt+多任务微调的范式，但是不得不承认这种范式确实已经成为一种主流。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.</p>\n","site":{"data":{}},"excerpt":"<p>百度最近提出的统一文本到结构生成的框架UIE（Universal Information Extraction），将实体、关系、事件和情感四大抽取任务统一建模，并在13个数据集上达到了SOTA。令人振奋的不仅是效果上的提升，还有对于信息抽取革命性的统一！下面看看UIE是怎么做的吧！</p>","more":"<h3 id=\"Task-specialized-IE-VS-Universial-IE\"><a href=\"#Task-specialized-IE-VS-Universial-IE\" class=\"headerlink\" title=\"Task-specialized IE VS. Universial IE\"></a>Task-specialized IE VS. Universial IE</h3><p>目前大多数的信息抽取都是分任务进行的，针对一个任务建立一个模型进行任务学习，这给信息抽取带来了很大的复杂度，百度提出一个统一文本到结构生成的框架UIE，将实体信息抽取、关系抽取、事件抽取和敏感性抽取统一建模，实现一个模型抽取多种信息。下图是这两种信息抽取方式比较直观的对比。</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/task-IEvsUIE.png\" width=\"50%\" height=\"60%\"></p>\n<p>综上可以将所有的信息抽取进行统一建模为两个步骤：（1）抽取点，可以是实体、事件等；（2）建立联系，将收取到的点信息之间建立关联。</p>\n<h3 id=\"Universial-IE-架构\"><a href=\"#Universial-IE-架构\" class=\"headerlink\" title=\"Universial IE 架构\"></a>Universial IE 架构</h3><p>UIE是如何统一建模的呢？首先，因为之前的信息抽取都是分任务的，每一个任务都有自己的输出表示，要建立一个统一的模型在结构输出上首先要有一个结构规范，为此论文中提出了一种结构化抽取语言（SEL），对信息结果进行统一管理。其次，虽然是统一的信息抽取模型，但是我们希望它能够自适应的输出所需要的任务的结果，而不是给出所有的结果，因此论文中提出了在此基础上提出了结构化模式提示器（SSI）来控制对不同任务的生成需求。综上就是UIE结构的核心，它可以表达为：SSI+Text-&gt;SEL，<br>UIE架构图如下：<br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/UIE.png\" width=\"80%\" height=\"70%\"></p>\n<h4 id=\"SEL：结构化抽取语言\"><a href=\"#SEL：结构化抽取语言\" class=\"headerlink\" title=\"SEL：结构化抽取语言\"></a>SEL：结构化抽取语言</h4><p>举例如图所示：<br><img src=\"https://github.com/Quelisa/picture/raw/main/information-extraction/SEL.png\" width=\"40%\" height=\"60%\"></p>\n<h4 id=\"SSI：结构化模式提示器\"><a href=\"#SSI：结构化模式提示器\" class=\"headerlink\" title=\"SSI：结构化模式提示器\"></a>SSI：结构化模式提示器</h4><p>SSI的本质一个基于schema的prompt机制，用于控制不同的生成需求：在Text前拼接上相应的Schema Prompt，输出相应的SEL结构语言。例如：</p>\n<ol>\n<li>实体抽取：[spot] 实体类别 [text]</li>\n<li>关系抽取：[spot] 实体类别 [asso] 关系类别 [text]</li>\n<li>事件抽取：[spot] 事件类别 [asso] 论元类别 [text]</li>\n<li>观点抽取：[spot] 评价维度 [asso] 观点类别 [text]</li>\n</ol>\n<h3 id=\"UIE-训练\"><a href=\"#UIE-训练\" class=\"headerlink\" title=\"UIE 训练\"></a>UIE 训练</h3><p>UIE采用预训练+微调的方式进行训练，预训练主要针对：</p>\n<ol>\n<li>Text-to-Structure Pre-training：构建基础的文本到结构的映射能力，使用text-to-struct的平行语料（SSI，TEXT，SEL）进行训练；</li>\n<li>Structure Generation Pre-training：训练具备SEL语言的结构化能力，使用包含SEL语法的record数据（None，None，SEL）进行训练；</li>\n<li>Retrofitting Semantic Representation：训练具备基础的语义编码能力，使用原始文本（None，TEXT，TEXT）进行训练。</li>\n</ol>\n<p>微调阶段为解决自回归teacher-forcing的暴露偏差，构建了拒识噪声注入的模型微调机制。</p>\n<h3 id=\"展望\"><a href=\"#展望\" class=\"headerlink\" title=\"展望\"></a>展望</h3><p>少样本实验可以发现，大规模异构监督预训练可以学习通用的信息抽取能力，使模型具有更好小样本学习能力。UIE的提出只是信息抽取统一建模的开始，论文的作者也表明未来希望将类似指代消解等任务也在信息抽取中进行统一建模学习。我感觉UIE本质上还是走的大规模预训练训练+Prompt+多任务微调的范式，但是不得不承认这种范式确实已经成为一种主流。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Lu Y, Liu Q, Dai D, et al. Unified Structure Generation for Universal Information Extraction[J]. arXiv preprint arXiv:2203.12277, 2022.</p>"},{"title":"Learning NLP","date":"2021-01-05T09:29:16.000Z","_content":"\n\n分享NLP学习从业心得，不断更新中...\n<!--more-->\n\n一般的学习是从底层到上层，比如先学习机器学习、深度学习原理、学习自然语言处理基础分词句法分析等、词向量、预训练模型、自然语言处理任务（分类、标注、生成等）、自然语言的高级应用（文案生成、开放式问答系统、推荐系统、文本分析等）。在实际工作中，自我的感受是，要低头赶路也要抬头看天。到目前为止，自然语言处理在工业界的落地场景其实还是相对单一，商业化程度不高的，所以在工作中要不断的提高基础能力的同时，关注最新的行业动态、技术热点，交叉学科的结合，比如跨语言、跨任务、多模态等。\n\n\n### 基础入门\n\n对于新手来说，基础肯定的最重要的，这里有几个理论和实践结合比较好的资料，个人也是实践过，感觉比较好~\n\n1. 动手学习深度学习：https://github.com/d2l-ai/d2l-zh\n2. 《基于BERT模型的自然语言处理实战》\n3. 《自然语言处理：基于预训练的方法》\n4. Stanford EE364\n5. CMU https://www.stat.cmu.edu/~ryantibs/convexopt/\n6. https://www.langboat.com/academy/basics/0-basics\n\n这个阶段除了掌握基础的数理知识、机器学原理外还要着重动手实践、复现经典的算法。\n\n\n### 进阶学习\n\n根据自己的爱好，选择一个自然语言处理的领域和一个或者几个自然语言处理的任务进行深入的研究，下面是常见的自然语言处理任务：\n\n1. 文本分类\n2. 信息抽取\n3. 文本相似度、检索\n4. 问答、推理\n5. 机器翻译\n6. 文本生成\n7. 跨语言迁移\n8. 预训练模型\n9. 模型加速与模型压缩\n10. 多模态\n\n\n此阶段要结合实际业务场景，考虑实际应用中遇到的痛点问题结合技术特点进行应用。关注主流技术发展、技术理论原理等。最一定程度上对一个技术点的发展有成体系的认知，并了解目前的发展趋势和挑战。在算法落地方面，还需要了解最新的推理框架、分布式训练和成熟的应用工具等。\n\n\n### 深入学习\n\n目前还在探索中...\n","source":"_posts/begin-to-learn-nlp.md","raw":"---\ntitle: Learning NLP\ndate: 2021-01-05 17:29:16\ntags:\n- 学习心得\ncategories: \n- 自然语言处理\n---\n\n\n分享NLP学习从业心得，不断更新中...\n<!--more-->\n\n一般的学习是从底层到上层，比如先学习机器学习、深度学习原理、学习自然语言处理基础分词句法分析等、词向量、预训练模型、自然语言处理任务（分类、标注、生成等）、自然语言的高级应用（文案生成、开放式问答系统、推荐系统、文本分析等）。在实际工作中，自我的感受是，要低头赶路也要抬头看天。到目前为止，自然语言处理在工业界的落地场景其实还是相对单一，商业化程度不高的，所以在工作中要不断的提高基础能力的同时，关注最新的行业动态、技术热点，交叉学科的结合，比如跨语言、跨任务、多模态等。\n\n\n### 基础入门\n\n对于新手来说，基础肯定的最重要的，这里有几个理论和实践结合比较好的资料，个人也是实践过，感觉比较好~\n\n1. 动手学习深度学习：https://github.com/d2l-ai/d2l-zh\n2. 《基于BERT模型的自然语言处理实战》\n3. 《自然语言处理：基于预训练的方法》\n4. Stanford EE364\n5. CMU https://www.stat.cmu.edu/~ryantibs/convexopt/\n6. https://www.langboat.com/academy/basics/0-basics\n\n这个阶段除了掌握基础的数理知识、机器学原理外还要着重动手实践、复现经典的算法。\n\n\n### 进阶学习\n\n根据自己的爱好，选择一个自然语言处理的领域和一个或者几个自然语言处理的任务进行深入的研究，下面是常见的自然语言处理任务：\n\n1. 文本分类\n2. 信息抽取\n3. 文本相似度、检索\n4. 问答、推理\n5. 机器翻译\n6. 文本生成\n7. 跨语言迁移\n8. 预训练模型\n9. 模型加速与模型压缩\n10. 多模态\n\n\n此阶段要结合实际业务场景，考虑实际应用中遇到的痛点问题结合技术特点进行应用。关注主流技术发展、技术理论原理等。最一定程度上对一个技术点的发展有成体系的认知，并了解目前的发展趋势和挑战。在算法落地方面，还需要了解最新的推理框架、分布式训练和成熟的应用工具等。\n\n\n### 深入学习\n\n目前还在探索中...\n","slug":"begin-to-learn-nlp","published":1,"updated":"2022-06-08T08:47:02.364Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6l0003q4uxaami25xn","content":"<p>分享NLP学习从业心得，不断更新中…<br><span id=\"more\"></span></p>\n<p>一般的学习是从底层到上层，比如先学习机器学习、深度学习原理、学习自然语言处理基础分词句法分析等、词向量、预训练模型、自然语言处理任务（分类、标注、生成等）、自然语言的高级应用（文案生成、开放式问答系统、推荐系统、文本分析等）。在实际工作中，自我的感受是，要低头赶路也要抬头看天。到目前为止，自然语言处理在工业界的落地场景其实还是相对单一，商业化程度不高的，所以在工作中要不断的提高基础能力的同时，关注最新的行业动态、技术热点，交叉学科的结合，比如跨语言、跨任务、多模态等。</p>\n<h3 id=\"基础入门\"><a href=\"#基础入门\" class=\"headerlink\" title=\"基础入门\"></a>基础入门</h3><p>对于新手来说，基础肯定的最重要的，这里有几个理论和实践结合比较好的资料，个人也是实践过，感觉比较好~</p>\n<ol>\n<li>动手学习深度学习：<a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></li>\n<li>《基于BERT模型的自然语言处理实战》</li>\n<li>《自然语言处理：基于预训练的方法》</li>\n<li>Stanford EE364</li>\n<li>CMU <a href=\"https://www.stat.cmu.edu/~ryantibs/convexopt/\">https://www.stat.cmu.edu/~ryantibs/convexopt/</a></li>\n<li><a href=\"https://www.langboat.com/academy/basics/0-basics\">https://www.langboat.com/academy/basics/0-basics</a></li>\n</ol>\n<p>这个阶段除了掌握基础的数理知识、机器学原理外还要着重动手实践、复现经典的算法。</p>\n<h3 id=\"进阶学习\"><a href=\"#进阶学习\" class=\"headerlink\" title=\"进阶学习\"></a>进阶学习</h3><p>根据自己的爱好，选择一个自然语言处理的领域和一个或者几个自然语言处理的任务进行深入的研究，下面是常见的自然语言处理任务：</p>\n<ol>\n<li>文本分类</li>\n<li>信息抽取</li>\n<li>文本相似度、检索</li>\n<li>问答、推理</li>\n<li>机器翻译</li>\n<li>文本生成</li>\n<li>跨语言迁移</li>\n<li>预训练模型</li>\n<li>模型加速与模型压缩</li>\n<li>多模态</li>\n</ol>\n<p>此阶段要结合实际业务场景，考虑实际应用中遇到的痛点问题结合技术特点进行应用。关注主流技术发展、技术理论原理等。最一定程度上对一个技术点的发展有成体系的认知，并了解目前的发展趋势和挑战。在算法落地方面，还需要了解最新的推理框架、分布式训练和成熟的应用工具等。</p>\n<h3 id=\"深入学习\"><a href=\"#深入学习\" class=\"headerlink\" title=\"深入学习\"></a>深入学习</h3><p>目前还在探索中…</p>\n","site":{"data":{}},"excerpt":"<p>分享NLP学习从业心得，不断更新中…<br>","more":"</p>\n<p>一般的学习是从底层到上层，比如先学习机器学习、深度学习原理、学习自然语言处理基础分词句法分析等、词向量、预训练模型、自然语言处理任务（分类、标注、生成等）、自然语言的高级应用（文案生成、开放式问答系统、推荐系统、文本分析等）。在实际工作中，自我的感受是，要低头赶路也要抬头看天。到目前为止，自然语言处理在工业界的落地场景其实还是相对单一，商业化程度不高的，所以在工作中要不断的提高基础能力的同时，关注最新的行业动态、技术热点，交叉学科的结合，比如跨语言、跨任务、多模态等。</p>\n<h3 id=\"基础入门\"><a href=\"#基础入门\" class=\"headerlink\" title=\"基础入门\"></a>基础入门</h3><p>对于新手来说，基础肯定的最重要的，这里有几个理论和实践结合比较好的资料，个人也是实践过，感觉比较好~</p>\n<ol>\n<li>动手学习深度学习：<a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></li>\n<li>《基于BERT模型的自然语言处理实战》</li>\n<li>《自然语言处理：基于预训练的方法》</li>\n<li>Stanford EE364</li>\n<li>CMU <a href=\"https://www.stat.cmu.edu/~ryantibs/convexopt/\">https://www.stat.cmu.edu/~ryantibs/convexopt/</a></li>\n<li><a href=\"https://www.langboat.com/academy/basics/0-basics\">https://www.langboat.com/academy/basics/0-basics</a></li>\n</ol>\n<p>这个阶段除了掌握基础的数理知识、机器学原理外还要着重动手实践、复现经典的算法。</p>\n<h3 id=\"进阶学习\"><a href=\"#进阶学习\" class=\"headerlink\" title=\"进阶学习\"></a>进阶学习</h3><p>根据自己的爱好，选择一个自然语言处理的领域和一个或者几个自然语言处理的任务进行深入的研究，下面是常见的自然语言处理任务：</p>\n<ol>\n<li>文本分类</li>\n<li>信息抽取</li>\n<li>文本相似度、检索</li>\n<li>问答、推理</li>\n<li>机器翻译</li>\n<li>文本生成</li>\n<li>跨语言迁移</li>\n<li>预训练模型</li>\n<li>模型加速与模型压缩</li>\n<li>多模态</li>\n</ol>\n<p>此阶段要结合实际业务场景，考虑实际应用中遇到的痛点问题结合技术特点进行应用。关注主流技术发展、技术理论原理等。最一定程度上对一个技术点的发展有成体系的认知，并了解目前的发展趋势和挑战。在算法落地方面，还需要了解最新的推理框架、分布式训练和成熟的应用工具等。</p>\n<h3 id=\"深入学习\"><a href=\"#深入学习\" class=\"headerlink\" title=\"深入学习\"></a>深入学习</h3><p>目前还在探索中…</p>"},{"title":"浅谈对比表示学习","date":"2022-05-11T08:41:52.000Z","mathjax":true,"_content":"\n\n最近用SimCSE[1]的对比方式对跨语言模型进行再次训练，在文本相似度的任务上效果都有提升，不仅效果好，而且做法还十分简单，非常适合无监督训练。因此打算了解一下对比表示学习。\n\n<!--more-->\n\n\n### 对比表示学习和sentence embedding\n以NLP为例，sentence embedding的核心思想是，语义相似的文本在空间的向量表示彼此相近，而语义无关的文本在空间的向量表示彼此疏远。而对比表示学习正是捕捉到了这种表示学习简单的建模方式，采用一定的数学手段，将样本中的正例也就是语义相近的文本和负例区分来开。所以这里涉及到的核心主要是两个，一个是如何构造正例和负例，另一个就是如何设计度量距离和loss。\n\n\n### 正例负例设计\n\n一般在对比学习中，正例的设计使用数据增强的方式生成带有噪声的正例，负例采用同一个batch中的其他的数据，所以负例的大小取决于batch的大小，而负例的选择也在一定程度上影响着对比表示学习的效果。同样，正例的数据增强方式也会对表示效果有不同的表现。下面介绍几种经典的数据增强方式：\n\n\n1. 词汇编辑（同义词替换、随机插入、随机交换、随即删除）\n2. Dropout（SimCSE采用此方法，效果较好）\n3. Cutoff（token、feature、span）[2]\n4. Back-translation\n\n\n\n### 度量距离和对比loss\n常用的度量距离有欧几里得距离、余弦相似度、马氏距离等，下面按照对比loss的发展介绍几种经典的loss。\n\n#### Contrastive Loss[3]\n该论文中采用欧几里得距离作为度量，损失函数定义如下：\n\n$$ \\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1) $$\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2) $$\n\n其中$W$是模型参数，$X_1$、$X_2$是样本在空间的向量表示，$Y$是0-1变量，取0时表示两个样本一致，取1表示不一致。$L_S$表示正正样本对loss，$L_D$表示正负样本对loss。其中正正、正负loss的设计思路如下图，最后的loss表达式如（3）。\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\">\n\n\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3) $$\n\n可以看到这里设置了一个类似于分类中的margin，当两个样本为同一个类别时，loss变为优化$L_S$，这时候直接最小化他们的欧几里得距离，当两个样本不是同一个类时，loss变为优化$L_D$，这是如果样本距离已经大于margin距离则不对他们进行处理，如果样本距离小于margin距离，则最大化他们的欧几里得距离到margin的距离。\n\n\n#### NEC（Noise Contrastive Estimation）Loss[5]\n为了解决无法直接计算归一化因子的情况NEC，也就是 Noise Contrastive Estimation（噪声对比估计）最初在[4]中被提出，NCE通过对比真实样本与噪声样本，从而能够去估算出真实样本的概率分布的参数。具体做法是讲问题转化为一个二分类问题，在一批含噪声的样本中，判断样本是真实样本还是噪声样本。假设变量真实样本x服从概率$p(x)$，$$p(x) = \\frac{e^{G(x)}}{Z}$$, $$Z=\\sum_x{e^{G(x)}}$$则称为归一化函数。NEC通过计算$P(1|x)$来绕开直接计算$p(x)$带来的无法计算归一化函数的问题，\n$$ p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4) $$\n\n设$$\\tilde{p(x)}$$为真实样本的分布，$U(x)$是服从某一个确定的分布，则其loss为：\n$$ \\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5) $$\n通过公式推导可以得到：\n\n$$ \\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)$$\n\n则式中的$$\\gamma - logU(x)$$就是归一化常数，因此达到原来的目的。NEC很大的一个贡献是证明了仅适用负采样技术就可以加速归一化项的计算，因此为对比学习提供了很强大的理论支撑。\n\n\n#### Triplet Loss[6]\n正如loss的名字所示，我们将被优化的目标设置为一个三元组$(x,x^+,x^-)$，那么三元组的整体距离为：\n$$ \\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)$$\n\n从公式中可以很直观的看到，它通过在使正例到锚点的距离尽可能小，负例到锚点的距离尽可能大的同时还做了一定的约束来防止过拟合。\n\n\n#### InfoNCE Loss[7]\nInfoNCE是引入了互信息的NEC，首先回顾一下什么是互信息。设离散随机变量$X$和$Y$，$p(x,y)$是$X$和$Y$的联合分布，$p(x)$和$p(y)$分别是$X$和$Y$的边缘概率分布函数，则$X$和$Y$的互信息可以定义为：\n\n$$I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)$$\n\n\n现在假设有一个生成模型$$p(x_{t+k}|c_t)$$根据当前上下文$$c_t$$预测$$k$$个时刻后的数据$$x_{t+k}$$，引入互信息后可以通过最大化当前上下文$$c_t$$未来的数据$$x_{t+k}$$之间的互信息来进行预测，根据条件概率:\n\n\n$$P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)$$\n\n\n结合公式（8）其互信息表示：\n$$I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)$$\n\n\n由于不知道$$c_t$$和$$x_{t+k}$$的联合概率分布$$p(x_{t+k},c_t)$$，所以最大化互信息就要最大化$$\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$$。根据NCE中提供的思路，将问题转换为一个二分类的问题，从条件$$p(x_{t+k}|c_t)$$中取出的数据为正样本，将它和上下文一起组成正样本对，类标签是1，将$$p(x_{t+k})$$取出的样本为负样本对，它是和当前上下文没有必然联系的随机数据，将它和上下文构成负样本对，类标签为0。根据NCE中说明的设定，正样本选取1个；因为在NCE中证明了噪声分布与数据分布越接近越好，所以负样本就直接在当前序列中随机选取，负样本数量越多越好。所以最后将根据$$c_t$$预测$$x_{t+k}$$的问题就转化为区分正负样本的能力，其对应的交叉熵损失即InfoNCE loss为:\n\n\n$$  \\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11) $$\n\n实际上，InfoNCE不仅可以作为自监督学习中的对比损失函数，还可以作为互信息的一个估计器。\n\n#### SCL Loss[8]\nSCL loss即监督对比学习损失，是2021年斯坦福提出的在fine-tune阶段对交叉熵loss的改进损失。一般认为交叉熵loss具有泛化性能差和稳定性不好的缺点，尤其是在少量标签的样本中fine-tune时，介于此并考虑到对比学习可以利用少量的样本进行有效的学习，因此提出监督对比损失。监督对比损失没有像其他的对学习方法中那样采用自监督的学习方式，通过数据增强制造正样本，而是采用了监督任务的对比目标。训练目标的总loss是交叉熵loss和SCL loss的加权和：\n\n$$ \\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12) $$\n$$ \\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13) $$\n\n\n$$ \\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14) $$\n\n\n### 参考文献\n\n[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.\n[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE, 2006, 2: 1735-1742.\n[4] Gutmann M, Hyvärinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.\n[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.\n[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.\n[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.\n[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.\n","source":"_posts/contrastive-representation-learning.md","raw":"---\ntitle: 浅谈对比表示学习\ndate: 2022-05-11 16:41:52\ntags:\n- Representation Learning\ncategories: \n- 对比学习\nmathjax: true\n---\n\n\n最近用SimCSE[1]的对比方式对跨语言模型进行再次训练，在文本相似度的任务上效果都有提升，不仅效果好，而且做法还十分简单，非常适合无监督训练。因此打算了解一下对比表示学习。\n\n<!--more-->\n\n\n### 对比表示学习和sentence embedding\n以NLP为例，sentence embedding的核心思想是，语义相似的文本在空间的向量表示彼此相近，而语义无关的文本在空间的向量表示彼此疏远。而对比表示学习正是捕捉到了这种表示学习简单的建模方式，采用一定的数学手段，将样本中的正例也就是语义相近的文本和负例区分来开。所以这里涉及到的核心主要是两个，一个是如何构造正例和负例，另一个就是如何设计度量距离和loss。\n\n\n### 正例负例设计\n\n一般在对比学习中，正例的设计使用数据增强的方式生成带有噪声的正例，负例采用同一个batch中的其他的数据，所以负例的大小取决于batch的大小，而负例的选择也在一定程度上影响着对比表示学习的效果。同样，正例的数据增强方式也会对表示效果有不同的表现。下面介绍几种经典的数据增强方式：\n\n\n1. 词汇编辑（同义词替换、随机插入、随机交换、随即删除）\n2. Dropout（SimCSE采用此方法，效果较好）\n3. Cutoff（token、feature、span）[2]\n4. Back-translation\n\n\n\n### 度量距离和对比loss\n常用的度量距离有欧几里得距离、余弦相似度、马氏距离等，下面按照对比loss的发展介绍几种经典的loss。\n\n#### Contrastive Loss[3]\n该论文中采用欧几里得距离作为度量，损失函数定义如下：\n\n$$ \\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1) $$\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2) $$\n\n其中$W$是模型参数，$X_1$、$X_2$是样本在空间的向量表示，$Y$是0-1变量，取0时表示两个样本一致，取1表示不一致。$L_S$表示正正样本对loss，$L_D$表示正负样本对loss。其中正正、正负loss的设计思路如下图，最后的loss表达式如（3）。\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\">\n\n\n$$L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3) $$\n\n可以看到这里设置了一个类似于分类中的margin，当两个样本为同一个类别时，loss变为优化$L_S$，这时候直接最小化他们的欧几里得距离，当两个样本不是同一个类时，loss变为优化$L_D$，这是如果样本距离已经大于margin距离则不对他们进行处理，如果样本距离小于margin距离，则最大化他们的欧几里得距离到margin的距离。\n\n\n#### NEC（Noise Contrastive Estimation）Loss[5]\n为了解决无法直接计算归一化因子的情况NEC，也就是 Noise Contrastive Estimation（噪声对比估计）最初在[4]中被提出，NCE通过对比真实样本与噪声样本，从而能够去估算出真实样本的概率分布的参数。具体做法是讲问题转化为一个二分类问题，在一批含噪声的样本中，判断样本是真实样本还是噪声样本。假设变量真实样本x服从概率$p(x)$，$$p(x) = \\frac{e^{G(x)}}{Z}$$, $$Z=\\sum_x{e^{G(x)}}$$则称为归一化函数。NEC通过计算$P(1|x)$来绕开直接计算$p(x)$带来的无法计算归一化函数的问题，\n$$ p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4) $$\n\n设$$\\tilde{p(x)}$$为真实样本的分布，$U(x)$是服从某一个确定的分布，则其loss为：\n$$ \\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5) $$\n通过公式推导可以得到：\n\n$$ \\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)$$\n\n则式中的$$\\gamma - logU(x)$$就是归一化常数，因此达到原来的目的。NEC很大的一个贡献是证明了仅适用负采样技术就可以加速归一化项的计算，因此为对比学习提供了很强大的理论支撑。\n\n\n#### Triplet Loss[6]\n正如loss的名字所示，我们将被优化的目标设置为一个三元组$(x,x^+,x^-)$，那么三元组的整体距离为：\n$$ \\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)$$\n\n从公式中可以很直观的看到，它通过在使正例到锚点的距离尽可能小，负例到锚点的距离尽可能大的同时还做了一定的约束来防止过拟合。\n\n\n#### InfoNCE Loss[7]\nInfoNCE是引入了互信息的NEC，首先回顾一下什么是互信息。设离散随机变量$X$和$Y$，$p(x,y)$是$X$和$Y$的联合分布，$p(x)$和$p(y)$分别是$X$和$Y$的边缘概率分布函数，则$X$和$Y$的互信息可以定义为：\n\n$$I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)$$\n\n\n现在假设有一个生成模型$$p(x_{t+k}|c_t)$$根据当前上下文$$c_t$$预测$$k$$个时刻后的数据$$x_{t+k}$$，引入互信息后可以通过最大化当前上下文$$c_t$$未来的数据$$x_{t+k}$$之间的互信息来进行预测，根据条件概率:\n\n\n$$P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)$$\n\n\n结合公式（8）其互信息表示：\n$$I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)$$\n\n\n由于不知道$$c_t$$和$$x_{t+k}$$的联合概率分布$$p(x_{t+k},c_t)$$，所以最大化互信息就要最大化$$\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$$。根据NCE中提供的思路，将问题转换为一个二分类的问题，从条件$$p(x_{t+k}|c_t)$$中取出的数据为正样本，将它和上下文一起组成正样本对，类标签是1，将$$p(x_{t+k})$$取出的样本为负样本对，它是和当前上下文没有必然联系的随机数据，将它和上下文构成负样本对，类标签为0。根据NCE中说明的设定，正样本选取1个；因为在NCE中证明了噪声分布与数据分布越接近越好，所以负样本就直接在当前序列中随机选取，负样本数量越多越好。所以最后将根据$$c_t$$预测$$x_{t+k}$$的问题就转化为区分正负样本的能力，其对应的交叉熵损失即InfoNCE loss为:\n\n\n$$  \\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11) $$\n\n实际上，InfoNCE不仅可以作为自监督学习中的对比损失函数，还可以作为互信息的一个估计器。\n\n#### SCL Loss[8]\nSCL loss即监督对比学习损失，是2021年斯坦福提出的在fine-tune阶段对交叉熵loss的改进损失。一般认为交叉熵loss具有泛化性能差和稳定性不好的缺点，尤其是在少量标签的样本中fine-tune时，介于此并考虑到对比学习可以利用少量的样本进行有效的学习，因此提出监督对比损失。监督对比损失没有像其他的对学习方法中那样采用自监督的学习方式，通过数据增强制造正样本，而是采用了监督任务的对比目标。训练目标的总loss是交叉熵loss和SCL loss的加权和：\n\n$$ \\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12) $$\n$$ \\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13) $$\n\n\n$$ \\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14) $$\n\n\n### 参考文献\n\n[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.\n[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE, 2006, 2: 1735-1742.\n[4] Gutmann M, Hyvärinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.\n[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.\n[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.\n[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.\n[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.\n","slug":"contrastive-representation-learning","published":1,"updated":"2022-06-07T09:43:32.670Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6p0007q4ux5z0jezoz","content":"<p>最近用SimCSE[1]的对比方式对跨语言模型进行再次训练，在文本相似度的任务上效果都有提升，不仅效果好，而且做法还十分简单，非常适合无监督训练。因此打算了解一下对比表示学习。</p>\n<span id=\"more\"></span>\n<h3 id=\"对比表示学习和sentence-embedding\"><a href=\"#对比表示学习和sentence-embedding\" class=\"headerlink\" title=\"对比表示学习和sentence embedding\"></a>对比表示学习和sentence embedding</h3><p>以NLP为例，sentence embedding的核心思想是，语义相似的文本在空间的向量表示彼此相近，而语义无关的文本在空间的向量表示彼此疏远。而对比表示学习正是捕捉到了这种表示学习简单的建模方式，采用一定的数学手段，将样本中的正例也就是语义相近的文本和负例区分来开。所以这里涉及到的核心主要是两个，一个是如何构造正例和负例，另一个就是如何设计度量距离和loss。</p>\n<h3 id=\"正例负例设计\"><a href=\"#正例负例设计\" class=\"headerlink\" title=\"正例负例设计\"></a>正例负例设计</h3><p>一般在对比学习中，正例的设计使用数据增强的方式生成带有噪声的正例，负例采用同一个batch中的其他的数据，所以负例的大小取决于batch的大小，而负例的选择也在一定程度上影响着对比表示学习的效果。同样，正例的数据增强方式也会对表示效果有不同的表现。下面介绍几种经典的数据增强方式：</p>\n<ol>\n<li>词汇编辑（同义词替换、随机插入、随机交换、随即删除）</li>\n<li>Dropout（SimCSE采用此方法，效果较好）</li>\n<li>Cutoff（token、feature、span）[2]</li>\n<li>Back-translation</li>\n</ol>\n<h3 id=\"度量距离和对比loss\"><a href=\"#度量距离和对比loss\" class=\"headerlink\" title=\"度量距离和对比loss\"></a>度量距离和对比loss</h3><p>常用的度量距离有欧几里得距离、余弦相似度、马氏距离等，下面按照对比loss的发展介绍几种经典的loss。</p>\n<h4 id=\"Contrastive-Loss-3\"><a href=\"#Contrastive-Loss-3\" class=\"headerlink\" title=\"Contrastive Loss[3]\"></a>Contrastive Loss[3]</h4><p>该论文中采用欧几里得距离作为度量，损失函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1)</script><script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2)</script><p>其中$W$是模型参数，$X_1$、$X_2$是样本在空间的向量表示，$Y$是0-1变量，取0时表示两个样本一致，取1表示不一致。$L_S$表示正正样本对loss，$L_D$表示正负样本对loss。其中正正、正负loss的设计思路如下图，最后的loss表达式如（3）。</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\"></p>\n<script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3)</script><p>可以看到这里设置了一个类似于分类中的margin，当两个样本为同一个类别时，loss变为优化$L_S$，这时候直接最小化他们的欧几里得距离，当两个样本不是同一个类时，loss变为优化$L_D$，这是如果样本距离已经大于margin距离则不对他们进行处理，如果样本距离小于margin距离，则最大化他们的欧几里得距离到margin的距离。</p>\n<h4 id=\"NEC（Noise-Contrastive-Estimation）Loss-5\"><a href=\"#NEC（Noise-Contrastive-Estimation）Loss-5\" class=\"headerlink\" title=\"NEC（Noise Contrastive Estimation）Loss[5]\"></a>NEC（Noise Contrastive Estimation）Loss[5]</h4><p>为了解决无法直接计算归一化因子的情况NEC，也就是 Noise Contrastive Estimation（噪声对比估计）最初在[4]中被提出，NCE通过对比真实样本与噪声样本，从而能够去估算出真实样本的概率分布的参数。具体做法是讲问题转化为一个二分类问题，在一批含噪声的样本中，判断样本是真实样本还是噪声样本。假设变量真实样本x服从概率$p(x)$，<script type=\"math/tex\">p(x) = \\frac{e^{G(x)}}{Z}</script>, <script type=\"math/tex\">Z=\\sum_x{e^{G(x)}}</script>则称为归一化函数。NEC通过计算$P(1|x)$来绕开直接计算$p(x)$带来的无法计算归一化函数的问题，</p>\n<script type=\"math/tex; mode=display\">p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4)</script><p>设<script type=\"math/tex\">\\tilde{p(x)}</script>为真实样本的分布，$U(x)$是服从某一个确定的分布，则其loss为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5)</script><p>通过公式推导可以得到：</p>\n<script type=\"math/tex; mode=display\">\\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)</script><p>则式中的<script type=\"math/tex\">\\gamma - logU(x)</script>就是归一化常数，因此达到原来的目的。NEC很大的一个贡献是证明了仅适用负采样技术就可以加速归一化项的计算，因此为对比学习提供了很强大的理论支撑。</p>\n<h4 id=\"Triplet-Loss-6\"><a href=\"#Triplet-Loss-6\" class=\"headerlink\" title=\"Triplet Loss[6]\"></a>Triplet Loss[6]</h4><p>正如loss的名字所示，我们将被优化的目标设置为一个三元组$(x,x^+,x^-)$，那么三元组的整体距离为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)</script><p>从公式中可以很直观的看到，它通过在使正例到锚点的距离尽可能小，负例到锚点的距离尽可能大的同时还做了一定的约束来防止过拟合。</p>\n<h4 id=\"InfoNCE-Loss-7\"><a href=\"#InfoNCE-Loss-7\" class=\"headerlink\" title=\"InfoNCE Loss[7]\"></a>InfoNCE Loss[7]</h4><p>InfoNCE是引入了互信息的NEC，首先回顾一下什么是互信息。设离散随机变量$X$和$Y$，$p(x,y)$是$X$和$Y$的联合分布，$p(x)$和$p(y)$分别是$X$和$Y$的边缘概率分布函数，则$X$和$Y$的互信息可以定义为：</p>\n<script type=\"math/tex; mode=display\">I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)</script><p>现在假设有一个生成模型<script type=\"math/tex\">p(x_{t+k}|c_t)</script>根据当前上下文<script type=\"math/tex\">c_t</script>预测<script type=\"math/tex\">k</script>个时刻后的数据<script type=\"math/tex\">x_{t+k}</script>，引入互信息后可以通过最大化当前上下文<script type=\"math/tex\">c_t</script>未来的数据<script type=\"math/tex\">x_{t+k}</script>之间的互信息来进行预测，根据条件概率:</p>\n<script type=\"math/tex; mode=display\">P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)</script><p>结合公式（8）其互信息表示：</p>\n<script type=\"math/tex; mode=display\">I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)</script><p>由于不知道<script type=\"math/tex\">c_t</script>和<script type=\"math/tex\">x_{t+k}</script>的联合概率分布<script type=\"math/tex\">p(x_{t+k},c_t)</script>，所以最大化互信息就要最大化<script type=\"math/tex\">\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}</script>。根据NCE中提供的思路，将问题转换为一个二分类的问题，从条件<script type=\"math/tex\">p(x_{t+k}|c_t)</script>中取出的数据为正样本，将它和上下文一起组成正样本对，类标签是1，将<script type=\"math/tex\">p(x_{t+k})</script>取出的样本为负样本对，它是和当前上下文没有必然联系的随机数据，将它和上下文构成负样本对，类标签为0。根据NCE中说明的设定，正样本选取1个；因为在NCE中证明了噪声分布与数据分布越接近越好，所以负样本就直接在当前序列中随机选取，负样本数量越多越好。所以最后将根据<script type=\"math/tex\">c_t</script>预测<script type=\"math/tex\">x_{t+k}</script>的问题就转化为区分正负样本的能力，其对应的交叉熵损失即InfoNCE loss为:</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11)</script><p>实际上，InfoNCE不仅可以作为自监督学习中的对比损失函数，还可以作为互信息的一个估计器。</p>\n<h4 id=\"SCL-Loss-8\"><a href=\"#SCL-Loss-8\" class=\"headerlink\" title=\"SCL Loss[8]\"></a>SCL Loss[8]</h4><p>SCL loss即监督对比学习损失，是2021年斯坦福提出的在fine-tune阶段对交叉熵loss的改进损失。一般认为交叉熵loss具有泛化性能差和稳定性不好的缺点，尤其是在少量标签的样本中fine-tune时，介于此并考虑到对比学习可以利用少量的样本进行有效的学习，因此提出监督对比损失。监督对比损失没有像其他的对学习方法中那样采用自监督的学习方式，通过数据增强制造正样本，而是采用了监督任务的对比目标。训练目标的总loss是交叉熵loss和SCL loss的加权和：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14)</script><h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.<br>[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). IEEE, 2006, 2: 1735-1742.<br>[4] Gutmann M, Hyvärinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.<br>[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.<br>[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.<br>[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.<br>[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.</p>\n","site":{"data":{}},"excerpt":"<p>最近用SimCSE[1]的对比方式对跨语言模型进行再次训练，在文本相似度的任务上效果都有提升，不仅效果好，而且做法还十分简单，非常适合无监督训练。因此打算了解一下对比表示学习。</p>","more":"<h3 id=\"对比表示学习和sentence-embedding\"><a href=\"#对比表示学习和sentence-embedding\" class=\"headerlink\" title=\"对比表示学习和sentence embedding\"></a>对比表示学习和sentence embedding</h3><p>以NLP为例，sentence embedding的核心思想是，语义相似的文本在空间的向量表示彼此相近，而语义无关的文本在空间的向量表示彼此疏远。而对比表示学习正是捕捉到了这种表示学习简单的建模方式，采用一定的数学手段，将样本中的正例也就是语义相近的文本和负例区分来开。所以这里涉及到的核心主要是两个，一个是如何构造正例和负例，另一个就是如何设计度量距离和loss。</p>\n<h3 id=\"正例负例设计\"><a href=\"#正例负例设计\" class=\"headerlink\" title=\"正例负例设计\"></a>正例负例设计</h3><p>一般在对比学习中，正例的设计使用数据增强的方式生成带有噪声的正例，负例采用同一个batch中的其他的数据，所以负例的大小取决于batch的大小，而负例的选择也在一定程度上影响着对比表示学习的效果。同样，正例的数据增强方式也会对表示效果有不同的表现。下面介绍几种经典的数据增强方式：</p>\n<ol>\n<li>词汇编辑（同义词替换、随机插入、随机交换、随即删除）</li>\n<li>Dropout（SimCSE采用此方法，效果较好）</li>\n<li>Cutoff（token、feature、span）[2]</li>\n<li>Back-translation</li>\n</ol>\n<h3 id=\"度量距离和对比loss\"><a href=\"#度量距离和对比loss\" class=\"headerlink\" title=\"度量距离和对比loss\"></a>度量距离和对比loss</h3><p>常用的度量距离有欧几里得距离、余弦相似度、马氏距离等，下面按照对比loss的发展介绍几种经典的loss。</p>\n<h4 id=\"Contrastive-Loss-3\"><a href=\"#Contrastive-Loss-3\" class=\"headerlink\" title=\"Contrastive Loss[3]\"></a>Contrastive Loss[3]</h4><p>该论文中采用欧几里得距离作为度量，损失函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(W) = \\sum_{i=1}^P{L(W,Y,\\vec{X_1},\\vec{X_2})^i} \\qquad (1)</script><script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)L_S(D^i_W) + YL_D(D^i_W) \\qquad (2)</script><p>其中$W$是模型参数，$X_1$、$X_2$是样本在空间的向量表示，$Y$是0-1变量，取0时表示两个样本一致，取1表示不一致。$L_S$表示正正样本对loss，$L_D$表示正负样本对loss。其中正正、正负loss的设计思路如下图，最后的loss表达式如（3）。</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/Loss/constrastive_loss.png\" width=\"50%\" height=\"50%\"></p>\n<script type=\"math/tex; mode=display\">L(W,(Y,\\vec{X_1}, \\vec{X_2^2})) = (1-Y)\\frac{1}{2}(D_W)^2 + Y\\frac{1}{2}\\{max(0,  m- D_W)\\}^2 \\qquad (3)</script><p>可以看到这里设置了一个类似于分类中的margin，当两个样本为同一个类别时，loss变为优化$L_S$，这时候直接最小化他们的欧几里得距离，当两个样本不是同一个类时，loss变为优化$L_D$，这是如果样本距离已经大于margin距离则不对他们进行处理，如果样本距离小于margin距离，则最大化他们的欧几里得距离到margin的距离。</p>\n<h4 id=\"NEC（Noise-Contrastive-Estimation）Loss-5\"><a href=\"#NEC（Noise-Contrastive-Estimation）Loss-5\" class=\"headerlink\" title=\"NEC（Noise Contrastive Estimation）Loss[5]\"></a>NEC（Noise Contrastive Estimation）Loss[5]</h4><p>为了解决无法直接计算归一化因子的情况NEC，也就是 Noise Contrastive Estimation（噪声对比估计）最初在[4]中被提出，NCE通过对比真实样本与噪声样本，从而能够去估算出真实样本的概率分布的参数。具体做法是讲问题转化为一个二分类问题，在一批含噪声的样本中，判断样本是真实样本还是噪声样本。假设变量真实样本x服从概率$p(x)$，<script type=\"math/tex\">p(x) = \\frac{e^{G(x)}}{Z}</script>, <script type=\"math/tex\">Z=\\sum_x{e^{G(x)}}</script>则称为归一化函数。NEC通过计算$P(1|x)$来绕开直接计算$p(x)$带来的无法计算归一化函数的问题，</p>\n<script type=\"math/tex; mode=display\">p(1|x)=\\gamma (G(x;\\theta) - \\gamma) = \\frac{1}{1+e^{-G(x;\\theta)+\\gamma}} \\qquad (4)</script><p>设<script type=\"math/tex\">\\tilde{p(x)}</script>为真实样本的分布，$U(x)$是服从某一个确定的分布，则其loss为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta,\\gamma) = \\underset {\\theta,\\gamma}{arg min} - \\mathbb{E}_{x~\\tilde{~}{p(x)}}logp(1|x) - \\mathbb{E}_{x~\\tilde{~}{U(x)}}logp(0|x)\\qquad (5)</script><p>通过公式推导可以得到：</p>\n<script type=\"math/tex; mode=display\">\\tilde{p(x)} = exp{G(x;\\theta)-(\\gamma - logU(x))} \\qquad (6)</script><p>则式中的<script type=\"math/tex\">\\gamma - logU(x)</script>就是归一化常数，因此达到原来的目的。NEC很大的一个贡献是证明了仅适用负采样技术就可以加速归一化项的计算，因此为对比学习提供了很强大的理论支撑。</p>\n<h4 id=\"Triplet-Loss-6\"><a href=\"#Triplet-Loss-6\" class=\"headerlink\" title=\"Triplet Loss[6]\"></a>Triplet Loss[6]</h4><p>正如loss的名字所示，我们将被优化的目标设置为一个三元组$(x,x^+,x^-)$，那么三元组的整体距离为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = min \\{d(x,x^+)-d(x,x^-)+\\alpha, 0\\} \\qquad (7)</script><p>从公式中可以很直观的看到，它通过在使正例到锚点的距离尽可能小，负例到锚点的距离尽可能大的同时还做了一定的约束来防止过拟合。</p>\n<h4 id=\"InfoNCE-Loss-7\"><a href=\"#InfoNCE-Loss-7\" class=\"headerlink\" title=\"InfoNCE Loss[7]\"></a>InfoNCE Loss[7]</h4><p>InfoNCE是引入了互信息的NEC，首先回顾一下什么是互信息。设离散随机变量$X$和$Y$，$p(x,y)$是$X$和$Y$的联合分布，$p(x)$和$p(y)$分别是$X$和$Y$的边缘概率分布函数，则$X$和$Y$的互信息可以定义为：</p>\n<script type=\"math/tex; mode=display\">I(X;Y) =\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)}) \\qquad (8)</script><p>现在假设有一个生成模型<script type=\"math/tex\">p(x_{t+k}|c_t)</script>根据当前上下文<script type=\"math/tex\">c_t</script>预测<script type=\"math/tex\">k</script>个时刻后的数据<script type=\"math/tex\">x_{t+k}</script>，引入互信息后可以通过最大化当前上下文<script type=\"math/tex\">c_t</script>未来的数据<script type=\"math/tex\">x_{t+k}</script>之间的互信息来进行预测，根据条件概率:</p>\n<script type=\"math/tex; mode=display\">P(X=a|Y=b) = \\frac{P(X=a,Y=b)}{p(Y=b)} \\qquad (9)</script><p>结合公式（8）其互信息表示：</p>\n<script type=\"math/tex; mode=display\">I(x_{t+k};c_t) =\\sum_{x,c}p(x_{t+k},c_t)log\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \\qquad (10)</script><p>由于不知道<script type=\"math/tex\">c_t</script>和<script type=\"math/tex\">x_{t+k}</script>的联合概率分布<script type=\"math/tex\">p(x_{t+k},c_t)</script>，所以最大化互信息就要最大化<script type=\"math/tex\">\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}</script>。根据NCE中提供的思路，将问题转换为一个二分类的问题，从条件<script type=\"math/tex\">p(x_{t+k}|c_t)</script>中取出的数据为正样本，将它和上下文一起组成正样本对，类标签是1，将<script type=\"math/tex\">p(x_{t+k})</script>取出的样本为负样本对，它是和当前上下文没有必然联系的随机数据，将它和上下文构成负样本对，类标签为0。根据NCE中说明的设定，正样本选取1个；因为在NCE中证明了噪声分布与数据分布越接近越好，所以负样本就直接在当前序列中随机选取，负样本数量越多越好。所以最后将根据<script type=\"math/tex\">c_t</script>预测<script type=\"math/tex\">x_{t+k}</script>的问题就转化为区分正负样本的能力，其对应的交叉熵损失即InfoNCE loss为:</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{N} = -\\sum_{X}[p(x,c)log \\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}] = -\\mathbb{E_x}[log\\frac{f_k(x_{t+k},c_t)}{\\sum_{x_j\\in X}f_k(x_j,c_t)}]\\qquad (11)</script><p>实际上，InfoNCE不仅可以作为自监督学习中的对比损失函数，还可以作为互信息的一个估计器。</p>\n<h4 id=\"SCL-Loss-8\"><a href=\"#SCL-Loss-8\" class=\"headerlink\" title=\"SCL Loss[8]\"></a>SCL Loss[8]</h4><p>SCL loss即监督对比学习损失，是2021年斯坦福提出的在fine-tune阶段对交叉熵loss的改进损失。一般认为交叉熵loss具有泛化性能差和稳定性不好的缺点，尤其是在少量标签的样本中fine-tune时，介于此并考虑到对比学习可以利用少量的样本进行有效的学习，因此提出监督对比损失。监督对比损失没有像其他的对学习方法中那样采用自监督的学习方式，通过数据增强制造正样本，而是采用了监督任务的对比目标。训练目标的总loss是交叉熵loss和SCL loss的加权和：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L} = (1-\\lambda)\\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{SCL} \\qquad (12)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C y_{i,c} \\cdot log \\hat{y}_{i,c} \\qquad (13)</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{SCL} = \\sum_{i=1}^N -\\frac{1}{N_{y_i} - 1} \\sum_{j=1}^N\\mathrm{l}_{i \\not= j}\\mathrm{l}_{y_i \\not= y_j} log\\frac{exp(\\Phi(x_i)\\cdot \\Phi(x_j)/\\tau)}{\\sum_{k=1}^N\\mathrm{l}_{i \\not= k}exp(\\Phi(x_i)\\cdot \\Phi(x_k)/\\tau)} \\qquad (14)</script><h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[2] Shen D, Zheng M, Shen Y, et al. A simple but tough-to-beat data augmentation approach for natural language understanding and generation[J]. arXiv preprint arXiv:2009.13818, 2020.<br>[3] Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). IEEE, 2006, 2: 1735-1742.<br>[4] Gutmann M, Hyvärinen A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010: 297-304.<br>[5] Mnih A, Teh Y W. A fast and simple algorithm for training neural probabilistic language models[J]. arXiv preprint arXiv:1206.6426, 2012.<br>[6] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.<br>[7] Van den Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv e-prints, 2018: arXiv: 1807.03748.<br>[8] Gunel B, Du J, Conneau A, et al. Supervised contrastive learning for pre-trained language model fine-tuning[J]. arXiv preprint arXiv:2011.01403, 2020.</p>"},{"title":"跨语言预训练模型","date":"2022-03-18T11:30:17.000Z","mathjax":true,"_content":"\n目前在做国际化业务场景中的意图识别，主要探索的语种包括中文、英文、日文、阿文、西文和粤语的场景中。这一篇主要分享对于跨语言模型的调研，了解跨语言模型的现状，之后的文章会分享我自己在现有多语言模型上进行的训练过程，以及优化效果。\n<!--more-->\n\n\n### 跨语言模型调研\n\n众所周知，在bert推出后不久，就推出了多语言bert，简称mbert。mbert与bert的唯一区别在于训练语料的不同，mbert支持104种语言，采用了110k的共享词表。虽然mbert在多语言效果上差强人意，但是很显然在提高多语言表达能力上，预训练模型还有很多工作可以做！借鉴来自机器翻译的经验和自然语言语义对齐词嵌入的经验，在mbert之后陆续提出了XLM、XLM-R、LaBSE、InfoXLM、XLM-E、VECO以及ERNIE-M等预训练多语言模型。除了上述采用encoder的跨语言模型，还有关注与内容生成的序列到序列的跨语言模型MASS和mBART。下面主要讲解下这几个模型的核心思想和架构。\n\n\n#### XLM[1]\nfocebook在2019年提出了XLM模型，XLM主要是在bert的基础上，预训练借鉴了机器翻译中的翻译语言模型使用MLM+TLM或者CLM+MLM交替进行预训练。其中TLM是使用平行语料的监督方法，MLM和CLM是使用单语训练的无监督方法。同时为了平衡多种语言的分布差异，XLM在训练时采用一个缩放的分布对不同语种进行采样，以提高低资源语言的采样数。\n\n\n#### XLM-R[2]\n随后，focebook在2020年又提出支持100种语言的XLM-R模型，XLM-R采用了XLM的训练方法并使用RoBerta模型。XLM-R使用CommonCrawl数据集进行训练，其数据量较mbert使用的wiki有很大的增加，尤其是在低资源的语种上。实验表明XLM-R相较mbert在XNLI数据集上有很大的提升，在低资源的语种上提升也非常显著。在论文中也首次对影响多语言模型效果的因素进行了分析。主要是从高低资源的权衡以及语言采样和词汇大小的影响。论文通过实验证明了增加相似的语种进行训练可以提高该语种的效果，但是多语种会导致模型的参数增加，增加训练和推理成本。\n\n\n#### LaBSE[3]\nGoogle在2020年也推出了自己的跨语言模型LaBSE。LaBSE是基于bi-encoder结构的bert模型，LaBSE支持109种语言，使用MLM+TLM基于平行语料翻译排序任务进行预训练。LaBSE在Tatoeba数据集上获得83.7%的准确率，在此之前的最佳模型得分是65.5%。有意思的是Tatoeba数据集中的30+语种，并没有参与到LaBSE的训练，但是模型依然有较好的表现，展现了LaBSE模型zero-shot的能力。最后文章分析指出，使用预训练的bert可以减少对平行语料的需求。 使用CommonCrawl的数据训练的结果比使用wiki训练的效果好，因为CommonCrawl的数据量更大，噪声也更小。还有一个有趣的发现，使用预训练对比数据选择模型（CDS）选择出来的训练数据要比直接在原始数据上进行训练效果更好，这表明LaBSE对于数据的选择是敏感的。有关这一点我觉得大多数的模型对于数据的选择可以说都是敏感的。这类问题应该归属于领域泛化问题。\n\n\n\n#### XLM-E[4]\nXLM-E是微软提出的跨语言模型，它主要基于ELECTRA模型训练的，所以它是一种对抗生成式的模型。它的预训练包括两个任务，一个是多语言替换词检测（MRTD），另一个是翻译替换词检测（TRTD）。替换词检测是在单语语料上，针对每一个句子进行mask后让生成器生成对应的mask位置上的词，然后将生成的词替换原来的句子中的词，最后让判别器判别每个词是生成的还是原来的。翻译替换词检测是在翻译的平行语料句子对上，mask掉每个句子中的一部分词，让生成器去生成mask对应位置的上的词，用生成的词去替换原来句子中对应的词，最后再让判别器去判别该词是否是原来的词。\n\n\n#### InfoXLM[5]\nInfoXLM是微软开源的跨语言模型，它的创新在于受到信息论和对比学习的启发，设计了一个新的跨语言预训练任务cross-lingual contrast（XLCO）。InfoXLM的预训练的任务除了XLCO还有MMLM和TLM。其MMLM和TLM的损失函数是从词级别互信息的角度建立的。MMLM针对是单语语料输入的文本进行单词的mask来预测被mask的单词，最大化文本和被mask掉的单词的互信息，根据InfoNCE中的理论构建对比loss，$$L_{MMLM}$$。同理，TLM只是针对双语翻译的语料，对于同一对句子$$(c_1,c_2)$$，$$x_1$$是$$c_1$$中被mask掉的单词，目标是最大化句子对和被mask掉的词的互信息，这个互信息可以被拆解成两个部分，一部分是$$c1$$和$$x_1$$的互信息，另一部分是$$c_2$$和$$x_1|c_1$$的互信息，前一部分相当于是MMLM，后一部分则比较有趣，它其实是使用不同的语言来预测被mask掉的词，因此提高了模型跨语言迁移的能力，其loss为$$L_{TLM}$$。而XLCO的损失函数是最大化平行句子嵌入，是从句子级别的互信息角度建立的，其loss为$$L_{XLCO}$$。最后InfoXLM的训练loss为$$L = L_{MMLM} + L_{TLM} + L_{XLCO}$$\n\n\n#### ERNIE-M[6]\nERNIE-M是百度基于自己的预训练模型ERNIE在多语言语料上进行训练的跨语言模型。ERNIE-M主要是在预训练的过程中加入了机器翻译中经典的回译方法，对单语数据生成伪平行语料，来解决平行语料不足的问题。ERNIE-M的预训练任务主要包括交叉注意力掩码语言模型（CAMLM）、回译掩码语言模型（BTMLM）、多语言掩码语言模型（MMLM）、翻译语言模型（TLM）。\n\n\n这里主要介绍一下CAMLM和BTMLM。下面放两张有关预训练语言模型CAMLM和TLM与MMLM区别的图片，直观感受一下CAMLM的语言对齐的方式。CAMLM中对于被mask的词的词义推理仅依赖平行语料的的另一个句子，不能依赖自己所在的句子本身去推断，而MMLM和TLM则可以。\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\">\n\n\n对于BLMLM，其训练过程主要分为两个阶段：第一个阶段用源句子加掩码做为目标语言的句子，生成目标语言的翻译数据；第二个阶段将生成的伪平行语料输入，并对源句子进行掩码，训练掩码的输出。具体过程如下图：\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\">\n\n整体来说ERNIE-M效果还不错，在XNLI数据集上ERINE-M的效果比InfoXML要好。\n\n\n### 参考文献\n\n[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.\n[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.\n[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.\n[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.\n[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.\n[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.","source":"_posts/cross-lingual-PLM.md","raw":"---\ntitle: 跨语言预训练模型\ndate: 2022-03-18 19:30:17\ntags:\n- PLM\n- cross-lingual\ncategories: \n- 跨语言模型\nmathjax: true\n---\n\n目前在做国际化业务场景中的意图识别，主要探索的语种包括中文、英文、日文、阿文、西文和粤语的场景中。这一篇主要分享对于跨语言模型的调研，了解跨语言模型的现状，之后的文章会分享我自己在现有多语言模型上进行的训练过程，以及优化效果。\n<!--more-->\n\n\n### 跨语言模型调研\n\n众所周知，在bert推出后不久，就推出了多语言bert，简称mbert。mbert与bert的唯一区别在于训练语料的不同，mbert支持104种语言，采用了110k的共享词表。虽然mbert在多语言效果上差强人意，但是很显然在提高多语言表达能力上，预训练模型还有很多工作可以做！借鉴来自机器翻译的经验和自然语言语义对齐词嵌入的经验，在mbert之后陆续提出了XLM、XLM-R、LaBSE、InfoXLM、XLM-E、VECO以及ERNIE-M等预训练多语言模型。除了上述采用encoder的跨语言模型，还有关注与内容生成的序列到序列的跨语言模型MASS和mBART。下面主要讲解下这几个模型的核心思想和架构。\n\n\n#### XLM[1]\nfocebook在2019年提出了XLM模型，XLM主要是在bert的基础上，预训练借鉴了机器翻译中的翻译语言模型使用MLM+TLM或者CLM+MLM交替进行预训练。其中TLM是使用平行语料的监督方法，MLM和CLM是使用单语训练的无监督方法。同时为了平衡多种语言的分布差异，XLM在训练时采用一个缩放的分布对不同语种进行采样，以提高低资源语言的采样数。\n\n\n#### XLM-R[2]\n随后，focebook在2020年又提出支持100种语言的XLM-R模型，XLM-R采用了XLM的训练方法并使用RoBerta模型。XLM-R使用CommonCrawl数据集进行训练，其数据量较mbert使用的wiki有很大的增加，尤其是在低资源的语种上。实验表明XLM-R相较mbert在XNLI数据集上有很大的提升，在低资源的语种上提升也非常显著。在论文中也首次对影响多语言模型效果的因素进行了分析。主要是从高低资源的权衡以及语言采样和词汇大小的影响。论文通过实验证明了增加相似的语种进行训练可以提高该语种的效果，但是多语种会导致模型的参数增加，增加训练和推理成本。\n\n\n#### LaBSE[3]\nGoogle在2020年也推出了自己的跨语言模型LaBSE。LaBSE是基于bi-encoder结构的bert模型，LaBSE支持109种语言，使用MLM+TLM基于平行语料翻译排序任务进行预训练。LaBSE在Tatoeba数据集上获得83.7%的准确率，在此之前的最佳模型得分是65.5%。有意思的是Tatoeba数据集中的30+语种，并没有参与到LaBSE的训练，但是模型依然有较好的表现，展现了LaBSE模型zero-shot的能力。最后文章分析指出，使用预训练的bert可以减少对平行语料的需求。 使用CommonCrawl的数据训练的结果比使用wiki训练的效果好，因为CommonCrawl的数据量更大，噪声也更小。还有一个有趣的发现，使用预训练对比数据选择模型（CDS）选择出来的训练数据要比直接在原始数据上进行训练效果更好，这表明LaBSE对于数据的选择是敏感的。有关这一点我觉得大多数的模型对于数据的选择可以说都是敏感的。这类问题应该归属于领域泛化问题。\n\n\n\n#### XLM-E[4]\nXLM-E是微软提出的跨语言模型，它主要基于ELECTRA模型训练的，所以它是一种对抗生成式的模型。它的预训练包括两个任务，一个是多语言替换词检测（MRTD），另一个是翻译替换词检测（TRTD）。替换词检测是在单语语料上，针对每一个句子进行mask后让生成器生成对应的mask位置上的词，然后将生成的词替换原来的句子中的词，最后让判别器判别每个词是生成的还是原来的。翻译替换词检测是在翻译的平行语料句子对上，mask掉每个句子中的一部分词，让生成器去生成mask对应位置的上的词，用生成的词去替换原来句子中对应的词，最后再让判别器去判别该词是否是原来的词。\n\n\n#### InfoXLM[5]\nInfoXLM是微软开源的跨语言模型，它的创新在于受到信息论和对比学习的启发，设计了一个新的跨语言预训练任务cross-lingual contrast（XLCO）。InfoXLM的预训练的任务除了XLCO还有MMLM和TLM。其MMLM和TLM的损失函数是从词级别互信息的角度建立的。MMLM针对是单语语料输入的文本进行单词的mask来预测被mask的单词，最大化文本和被mask掉的单词的互信息，根据InfoNCE中的理论构建对比loss，$$L_{MMLM}$$。同理，TLM只是针对双语翻译的语料，对于同一对句子$$(c_1,c_2)$$，$$x_1$$是$$c_1$$中被mask掉的单词，目标是最大化句子对和被mask掉的词的互信息，这个互信息可以被拆解成两个部分，一部分是$$c1$$和$$x_1$$的互信息，另一部分是$$c_2$$和$$x_1|c_1$$的互信息，前一部分相当于是MMLM，后一部分则比较有趣，它其实是使用不同的语言来预测被mask掉的词，因此提高了模型跨语言迁移的能力，其loss为$$L_{TLM}$$。而XLCO的损失函数是最大化平行句子嵌入，是从句子级别的互信息角度建立的，其loss为$$L_{XLCO}$$。最后InfoXLM的训练loss为$$L = L_{MMLM} + L_{TLM} + L_{XLCO}$$\n\n\n#### ERNIE-M[6]\nERNIE-M是百度基于自己的预训练模型ERNIE在多语言语料上进行训练的跨语言模型。ERNIE-M主要是在预训练的过程中加入了机器翻译中经典的回译方法，对单语数据生成伪平行语料，来解决平行语料不足的问题。ERNIE-M的预训练任务主要包括交叉注意力掩码语言模型（CAMLM）、回译掩码语言模型（BTMLM）、多语言掩码语言模型（MMLM）、翻译语言模型（TLM）。\n\n\n这里主要介绍一下CAMLM和BTMLM。下面放两张有关预训练语言模型CAMLM和TLM与MMLM区别的图片，直观感受一下CAMLM的语言对齐的方式。CAMLM中对于被mask的词的词义推理仅依赖平行语料的的另一个句子，不能依赖自己所在的句子本身去推断，而MMLM和TLM则可以。\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\">\n\n\n对于BLMLM，其训练过程主要分为两个阶段：第一个阶段用源句子加掩码做为目标语言的句子，生成目标语言的翻译数据；第二个阶段将生成的伪平行语料输入，并对源句子进行掩码，训练掩码的输出。具体过程如下图：\n<img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\">\n\n整体来说ERNIE-M效果还不错，在XNLI数据集上ERINE-M的效果比InfoXML要好。\n\n\n### 参考文献\n\n[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.\n[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.\n[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.\n[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.\n[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.\n[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.","slug":"cross-lingual-PLM","published":1,"updated":"2022-06-07T07:12:05.510Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6q0008q4uxfjuh6b38","content":"<p>目前在做国际化业务场景中的意图识别，主要探索的语种包括中文、英文、日文、阿文、西文和粤语的场景中。这一篇主要分享对于跨语言模型的调研，了解跨语言模型的现状，之后的文章会分享我自己在现有多语言模型上进行的训练过程，以及优化效果。<br><span id=\"more\"></span></p>\n<h3 id=\"跨语言模型调研\"><a href=\"#跨语言模型调研\" class=\"headerlink\" title=\"跨语言模型调研\"></a>跨语言模型调研</h3><p>众所周知，在bert推出后不久，就推出了多语言bert，简称mbert。mbert与bert的唯一区别在于训练语料的不同，mbert支持104种语言，采用了110k的共享词表。虽然mbert在多语言效果上差强人意，但是很显然在提高多语言表达能力上，预训练模型还有很多工作可以做！借鉴来自机器翻译的经验和自然语言语义对齐词嵌入的经验，在mbert之后陆续提出了XLM、XLM-R、LaBSE、InfoXLM、XLM-E、VECO以及ERNIE-M等预训练多语言模型。除了上述采用encoder的跨语言模型，还有关注与内容生成的序列到序列的跨语言模型MASS和mBART。下面主要讲解下这几个模型的核心思想和架构。</p>\n<h4 id=\"XLM-1\"><a href=\"#XLM-1\" class=\"headerlink\" title=\"XLM[1]\"></a>XLM[1]</h4><p>focebook在2019年提出了XLM模型，XLM主要是在bert的基础上，预训练借鉴了机器翻译中的翻译语言模型使用MLM+TLM或者CLM+MLM交替进行预训练。其中TLM是使用平行语料的监督方法，MLM和CLM是使用单语训练的无监督方法。同时为了平衡多种语言的分布差异，XLM在训练时采用一个缩放的分布对不同语种进行采样，以提高低资源语言的采样数。</p>\n<h4 id=\"XLM-R-2\"><a href=\"#XLM-R-2\" class=\"headerlink\" title=\"XLM-R[2]\"></a>XLM-R[2]</h4><p>随后，focebook在2020年又提出支持100种语言的XLM-R模型，XLM-R采用了XLM的训练方法并使用RoBerta模型。XLM-R使用CommonCrawl数据集进行训练，其数据量较mbert使用的wiki有很大的增加，尤其是在低资源的语种上。实验表明XLM-R相较mbert在XNLI数据集上有很大的提升，在低资源的语种上提升也非常显著。在论文中也首次对影响多语言模型效果的因素进行了分析。主要是从高低资源的权衡以及语言采样和词汇大小的影响。论文通过实验证明了增加相似的语种进行训练可以提高该语种的效果，但是多语种会导致模型的参数增加，增加训练和推理成本。</p>\n<h4 id=\"LaBSE-3\"><a href=\"#LaBSE-3\" class=\"headerlink\" title=\"LaBSE[3]\"></a>LaBSE[3]</h4><p>Google在2020年也推出了自己的跨语言模型LaBSE。LaBSE是基于bi-encoder结构的bert模型，LaBSE支持109种语言，使用MLM+TLM基于平行语料翻译排序任务进行预训练。LaBSE在Tatoeba数据集上获得83.7%的准确率，在此之前的最佳模型得分是65.5%。有意思的是Tatoeba数据集中的30+语种，并没有参与到LaBSE的训练，但是模型依然有较好的表现，展现了LaBSE模型zero-shot的能力。最后文章分析指出，使用预训练的bert可以减少对平行语料的需求。 使用CommonCrawl的数据训练的结果比使用wiki训练的效果好，因为CommonCrawl的数据量更大，噪声也更小。还有一个有趣的发现，使用预训练对比数据选择模型（CDS）选择出来的训练数据要比直接在原始数据上进行训练效果更好，这表明LaBSE对于数据的选择是敏感的。有关这一点我觉得大多数的模型对于数据的选择可以说都是敏感的。这类问题应该归属于领域泛化问题。</p>\n<h4 id=\"XLM-E-4\"><a href=\"#XLM-E-4\" class=\"headerlink\" title=\"XLM-E[4]\"></a>XLM-E[4]</h4><p>XLM-E是微软提出的跨语言模型，它主要基于ELECTRA模型训练的，所以它是一种对抗生成式的模型。它的预训练包括两个任务，一个是多语言替换词检测（MRTD），另一个是翻译替换词检测（TRTD）。替换词检测是在单语语料上，针对每一个句子进行mask后让生成器生成对应的mask位置上的词，然后将生成的词替换原来的句子中的词，最后让判别器判别每个词是生成的还是原来的。翻译替换词检测是在翻译的平行语料句子对上，mask掉每个句子中的一部分词，让生成器去生成mask对应位置的上的词，用生成的词去替换原来句子中对应的词，最后再让判别器去判别该词是否是原来的词。</p>\n<h4 id=\"InfoXLM-5\"><a href=\"#InfoXLM-5\" class=\"headerlink\" title=\"InfoXLM[5]\"></a>InfoXLM[5]</h4><p>InfoXLM是微软开源的跨语言模型，它的创新在于受到信息论和对比学习的启发，设计了一个新的跨语言预训练任务cross-lingual contrast（XLCO）。InfoXLM的预训练的任务除了XLCO还有MMLM和TLM。其MMLM和TLM的损失函数是从词级别互信息的角度建立的。MMLM针对是单语语料输入的文本进行单词的mask来预测被mask的单词，最大化文本和被mask掉的单词的互信息，根据InfoNCE中的理论构建对比loss，<script type=\"math/tex\">L_{MMLM}</script>。同理，TLM只是针对双语翻译的语料，对于同一对句子<script type=\"math/tex\">(c_1,c_2)</script>，<script type=\"math/tex\">x_1</script>是<script type=\"math/tex\">c_1</script>中被mask掉的单词，目标是最大化句子对和被mask掉的词的互信息，这个互信息可以被拆解成两个部分，一部分是<script type=\"math/tex\">c1</script>和<script type=\"math/tex\">x_1</script>的互信息，另一部分是<script type=\"math/tex\">c_2</script>和<script type=\"math/tex\">x_1|c_1</script>的互信息，前一部分相当于是MMLM，后一部分则比较有趣，它其实是使用不同的语言来预测被mask掉的词，因此提高了模型跨语言迁移的能力，其loss为<script type=\"math/tex\">L_{TLM}</script>。而XLCO的损失函数是最大化平行句子嵌入，是从句子级别的互信息角度建立的，其loss为<script type=\"math/tex\">L_{XLCO}</script>。最后InfoXLM的训练loss为<script type=\"math/tex\">L = L_{MMLM} + L_{TLM} + L_{XLCO}</script></p>\n<h4 id=\"ERNIE-M-6\"><a href=\"#ERNIE-M-6\" class=\"headerlink\" title=\"ERNIE-M[6]\"></a>ERNIE-M[6]</h4><p>ERNIE-M是百度基于自己的预训练模型ERNIE在多语言语料上进行训练的跨语言模型。ERNIE-M主要是在预训练的过程中加入了机器翻译中经典的回译方法，对单语数据生成伪平行语料，来解决平行语料不足的问题。ERNIE-M的预训练任务主要包括交叉注意力掩码语言模型（CAMLM）、回译掩码语言模型（BTMLM）、多语言掩码语言模型（MMLM）、翻译语言模型（TLM）。</p>\n<p>这里主要介绍一下CAMLM和BTMLM。下面放两张有关预训练语言模型CAMLM和TLM与MMLM区别的图片，直观感受一下CAMLM的语言对齐的方式。CAMLM中对于被mask的词的词义推理仅依赖平行语料的的另一个句子，不能依赖自己所在的句子本身去推断，而MMLM和TLM则可以。<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\"></p>\n<p>对于BLMLM，其训练过程主要分为两个阶段：第一个阶段用源句子加掩码做为目标语言的句子，生成目标语言的翻译数据；第二个阶段将生成的伪平行语料输入，并对源句子进行掩码，训练掩码的输出。具体过程如下图：<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\"></p>\n<p>整体来说ERNIE-M效果还不错，在XNLI数据集上ERINE-M的效果比InfoXML要好。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.<br>[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.<br>[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.<br>[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.<br>[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.<br>[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.</p>\n","site":{"data":{}},"excerpt":"<p>目前在做国际化业务场景中的意图识别，主要探索的语种包括中文、英文、日文、阿文、西文和粤语的场景中。这一篇主要分享对于跨语言模型的调研，了解跨语言模型的现状，之后的文章会分享我自己在现有多语言模型上进行的训练过程，以及优化效果。<br>","more":"</p>\n<h3 id=\"跨语言模型调研\"><a href=\"#跨语言模型调研\" class=\"headerlink\" title=\"跨语言模型调研\"></a>跨语言模型调研</h3><p>众所周知，在bert推出后不久，就推出了多语言bert，简称mbert。mbert与bert的唯一区别在于训练语料的不同，mbert支持104种语言，采用了110k的共享词表。虽然mbert在多语言效果上差强人意，但是很显然在提高多语言表达能力上，预训练模型还有很多工作可以做！借鉴来自机器翻译的经验和自然语言语义对齐词嵌入的经验，在mbert之后陆续提出了XLM、XLM-R、LaBSE、InfoXLM、XLM-E、VECO以及ERNIE-M等预训练多语言模型。除了上述采用encoder的跨语言模型，还有关注与内容生成的序列到序列的跨语言模型MASS和mBART。下面主要讲解下这几个模型的核心思想和架构。</p>\n<h4 id=\"XLM-1\"><a href=\"#XLM-1\" class=\"headerlink\" title=\"XLM[1]\"></a>XLM[1]</h4><p>focebook在2019年提出了XLM模型，XLM主要是在bert的基础上，预训练借鉴了机器翻译中的翻译语言模型使用MLM+TLM或者CLM+MLM交替进行预训练。其中TLM是使用平行语料的监督方法，MLM和CLM是使用单语训练的无监督方法。同时为了平衡多种语言的分布差异，XLM在训练时采用一个缩放的分布对不同语种进行采样，以提高低资源语言的采样数。</p>\n<h4 id=\"XLM-R-2\"><a href=\"#XLM-R-2\" class=\"headerlink\" title=\"XLM-R[2]\"></a>XLM-R[2]</h4><p>随后，focebook在2020年又提出支持100种语言的XLM-R模型，XLM-R采用了XLM的训练方法并使用RoBerta模型。XLM-R使用CommonCrawl数据集进行训练，其数据量较mbert使用的wiki有很大的增加，尤其是在低资源的语种上。实验表明XLM-R相较mbert在XNLI数据集上有很大的提升，在低资源的语种上提升也非常显著。在论文中也首次对影响多语言模型效果的因素进行了分析。主要是从高低资源的权衡以及语言采样和词汇大小的影响。论文通过实验证明了增加相似的语种进行训练可以提高该语种的效果，但是多语种会导致模型的参数增加，增加训练和推理成本。</p>\n<h4 id=\"LaBSE-3\"><a href=\"#LaBSE-3\" class=\"headerlink\" title=\"LaBSE[3]\"></a>LaBSE[3]</h4><p>Google在2020年也推出了自己的跨语言模型LaBSE。LaBSE是基于bi-encoder结构的bert模型，LaBSE支持109种语言，使用MLM+TLM基于平行语料翻译排序任务进行预训练。LaBSE在Tatoeba数据集上获得83.7%的准确率，在此之前的最佳模型得分是65.5%。有意思的是Tatoeba数据集中的30+语种，并没有参与到LaBSE的训练，但是模型依然有较好的表现，展现了LaBSE模型zero-shot的能力。最后文章分析指出，使用预训练的bert可以减少对平行语料的需求。 使用CommonCrawl的数据训练的结果比使用wiki训练的效果好，因为CommonCrawl的数据量更大，噪声也更小。还有一个有趣的发现，使用预训练对比数据选择模型（CDS）选择出来的训练数据要比直接在原始数据上进行训练效果更好，这表明LaBSE对于数据的选择是敏感的。有关这一点我觉得大多数的模型对于数据的选择可以说都是敏感的。这类问题应该归属于领域泛化问题。</p>\n<h4 id=\"XLM-E-4\"><a href=\"#XLM-E-4\" class=\"headerlink\" title=\"XLM-E[4]\"></a>XLM-E[4]</h4><p>XLM-E是微软提出的跨语言模型，它主要基于ELECTRA模型训练的，所以它是一种对抗生成式的模型。它的预训练包括两个任务，一个是多语言替换词检测（MRTD），另一个是翻译替换词检测（TRTD）。替换词检测是在单语语料上，针对每一个句子进行mask后让生成器生成对应的mask位置上的词，然后将生成的词替换原来的句子中的词，最后让判别器判别每个词是生成的还是原来的。翻译替换词检测是在翻译的平行语料句子对上，mask掉每个句子中的一部分词，让生成器去生成mask对应位置的上的词，用生成的词去替换原来句子中对应的词，最后再让判别器去判别该词是否是原来的词。</p>\n<h4 id=\"InfoXLM-5\"><a href=\"#InfoXLM-5\" class=\"headerlink\" title=\"InfoXLM[5]\"></a>InfoXLM[5]</h4><p>InfoXLM是微软开源的跨语言模型，它的创新在于受到信息论和对比学习的启发，设计了一个新的跨语言预训练任务cross-lingual contrast（XLCO）。InfoXLM的预训练的任务除了XLCO还有MMLM和TLM。其MMLM和TLM的损失函数是从词级别互信息的角度建立的。MMLM针对是单语语料输入的文本进行单词的mask来预测被mask的单词，最大化文本和被mask掉的单词的互信息，根据InfoNCE中的理论构建对比loss，<script type=\"math/tex\">L_{MMLM}</script>。同理，TLM只是针对双语翻译的语料，对于同一对句子<script type=\"math/tex\">(c_1,c_2)</script>，<script type=\"math/tex\">x_1</script>是<script type=\"math/tex\">c_1</script>中被mask掉的单词，目标是最大化句子对和被mask掉的词的互信息，这个互信息可以被拆解成两个部分，一部分是<script type=\"math/tex\">c1</script>和<script type=\"math/tex\">x_1</script>的互信息，另一部分是<script type=\"math/tex\">c_2</script>和<script type=\"math/tex\">x_1|c_1</script>的互信息，前一部分相当于是MMLM，后一部分则比较有趣，它其实是使用不同的语言来预测被mask掉的词，因此提高了模型跨语言迁移的能力，其loss为<script type=\"math/tex\">L_{TLM}</script>。而XLCO的损失函数是最大化平行句子嵌入，是从句子级别的互信息角度建立的，其loss为<script type=\"math/tex\">L_{XLCO}</script>。最后InfoXLM的训练loss为<script type=\"math/tex\">L = L_{MMLM} + L_{TLM} + L_{XLCO}</script></p>\n<h4 id=\"ERNIE-M-6\"><a href=\"#ERNIE-M-6\" class=\"headerlink\" title=\"ERNIE-M[6]\"></a>ERNIE-M[6]</h4><p>ERNIE-M是百度基于自己的预训练模型ERNIE在多语言语料上进行训练的跨语言模型。ERNIE-M主要是在预训练的过程中加入了机器翻译中经典的回译方法，对单语数据生成伪平行语料，来解决平行语料不足的问题。ERNIE-M的预训练任务主要包括交叉注意力掩码语言模型（CAMLM）、回译掩码语言模型（BTMLM）、多语言掩码语言模型（MMLM）、翻译语言模型（TLM）。</p>\n<p>这里主要介绍一下CAMLM和BTMLM。下面放两张有关预训练语言模型CAMLM和TLM与MMLM区别的图片，直观感受一下CAMLM的语言对齐的方式。CAMLM中对于被mask的词的词义推理仅依赖平行语料的的另一个句子，不能依赖自己所在的句子本身去推断，而MMLM和TLM则可以。<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/CAMLM.png\" width=\"80%\" height=\"50%\"></p>\n<p>对于BLMLM，其训练过程主要分为两个阶段：第一个阶段用源句子加掩码做为目标语言的句子，生成目标语言的翻译数据；第二个阶段将生成的伪平行语料输入，并对源句子进行掩码，训练掩码的输出。具体过程如下图：<br><img src=\"https://github.com/Quelisa/picture/raw/main/LM/BTMLM.png\" width=\"50%\" height=\"60%\"></p>\n<p>整体来说ERNIE-M效果还不错，在XNLI数据集上ERINE-M的效果比InfoXML要好。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Lample G, Conneau A. Cross-lingual language model pretraining[J]. arXiv preprint arXiv:1901.07291, 2019.<br>[2] Conneau A, Khandelwal K, Goyal N, et al. Unsupervised cross-lingual representation learning at scale[J]. arXiv preprint arXiv:1911.02116, 2019.<br>[3] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.<br>[4] Chi Z, Huang S, Dong L, et al. XLM-E: cross-lingual language model pre-training via ELECTRA[J]. arXiv preprint arXiv:2106.16138, 2021.<br>[5] Chi Z, Dong L, Wei F, et al. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training[J]. arXiv preprint arXiv:2007.07834, 2020.<br>[6] Ouyang X, Wang S, Pang C, et al. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.</p>"},{"title":"GALAXY：面向任务型对话的生成式预训练模型","date":"2022-01-11T08:43:21.000Z","_content":"\nGALAXY[1]是阿里提出的一个针对任务型对话的预训练对话模型（Pre-trained Conversation Model，PCM），它引入了半监督学习对对话策略进行学习，在In-Car，MultiWOZ2.0和MultiWOZ2.1数据集上达到了SOTA。\n\n<!--more-->\n\n### 任务型对话技术现状\n任务型对话一般分为三个阶段：（1）自然语言理解；（2）对话策略以及（3）对话生成。对话策略主要是用来连接自然语言理解到自然语言生成部分的逻辑。任务型对话有两种主流实现，一种是pipeline的实现方式，另一种是端到端的实现。随着预训练语言模型的兴起，研究人员更多的开始研究基于预训练的对话模型。以往的对话模型更多的关注自然语言理解和自然语言生成部分，从而忽视了任务型中很重要的对话策略，GALAXY强调了对话策略的重要性，基于UniLM模型框架建立了新的预训练对话模型。\n\n\n### 基于预训练对话模型（PCM）的任务型挑战\n\n1. 缺乏大量连续对话数据\n2. 大部分对话数据没有标签\n3. 自监督的标签使得无法对对话动作空间进行探索，提取知识有限\n\n\n### GALAXY的做了啥\n\n1. 从现有的对话数据集中清洗出有标签数据集UniDA和无标签数据集UnDial\n2. 基于UniLM建立了优化Loss为对话选择、对话生成、对话预测和KL散度之和的半监督预训练对话模型\n\n下图是GALAXY的架构:\n<img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\">\n\n可以看到左侧为模型的输入包括对话文本，对话角色对话伦次以及词在文本中的位置信息，右侧是模型的主结构主结构中将分为三种网络连接，一种是context到context的自关注全连接网络，一种是conext到response关注的全连接网络，另一种是双向的context到response全连接网络，针对context的自关注计算对话预测损失和KL散度（这里之所以加一个KL散度是为了是模型的泛化能力更好），针对context到response的单向网络连接计算对话生成损失，针对全连接关注计算对话选择的损失。预训练模型的损失是上述损失之和，微调的损失是对话选择、对话生成和对话预测损失之和。其中只有对话预测要求有监督的数据进行训练，所以说GALAXY是一种半监督的预训练对话模型。\n\n\n### 参考文献\n[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.","source":"_posts/galaxy-task-oriented-dialog.md","raw":"---\ntitle: GALAXY：面向任务型对话的生成式预训练模型\ndate: 2022-01-11 16:43:21\ntags:\n- PCM\n- Dialog\ncategories: \n- 任务型对话\n---\n\nGALAXY[1]是阿里提出的一个针对任务型对话的预训练对话模型（Pre-trained Conversation Model，PCM），它引入了半监督学习对对话策略进行学习，在In-Car，MultiWOZ2.0和MultiWOZ2.1数据集上达到了SOTA。\n\n<!--more-->\n\n### 任务型对话技术现状\n任务型对话一般分为三个阶段：（1）自然语言理解；（2）对话策略以及（3）对话生成。对话策略主要是用来连接自然语言理解到自然语言生成部分的逻辑。任务型对话有两种主流实现，一种是pipeline的实现方式，另一种是端到端的实现。随着预训练语言模型的兴起，研究人员更多的开始研究基于预训练的对话模型。以往的对话模型更多的关注自然语言理解和自然语言生成部分，从而忽视了任务型中很重要的对话策略，GALAXY强调了对话策略的重要性，基于UniLM模型框架建立了新的预训练对话模型。\n\n\n### 基于预训练对话模型（PCM）的任务型挑战\n\n1. 缺乏大量连续对话数据\n2. 大部分对话数据没有标签\n3. 自监督的标签使得无法对对话动作空间进行探索，提取知识有限\n\n\n### GALAXY的做了啥\n\n1. 从现有的对话数据集中清洗出有标签数据集UniDA和无标签数据集UnDial\n2. 基于UniLM建立了优化Loss为对话选择、对话生成、对话预测和KL散度之和的半监督预训练对话模型\n\n下图是GALAXY的架构:\n<img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\">\n\n可以看到左侧为模型的输入包括对话文本，对话角色对话伦次以及词在文本中的位置信息，右侧是模型的主结构主结构中将分为三种网络连接，一种是context到context的自关注全连接网络，一种是conext到response关注的全连接网络，另一种是双向的context到response全连接网络，针对context的自关注计算对话预测损失和KL散度（这里之所以加一个KL散度是为了是模型的泛化能力更好），针对context到response的单向网络连接计算对话生成损失，针对全连接关注计算对话选择的损失。预训练模型的损失是上述损失之和，微调的损失是对话选择、对话生成和对话预测损失之和。其中只有对话预测要求有监督的数据进行训练，所以说GALAXY是一种半监督的预训练对话模型。\n\n\n### 参考文献\n[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.","slug":"galaxy-task-oriented-dialog","published":1,"updated":"2022-06-07T07:12:31.362Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6r0009q4ux270ib8fz","content":"<p>GALAXY[1]是阿里提出的一个针对任务型对话的预训练对话模型（Pre-trained Conversation Model，PCM），它引入了半监督学习对对话策略进行学习，在In-Car，MultiWOZ2.0和MultiWOZ2.1数据集上达到了SOTA。</p>\n<span id=\"more\"></span>\n<h3 id=\"任务型对话技术现状\"><a href=\"#任务型对话技术现状\" class=\"headerlink\" title=\"任务型对话技术现状\"></a>任务型对话技术现状</h3><p>任务型对话一般分为三个阶段：（1）自然语言理解；（2）对话策略以及（3）对话生成。对话策略主要是用来连接自然语言理解到自然语言生成部分的逻辑。任务型对话有两种主流实现，一种是pipeline的实现方式，另一种是端到端的实现。随着预训练语言模型的兴起，研究人员更多的开始研究基于预训练的对话模型。以往的对话模型更多的关注自然语言理解和自然语言生成部分，从而忽视了任务型中很重要的对话策略，GALAXY强调了对话策略的重要性，基于UniLM模型框架建立了新的预训练对话模型。</p>\n<h3 id=\"基于预训练对话模型（PCM）的任务型挑战\"><a href=\"#基于预训练对话模型（PCM）的任务型挑战\" class=\"headerlink\" title=\"基于预训练对话模型（PCM）的任务型挑战\"></a>基于预训练对话模型（PCM）的任务型挑战</h3><ol>\n<li>缺乏大量连续对话数据</li>\n<li>大部分对话数据没有标签</li>\n<li>自监督的标签使得无法对对话动作空间进行探索，提取知识有限</li>\n</ol>\n<h3 id=\"GALAXY的做了啥\"><a href=\"#GALAXY的做了啥\" class=\"headerlink\" title=\"GALAXY的做了啥\"></a>GALAXY的做了啥</h3><ol>\n<li>从现有的对话数据集中清洗出有标签数据集UniDA和无标签数据集UnDial</li>\n<li>基于UniLM建立了优化Loss为对话选择、对话生成、对话预测和KL散度之和的半监督预训练对话模型</li>\n</ol>\n<p>下图是GALAXY的架构:<br><img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\"></p>\n<p>可以看到左侧为模型的输入包括对话文本，对话角色对话伦次以及词在文本中的位置信息，右侧是模型的主结构主结构中将分为三种网络连接，一种是context到context的自关注全连接网络，一种是conext到response关注的全连接网络，另一种是双向的context到response全连接网络，针对context的自关注计算对话预测损失和KL散度（这里之所以加一个KL散度是为了是模型的泛化能力更好），针对context到response的单向网络连接计算对话生成损失，针对全连接关注计算对话选择的损失。预训练模型的损失是上述损失之和，微调的损失是对话选择、对话生成和对话预测损失之和。其中只有对话预测要求有监督的数据进行训练，所以说GALAXY是一种半监督的预训练对话模型。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.</p>\n","site":{"data":{}},"excerpt":"<p>GALAXY[1]是阿里提出的一个针对任务型对话的预训练对话模型（Pre-trained Conversation Model，PCM），它引入了半监督学习对对话策略进行学习，在In-Car，MultiWOZ2.0和MultiWOZ2.1数据集上达到了SOTA。</p>","more":"<h3 id=\"任务型对话技术现状\"><a href=\"#任务型对话技术现状\" class=\"headerlink\" title=\"任务型对话技术现状\"></a>任务型对话技术现状</h3><p>任务型对话一般分为三个阶段：（1）自然语言理解；（2）对话策略以及（3）对话生成。对话策略主要是用来连接自然语言理解到自然语言生成部分的逻辑。任务型对话有两种主流实现，一种是pipeline的实现方式，另一种是端到端的实现。随着预训练语言模型的兴起，研究人员更多的开始研究基于预训练的对话模型。以往的对话模型更多的关注自然语言理解和自然语言生成部分，从而忽视了任务型中很重要的对话策略，GALAXY强调了对话策略的重要性，基于UniLM模型框架建立了新的预训练对话模型。</p>\n<h3 id=\"基于预训练对话模型（PCM）的任务型挑战\"><a href=\"#基于预训练对话模型（PCM）的任务型挑战\" class=\"headerlink\" title=\"基于预训练对话模型（PCM）的任务型挑战\"></a>基于预训练对话模型（PCM）的任务型挑战</h3><ol>\n<li>缺乏大量连续对话数据</li>\n<li>大部分对话数据没有标签</li>\n<li>自监督的标签使得无法对对话动作空间进行探索，提取知识有限</li>\n</ol>\n<h3 id=\"GALAXY的做了啥\"><a href=\"#GALAXY的做了啥\" class=\"headerlink\" title=\"GALAXY的做了啥\"></a>GALAXY的做了啥</h3><ol>\n<li>从现有的对话数据集中清洗出有标签数据集UniDA和无标签数据集UnDial</li>\n<li>基于UniLM建立了优化Loss为对话选择、对话生成、对话预测和KL散度之和的半监督预训练对话模型</li>\n</ol>\n<p>下图是GALAXY的架构:<br><img src=\"https://github.com/Quelisa/picture/raw/main/Dialogue/GALAXY.png\" width=\"80%\" height=\"50%\"></p>\n<p>可以看到左侧为模型的输入包括对话文本，对话角色对话伦次以及词在文本中的位置信息，右侧是模型的主结构主结构中将分为三种网络连接，一种是context到context的自关注全连接网络，一种是conext到response关注的全连接网络，另一种是双向的context到response全连接网络，针对context的自关注计算对话预测损失和KL散度（这里之所以加一个KL散度是为了是模型的泛化能力更好），针对context到response的单向网络连接计算对话生成损失，针对全连接关注计算对话选择的损失。预训练模型的损失是上述损失之和，微调的损失是对话选择、对话生成和对话预测损失之和。其中只有对话预测要求有监督的数据进行训练，所以说GALAXY是一种半监督的预训练对话模型。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] He W, Dai Y, Zheng Y, et al. GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection[J]. arXiv preprint arXiv:2111.14592, 2021.</p>"},{"title":"粤语模型训练系列之数据收集","date":"2022-03-10T03:13:48.000Z","_content":"\n最近在做粤语模型的训练，训练的第一步，当然就是要收集数据啦！这一篇主要分享粤语语料的收集和数据处理。\n<!--more-->\n\n\n### 粤语语料收集\n提到多语言语料收集，Wikipedia的数据绝对是第一个想到，Wikipedia有一个专门备份的网站[2]，提高各个语种信息的备份，所有的网页内容都可以在此下载。我下载的是20220501的数据，粤语wiki的大小是88M。我在huggingface的数据集中搜到了中文和粤语的翻译平行语料：x-tech/cantonese-mandarin-translation[3]。网上有关粤语的语料有一个叫《香港二十世纪中期粤语语料库》[4]，该语料库需要注册登录，是一个检索式的语料库且不可下载。除此之外，可以直接拿到的数据少之又少0.0。所以就需要用到爬虫工具爬取一些粤语的微博、博客、电影对话等。记录一下粤语的网站：\n1. http://www.hkcna.hk/index.jsp?channel=2803\n2. https://weibo.com/u/6227634537?refer_flag=1005055013_\n3. https://subanana.com/ 是一个AI自动生成粤语字幕的网站\n4. https://commonvoice.mozilla.org/zh-CN/datasets\n\n\n### 网络爬虫介绍\n网上搜索了一堆的爬虫应用和脚本，真正免费而又好用的没几个。正好淘一淘，也学习一下爬虫。找到了几个看起来还不错的爬虫服务，但是用起来体验一般，不过还是可以记录一下，毕竟做的还不错，也是一种商业化渠道。\n1. https://app.diffbot.com/\n2. https://commoncrawl.org/the-data/get-started/\n3. https://webz.io/\n4. https://www.parsehub.com/\n5. https://www.scrapingbee.com/\n\n\n### 语料数据清洗\n前面提到的wiki数据还是带有网络格式的数据，并不能直接用于模型训练，而且数据质量并没有经过筛选。下面介绍一个好用的工具GENSIM[5]，用来直接抽取wiki中的文本数据。下面的代码可以抽取出wiki的内容，但是它会去掉所有的标点符号用空格代替。\n\n```python\nfrom gensim.corpora import WikiCorpus\n\nif __name__ == '__main__':\n    output = open('wiki.txt', 'w', encoding='utf8')\n    wiki = WikiCorpus(\"wiki-xxx.xml.bz2\", dictionary={})\n    for text in wiki.get_texts():\n        output.write(\" \".join(text) + \"\\n\")\n    output.close()\t\t \n```\n\n后续参考苏神的代码，重新对wiki数据进行了整理，清洗代码在代码仓库：https://github.com/Quelisa/data_cleaner.git\n\n### 总结\n\n因为粤语的收集难度确实大于中文的收集难度，并且我们已经知道这两个模型都支持中文并且都已经在大规模的中文数据集上进行了训练，所以我打算利用一些现成的中文数据集和收集到的粤语语料分别进行单语种的训练，然后主要通过收集到的平行语料进行中文和粤语的对齐训练，因为可用的平行语料不是很多所以可以通过翻译的方式获得。语料收集是算法训练中看似枯燥，却又极其重要的一步，语料的质量直接关系到模型效果的好坏，可谓是失之毫厘谬以千里。收集并设计好的语料库，高效的开发工具和科学的处理方法同样重要！第一次真正意义上感受到了big data和数据质量的重要性！\n\n\n目前专门针对粤语的预训练语言模型只有哈工大讯飞联合实验室发布的CINO (Chinese minority PLM)[1]，理论上具有和粤语语系相近的多语言模型也具有一定的粤语识别能力。在LaBSE的论文中看到,虽然没有用到粤语训练，但是粤语的识别效果也比较可观。所以打算在CINO和LaBSE上针对粤语再进行训练，以提高粤语的意图识别能力。\n\n\n### 参考文献\n[1] https://github.com/ymcui/Chinese-Minority-PLM\n[2] https://dumps.wikimedia.org/zh_yuewiki/\n[3] https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\n[4] https://hkcc.eduhk.hk/v1/introduction.html\n[5] https://radimrehurek.com/gensim/apiref.html#api-reference","source":"_posts/pretrain-model-yue-01.md","raw":"---\ntitle: 粤语模型训练系列之数据收集\ndate: 2022-03-10 11:13:48\ntags:\n- 粤语\n- 数据收集与清洗\ncategories: \n- 跨语言模型\n---\n\n最近在做粤语模型的训练，训练的第一步，当然就是要收集数据啦！这一篇主要分享粤语语料的收集和数据处理。\n<!--more-->\n\n\n### 粤语语料收集\n提到多语言语料收集，Wikipedia的数据绝对是第一个想到，Wikipedia有一个专门备份的网站[2]，提高各个语种信息的备份，所有的网页内容都可以在此下载。我下载的是20220501的数据，粤语wiki的大小是88M。我在huggingface的数据集中搜到了中文和粤语的翻译平行语料：x-tech/cantonese-mandarin-translation[3]。网上有关粤语的语料有一个叫《香港二十世纪中期粤语语料库》[4]，该语料库需要注册登录，是一个检索式的语料库且不可下载。除此之外，可以直接拿到的数据少之又少0.0。所以就需要用到爬虫工具爬取一些粤语的微博、博客、电影对话等。记录一下粤语的网站：\n1. http://www.hkcna.hk/index.jsp?channel=2803\n2. https://weibo.com/u/6227634537?refer_flag=1005055013_\n3. https://subanana.com/ 是一个AI自动生成粤语字幕的网站\n4. https://commonvoice.mozilla.org/zh-CN/datasets\n\n\n### 网络爬虫介绍\n网上搜索了一堆的爬虫应用和脚本，真正免费而又好用的没几个。正好淘一淘，也学习一下爬虫。找到了几个看起来还不错的爬虫服务，但是用起来体验一般，不过还是可以记录一下，毕竟做的还不错，也是一种商业化渠道。\n1. https://app.diffbot.com/\n2. https://commoncrawl.org/the-data/get-started/\n3. https://webz.io/\n4. https://www.parsehub.com/\n5. https://www.scrapingbee.com/\n\n\n### 语料数据清洗\n前面提到的wiki数据还是带有网络格式的数据，并不能直接用于模型训练，而且数据质量并没有经过筛选。下面介绍一个好用的工具GENSIM[5]，用来直接抽取wiki中的文本数据。下面的代码可以抽取出wiki的内容，但是它会去掉所有的标点符号用空格代替。\n\n```python\nfrom gensim.corpora import WikiCorpus\n\nif __name__ == '__main__':\n    output = open('wiki.txt', 'w', encoding='utf8')\n    wiki = WikiCorpus(\"wiki-xxx.xml.bz2\", dictionary={})\n    for text in wiki.get_texts():\n        output.write(\" \".join(text) + \"\\n\")\n    output.close()\t\t \n```\n\n后续参考苏神的代码，重新对wiki数据进行了整理，清洗代码在代码仓库：https://github.com/Quelisa/data_cleaner.git\n\n### 总结\n\n因为粤语的收集难度确实大于中文的收集难度，并且我们已经知道这两个模型都支持中文并且都已经在大规模的中文数据集上进行了训练，所以我打算利用一些现成的中文数据集和收集到的粤语语料分别进行单语种的训练，然后主要通过收集到的平行语料进行中文和粤语的对齐训练，因为可用的平行语料不是很多所以可以通过翻译的方式获得。语料收集是算法训练中看似枯燥，却又极其重要的一步，语料的质量直接关系到模型效果的好坏，可谓是失之毫厘谬以千里。收集并设计好的语料库，高效的开发工具和科学的处理方法同样重要！第一次真正意义上感受到了big data和数据质量的重要性！\n\n\n目前专门针对粤语的预训练语言模型只有哈工大讯飞联合实验室发布的CINO (Chinese minority PLM)[1]，理论上具有和粤语语系相近的多语言模型也具有一定的粤语识别能力。在LaBSE的论文中看到,虽然没有用到粤语训练，但是粤语的识别效果也比较可观。所以打算在CINO和LaBSE上针对粤语再进行训练，以提高粤语的意图识别能力。\n\n\n### 参考文献\n[1] https://github.com/ymcui/Chinese-Minority-PLM\n[2] https://dumps.wikimedia.org/zh_yuewiki/\n[3] https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\n[4] https://hkcc.eduhk.hk/v1/introduction.html\n[5] https://radimrehurek.com/gensim/apiref.html#api-reference","slug":"pretrain-model-yue-01","published":1,"updated":"2022-06-08T07:00:47.496Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6s000cq4uxh0vy08dp","content":"<p>最近在做粤语模型的训练，训练的第一步，当然就是要收集数据啦！这一篇主要分享粤语语料的收集和数据处理。<br><span id=\"more\"></span></p>\n<h3 id=\"粤语语料收集\"><a href=\"#粤语语料收集\" class=\"headerlink\" title=\"粤语语料收集\"></a>粤语语料收集</h3><p>提到多语言语料收集，Wikipedia的数据绝对是第一个想到，Wikipedia有一个专门备份的网站[2]，提高各个语种信息的备份，所有的网页内容都可以在此下载。我下载的是20220501的数据，粤语wiki的大小是88M。我在huggingface的数据集中搜到了中文和粤语的翻译平行语料：x-tech/cantonese-mandarin-translation[3]。网上有关粤语的语料有一个叫《香港二十世纪中期粤语语料库》[4]，该语料库需要注册登录，是一个检索式的语料库且不可下载。除此之外，可以直接拿到的数据少之又少0.0。所以就需要用到爬虫工具爬取一些粤语的微博、博客、电影对话等。记录一下粤语的网站：</p>\n<ol>\n<li><a href=\"http://www.hkcna.hk/index.jsp?channel=2803\">http://www.hkcna.hk/index.jsp?channel=2803</a></li>\n<li><a href=\"https://weibo.com/u/6227634537?refer_flag=1005055013_\">https://weibo.com/u/6227634537?refer_flag=1005055013_</a></li>\n<li><a href=\"https://subanana.com/\">https://subanana.com/</a> 是一个AI自动生成粤语字幕的网站</li>\n<li><a href=\"https://commonvoice.mozilla.org/zh-CN/datasets\">https://commonvoice.mozilla.org/zh-CN/datasets</a></li>\n</ol>\n<h3 id=\"网络爬虫介绍\"><a href=\"#网络爬虫介绍\" class=\"headerlink\" title=\"网络爬虫介绍\"></a>网络爬虫介绍</h3><p>网上搜索了一堆的爬虫应用和脚本，真正免费而又好用的没几个。正好淘一淘，也学习一下爬虫。找到了几个看起来还不错的爬虫服务，但是用起来体验一般，不过还是可以记录一下，毕竟做的还不错，也是一种商业化渠道。</p>\n<ol>\n<li><a href=\"https://app.diffbot.com/\">https://app.diffbot.com/</a></li>\n<li><a href=\"https://commoncrawl.org/the-data/get-started/\">https://commoncrawl.org/the-data/get-started/</a></li>\n<li><a href=\"https://webz.io/\">https://webz.io/</a></li>\n<li><a href=\"https://www.parsehub.com/\">https://www.parsehub.com/</a></li>\n<li><a href=\"https://www.scrapingbee.com/\">https://www.scrapingbee.com/</a></li>\n</ol>\n<h3 id=\"语料数据清洗\"><a href=\"#语料数据清洗\" class=\"headerlink\" title=\"语料数据清洗\"></a>语料数据清洗</h3><p>前面提到的wiki数据还是带有网络格式的数据，并不能直接用于模型训练，而且数据质量并没有经过筛选。下面介绍一个好用的工具GENSIM[5]，用来直接抽取wiki中的文本数据。下面的代码可以抽取出wiki的内容，但是它会去掉所有的标点符号用空格代替。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> gensim.corpora <span class=\"keyword\">import</span> WikiCorpus</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    output = <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;wiki.txt&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, encoding=<span class=\"string\">&#x27;utf8&#x27;</span>)</span><br><span class=\"line\">    wiki = WikiCorpus(<span class=\"string\">&quot;wiki-xxx.xml.bz2&quot;</span>, dictionary=&#123;&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> wiki.get_texts():</span><br><span class=\"line\">        output.write(<span class=\"string\">&quot; &quot;</span>.join(text) + <span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">    output.close()\t\t </span><br></pre></td></tr></table></figure>\n<p>后续参考苏神的代码，重新对wiki数据进行了整理，清洗代码在代码仓库：<a href=\"https://github.com/Quelisa/data_cleaner.git\">https://github.com/Quelisa/data_cleaner.git</a></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>因为粤语的收集难度确实大于中文的收集难度，并且我们已经知道这两个模型都支持中文并且都已经在大规模的中文数据集上进行了训练，所以我打算利用一些现成的中文数据集和收集到的粤语语料分别进行单语种的训练，然后主要通过收集到的平行语料进行中文和粤语的对齐训练，因为可用的平行语料不是很多所以可以通过翻译的方式获得。语料收集是算法训练中看似枯燥，却又极其重要的一步，语料的质量直接关系到模型效果的好坏，可谓是失之毫厘谬以千里。收集并设计好的语料库，高效的开发工具和科学的处理方法同样重要！第一次真正意义上感受到了big data和数据质量的重要性！</p>\n<p>目前专门针对粤语的预训练语言模型只有哈工大讯飞联合实验室发布的CINO (Chinese minority PLM)[1]，理论上具有和粤语语系相近的多语言模型也具有一定的粤语识别能力。在LaBSE的论文中看到,虽然没有用到粤语训练，但是粤语的识别效果也比较可观。所以打算在CINO和LaBSE上针对粤语再进行训练，以提高粤语的意图识别能力。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] <a href=\"https://github.com/ymcui/Chinese-Minority-PLM\">https://github.com/ymcui/Chinese-Minority-PLM</a><br>[2] <a href=\"https://dumps.wikimedia.org/zh_yuewiki/\">https://dumps.wikimedia.org/zh_yuewiki/</a><br>[3] <a href=\"https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\">https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations</a><br>[4] <a href=\"https://hkcc.eduhk.hk/v1/introduction.html\">https://hkcc.eduhk.hk/v1/introduction.html</a><br>[5] <a href=\"https://radimrehurek.com/gensim/apiref.html#api-reference\">https://radimrehurek.com/gensim/apiref.html#api-reference</a></p>\n","site":{"data":{}},"excerpt":"<p>最近在做粤语模型的训练，训练的第一步，当然就是要收集数据啦！这一篇主要分享粤语语料的收集和数据处理。<br>","more":"</p>\n<h3 id=\"粤语语料收集\"><a href=\"#粤语语料收集\" class=\"headerlink\" title=\"粤语语料收集\"></a>粤语语料收集</h3><p>提到多语言语料收集，Wikipedia的数据绝对是第一个想到，Wikipedia有一个专门备份的网站[2]，提高各个语种信息的备份，所有的网页内容都可以在此下载。我下载的是20220501的数据，粤语wiki的大小是88M。我在huggingface的数据集中搜到了中文和粤语的翻译平行语料：x-tech/cantonese-mandarin-translation[3]。网上有关粤语的语料有一个叫《香港二十世纪中期粤语语料库》[4]，该语料库需要注册登录，是一个检索式的语料库且不可下载。除此之外，可以直接拿到的数据少之又少0.0。所以就需要用到爬虫工具爬取一些粤语的微博、博客、电影对话等。记录一下粤语的网站：</p>\n<ol>\n<li><a href=\"http://www.hkcna.hk/index.jsp?channel=2803\">http://www.hkcna.hk/index.jsp?channel=2803</a></li>\n<li><a href=\"https://weibo.com/u/6227634537?refer_flag=1005055013_\">https://weibo.com/u/6227634537?refer_flag=1005055013_</a></li>\n<li><a href=\"https://subanana.com/\">https://subanana.com/</a> 是一个AI自动生成粤语字幕的网站</li>\n<li><a href=\"https://commonvoice.mozilla.org/zh-CN/datasets\">https://commonvoice.mozilla.org/zh-CN/datasets</a></li>\n</ol>\n<h3 id=\"网络爬虫介绍\"><a href=\"#网络爬虫介绍\" class=\"headerlink\" title=\"网络爬虫介绍\"></a>网络爬虫介绍</h3><p>网上搜索了一堆的爬虫应用和脚本，真正免费而又好用的没几个。正好淘一淘，也学习一下爬虫。找到了几个看起来还不错的爬虫服务，但是用起来体验一般，不过还是可以记录一下，毕竟做的还不错，也是一种商业化渠道。</p>\n<ol>\n<li><a href=\"https://app.diffbot.com/\">https://app.diffbot.com/</a></li>\n<li><a href=\"https://commoncrawl.org/the-data/get-started/\">https://commoncrawl.org/the-data/get-started/</a></li>\n<li><a href=\"https://webz.io/\">https://webz.io/</a></li>\n<li><a href=\"https://www.parsehub.com/\">https://www.parsehub.com/</a></li>\n<li><a href=\"https://www.scrapingbee.com/\">https://www.scrapingbee.com/</a></li>\n</ol>\n<h3 id=\"语料数据清洗\"><a href=\"#语料数据清洗\" class=\"headerlink\" title=\"语料数据清洗\"></a>语料数据清洗</h3><p>前面提到的wiki数据还是带有网络格式的数据，并不能直接用于模型训练，而且数据质量并没有经过筛选。下面介绍一个好用的工具GENSIM[5]，用来直接抽取wiki中的文本数据。下面的代码可以抽取出wiki的内容，但是它会去掉所有的标点符号用空格代替。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> gensim.corpora <span class=\"keyword\">import</span> WikiCorpus</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    output = <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;wiki.txt&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, encoding=<span class=\"string\">&#x27;utf8&#x27;</span>)</span><br><span class=\"line\">    wiki = WikiCorpus(<span class=\"string\">&quot;wiki-xxx.xml.bz2&quot;</span>, dictionary=&#123;&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> wiki.get_texts():</span><br><span class=\"line\">        output.write(<span class=\"string\">&quot; &quot;</span>.join(text) + <span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">    output.close()\t\t </span><br></pre></td></tr></table></figure>\n<p>后续参考苏神的代码，重新对wiki数据进行了整理，清洗代码在代码仓库：<a href=\"https://github.com/Quelisa/data_cleaner.git\">https://github.com/Quelisa/data_cleaner.git</a></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>因为粤语的收集难度确实大于中文的收集难度，并且我们已经知道这两个模型都支持中文并且都已经在大规模的中文数据集上进行了训练，所以我打算利用一些现成的中文数据集和收集到的粤语语料分别进行单语种的训练，然后主要通过收集到的平行语料进行中文和粤语的对齐训练，因为可用的平行语料不是很多所以可以通过翻译的方式获得。语料收集是算法训练中看似枯燥，却又极其重要的一步，语料的质量直接关系到模型效果的好坏，可谓是失之毫厘谬以千里。收集并设计好的语料库，高效的开发工具和科学的处理方法同样重要！第一次真正意义上感受到了big data和数据质量的重要性！</p>\n<p>目前专门针对粤语的预训练语言模型只有哈工大讯飞联合实验室发布的CINO (Chinese minority PLM)[1]，理论上具有和粤语语系相近的多语言模型也具有一定的粤语识别能力。在LaBSE的论文中看到,虽然没有用到粤语训练，但是粤语的识别效果也比较可观。所以打算在CINO和LaBSE上针对粤语再进行训练，以提高粤语的意图识别能力。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] <a href=\"https://github.com/ymcui/Chinese-Minority-PLM\">https://github.com/ymcui/Chinese-Minority-PLM</a><br>[2] <a href=\"https://dumps.wikimedia.org/zh_yuewiki/\">https://dumps.wikimedia.org/zh_yuewiki/</a><br>[3] <a href=\"https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations\">https://huggingface.co/datasets/x-tech/cantonese-mandarin-translations</a><br>[4] <a href=\"https://hkcc.eduhk.hk/v1/introduction.html\">https://hkcc.eduhk.hk/v1/introduction.html</a><br>[5] <a href=\"https://radimrehurek.com/gensim/apiref.html#api-reference\">https://radimrehurek.com/gensim/apiref.html#api-reference</a></p>"},{"title":"还有人不了解这些预训练模型？","date":"2021-11-15T08:40:46.000Z","mathjax":true,"_content":"\n自Google在2017年推出Transformer后，自然语言处理全面进入预训练大模型的时代，随之而来的BERT、GPT等大的预训练模型如雨后春笋般出现。带动了整个自然语言处理任务快速发展的同时，还影响了机器视觉和多模态技术的发展。本篇将介绍几个经典主流的预训练语言模型~\n\n\n<!--more-->\n\n\n### ELMo\n是一种动态词向量预训练模型，它使用双向的网络结构进行建模，使用语言模型，因此是双向的。他是用C卷积神经网络对词进行embedding，主体模型使用LSTM。它在多个下游任务上表现达到了SOTA，启发了后来的预训练语言模型。\n\n\n### BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\n1. BETR采用了Transformer中encode部分。在输入中加入位置信息，使用了掩码语言模型（MLM）和预测下一个句子（NSP）进行预训练。因为MLM本身的建模就是双向的，所以不需要特殊的网络结构就可以实现双向的语义理解。\n2. RoBERTa是对BERT中的方法进行更加充分的发掘，增加了训练预料，使用基于字节的token，增加batch和step，并去掉了NSP预训练任务，同时对MLM任务进行优化，针对同一个本文有不同种的掩码方案。\n3. ALBERT是一个轻量级的BETR，主要是对词向量的维度进行压缩，所以模型的参数量会减少。之所以对词向量进行压缩是基于这样的假设，BERT中的词向量和输出向量的维度是一致的，但是在词级别时其实所包含的信息相对较少而且向量相对稀疏，而在上层的网络中会学习到更加复杂的语义表示，所以需要更大维度的向量表示。因为之前的试验认为NSP预训练任务过于简单，所以学习不到太多的语义，所以使用句子顺序预测任务，预测两个句子对的顺序。\n4. ELECTRA使用了生成对抗的网络结构，训练时使用两个BERT，一个做生成器，一个做判别器，使用替换词检测任务（RTD）进行预训练，预测时只是用判别器，所以生成器的模型结构可以适度减少，大概在判别器的四分之一到二分之一之间。同时由于判别器的输入没有[MASK]字符，所以与下游任务保持一致。\n5. XLNet使用了Transformer-XL作为主框架，相对传统的transformer拥有较好的性能，它使用了自回归语言模型结构，避免了[MASK]标记带来的上下游任务不一致，通过使用排列语言模型和双流自注意力实现了自编码语言模型中的双向上下文。\n6. MacBERT中应用了一种基于文本纠错的掩码语言模型，使用整词掩码和同义词替换，并采用了句子顺序预测任务进行预训练。它很好地解决了BERT使用[MASK]掩码带来的“预训练-精调”不一致问题。\n\n\n\n### DistilBERT、TinyBERT\n\n这类模型主要是对预训练模型进行蒸馏。知识蒸馏是一种常用的知识迁移方法，通常由教师模型和学生模型组成，在用于压缩模型方面一般可以通过设置一个更小的学生模型对原始模型进行压缩。\n\n\n1. DistilBERT是基于Triple Loss的知识蒸馏方法，教师模型使用BERT-base，学生模型是六层的BERT，使用BERT的前六层进行初始化，预训练使用MLM，损失函数是MLM损失、蒸馏损失和词向量余弦损失之和。\n\n2. TinyBERT主要使用了额外的词向量层蒸馏和中间层蒸馏来进一步提升蒸馏的效果，并使用了两段式蒸馏进一步提升下游任务的效果。\n\n\n\n### Transformer-XL、Reformer、Longformer、BigBird\nTransformer因为采用自注意力机制使其空间和时间复杂度为$O(n^2)$，因此处理长文本的效率很低。虽然有一些处理长文本的trick，但是并不能解决问题本身。因此提出一些模型来解决上述问题：\n\n\n1. Transformer-XL提出了两种改进策略：状态复用的块级别循环和相对位置编码。\n2. Reformer引入了局部敏感哈希注意力和可逆Transformer技术，它可以帮助减少模型的内存占用，进而提高对长文本的处理能力。局部敏感哈希注意力的提出基于对Transformer中注意力机制观察的优化，首先通过实验可以看到单独计算查询和键与共享$QK$对整体性能没有太大的差异，同时全局注意力也没有必要，实际上注意力机制中更关心的是经过softmax函数激活的值，而激活的结果主要取决于数值较大的元素，因此在计算中只需要关注与当前查询关联度高的n个词，它可以极大地降低计算量。可以Transformer则是受到可以残差网络的启发，其主要思想是任意一层的激活值可以通过后续激活层进行还原。\n3. Longformer将输入文本序列的最大长度扩充到4096，提出了三种稀疏注意力模式来降低计算复杂度，分别是滑动窗口注意力，扩张滑动窗口注意力和全局注意力。\n4. BigBird使用了随机注意力、滑动窗口注意力和全局注意力三种稀疏注意力的方式对Transformer的长文本处理能力进一步进行优化，并通过理论分析证明了稀疏注意力的有效性。\n\n\n\n### BART、UniLM、T5、GPT-3\n\n前面的模型主要关注自然语言表示，而下面的模型更多的集中用于文本生成：\n\n1. BART使用了标准的seq2seq tranformer结构，BART的预训练是在于破坏原文档然后优化重构loss。BART采用了多种方式破坏原文档，即采用了多种Noise。对于序列分类（文本分类）任务，encoder和decoder部分都用相同的输入，将deocoder最后一个节点用于多类别线性分类器中。对于序列标注任务，同样是在decoder和encoder采用相同的文本输入，以decoder的隐藏节点输出用于预测每个节点的类别。由于BART的模型框架本身就采用了自回归方式，因而在finetune序列生成任务时，可直接在encoder部分输入原始文本，decoder部分用于预测待生成的文本。BART预训练模型同样也可用于将其它语言翻译为英文。\n2. UniLM通过引入attention mask实现了单向、双向和seq2seq语言模型，在双向LM模型中，没有任何mask，因为上下文信息都会被融入token中。在从左向右的单向LM模型中，矩阵上半三角会被mask掉，因为token只考虑左边的文本信息。在seq-to-seq模型中，句子s1的矩阵部分没有mask，s1因为s1融合上下文信息，句子s2矩阵的右上三角有mask，因为s2融合了s1的信息和左边的文本信息。使用完形填空和下一句预测作为预训练任务。\n3. T5从另一个角度实现了NLU和NLG的统一建模，靠着大力出奇迹，采用了Prompt的训练方式，将所有NLP任务都转化成Text-to-Text（文本到文本）任务。\n4. GPT-3是一种自回归模型，使用仅有解码器的体系结构。使用下一个单词预测目标进行训练。\n\n\n\n\n### Vision Transformer\nVision Transformer可以是看做是CV领域对NLP领域的抄作业，直接将Transformer的结构拿来，使用embedding+encode+MLP head的网络结构，主要区别在于embedding层的不同。对于标准的Transformer模块，要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]，对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。\n\n\n\n### UNITER、CLIP、UNIMO\n而在Transformer之前，多模态领域并未出现行之有效的预训练模型，图像和文本都各玩各的，毕竟图像的预训练是从imagenet带标签的数据上进行的，而NLP更加牛，它能自回归，能自己mask自己然后让模型去预测。与CV一样多模态预训练模型也是对NLP的抄作业。\n1. UNITER采用Transformer的encoder结构，主要是在embedding层有所不同，UNITER有两个Embedder，Image Embedder通过对Faster-RCNN的输出ROI feature以及其位置特征（7维，normalized top/left/bottom/right coordinates, width, height, and area.）进行融合建模，需要两个FC和一个LN操作完成。Text Embedder则参考BERT的输入，但是没有segment，然后直接接入Transformer进行双向建模，融合两种模态，从而达成目的，简单明了，不同于双流预训练模型，这两类模态共享同一个Encoder。预训练任务采用MLM，MRM(Mask Region Model, 预测图像，回归或分类，有三种变体)，ITM（Image Text Match, 图文是否一致）和WRA（Word Region Alignment, 字和图像的对齐任务）。\n2. CLIP用到了零样本学习（zero-shot learning）、自然语言理解和多模态学习等技术，来完成图像的理解。\n3. UNIMO采用类似的掩码预测自监督方法学习图像和文本的表示。同时，为了将文本和图像的表示映射到统一的语义空间，论文提出跨模态对比学习，基于图文对数据实现图像与文本的统一表示学习。\n\n","source":"_posts/pretrained-model.md","raw":"---\ntitle: 还有人不了解这些预训练模型？\ndate: 2021-11-15 16:40:46\ntags:\n- PM\ncategories: \n- 预训练语言模型\nmathjax: true\n---\n\n自Google在2017年推出Transformer后，自然语言处理全面进入预训练大模型的时代，随之而来的BERT、GPT等大的预训练模型如雨后春笋般出现。带动了整个自然语言处理任务快速发展的同时，还影响了机器视觉和多模态技术的发展。本篇将介绍几个经典主流的预训练语言模型~\n\n\n<!--more-->\n\n\n### ELMo\n是一种动态词向量预训练模型，它使用双向的网络结构进行建模，使用语言模型，因此是双向的。他是用C卷积神经网络对词进行embedding，主体模型使用LSTM。它在多个下游任务上表现达到了SOTA，启发了后来的预训练语言模型。\n\n\n### BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\n1. BETR采用了Transformer中encode部分。在输入中加入位置信息，使用了掩码语言模型（MLM）和预测下一个句子（NSP）进行预训练。因为MLM本身的建模就是双向的，所以不需要特殊的网络结构就可以实现双向的语义理解。\n2. RoBERTa是对BERT中的方法进行更加充分的发掘，增加了训练预料，使用基于字节的token，增加batch和step，并去掉了NSP预训练任务，同时对MLM任务进行优化，针对同一个本文有不同种的掩码方案。\n3. ALBERT是一个轻量级的BETR，主要是对词向量的维度进行压缩，所以模型的参数量会减少。之所以对词向量进行压缩是基于这样的假设，BERT中的词向量和输出向量的维度是一致的，但是在词级别时其实所包含的信息相对较少而且向量相对稀疏，而在上层的网络中会学习到更加复杂的语义表示，所以需要更大维度的向量表示。因为之前的试验认为NSP预训练任务过于简单，所以学习不到太多的语义，所以使用句子顺序预测任务，预测两个句子对的顺序。\n4. ELECTRA使用了生成对抗的网络结构，训练时使用两个BERT，一个做生成器，一个做判别器，使用替换词检测任务（RTD）进行预训练，预测时只是用判别器，所以生成器的模型结构可以适度减少，大概在判别器的四分之一到二分之一之间。同时由于判别器的输入没有[MASK]字符，所以与下游任务保持一致。\n5. XLNet使用了Transformer-XL作为主框架，相对传统的transformer拥有较好的性能，它使用了自回归语言模型结构，避免了[MASK]标记带来的上下游任务不一致，通过使用排列语言模型和双流自注意力实现了自编码语言模型中的双向上下文。\n6. MacBERT中应用了一种基于文本纠错的掩码语言模型，使用整词掩码和同义词替换，并采用了句子顺序预测任务进行预训练。它很好地解决了BERT使用[MASK]掩码带来的“预训练-精调”不一致问题。\n\n\n\n### DistilBERT、TinyBERT\n\n这类模型主要是对预训练模型进行蒸馏。知识蒸馏是一种常用的知识迁移方法，通常由教师模型和学生模型组成，在用于压缩模型方面一般可以通过设置一个更小的学生模型对原始模型进行压缩。\n\n\n1. DistilBERT是基于Triple Loss的知识蒸馏方法，教师模型使用BERT-base，学生模型是六层的BERT，使用BERT的前六层进行初始化，预训练使用MLM，损失函数是MLM损失、蒸馏损失和词向量余弦损失之和。\n\n2. TinyBERT主要使用了额外的词向量层蒸馏和中间层蒸馏来进一步提升蒸馏的效果，并使用了两段式蒸馏进一步提升下游任务的效果。\n\n\n\n### Transformer-XL、Reformer、Longformer、BigBird\nTransformer因为采用自注意力机制使其空间和时间复杂度为$O(n^2)$，因此处理长文本的效率很低。虽然有一些处理长文本的trick，但是并不能解决问题本身。因此提出一些模型来解决上述问题：\n\n\n1. Transformer-XL提出了两种改进策略：状态复用的块级别循环和相对位置编码。\n2. Reformer引入了局部敏感哈希注意力和可逆Transformer技术，它可以帮助减少模型的内存占用，进而提高对长文本的处理能力。局部敏感哈希注意力的提出基于对Transformer中注意力机制观察的优化，首先通过实验可以看到单独计算查询和键与共享$QK$对整体性能没有太大的差异，同时全局注意力也没有必要，实际上注意力机制中更关心的是经过softmax函数激活的值，而激活的结果主要取决于数值较大的元素，因此在计算中只需要关注与当前查询关联度高的n个词，它可以极大地降低计算量。可以Transformer则是受到可以残差网络的启发，其主要思想是任意一层的激活值可以通过后续激活层进行还原。\n3. Longformer将输入文本序列的最大长度扩充到4096，提出了三种稀疏注意力模式来降低计算复杂度，分别是滑动窗口注意力，扩张滑动窗口注意力和全局注意力。\n4. BigBird使用了随机注意力、滑动窗口注意力和全局注意力三种稀疏注意力的方式对Transformer的长文本处理能力进一步进行优化，并通过理论分析证明了稀疏注意力的有效性。\n\n\n\n### BART、UniLM、T5、GPT-3\n\n前面的模型主要关注自然语言表示，而下面的模型更多的集中用于文本生成：\n\n1. BART使用了标准的seq2seq tranformer结构，BART的预训练是在于破坏原文档然后优化重构loss。BART采用了多种方式破坏原文档，即采用了多种Noise。对于序列分类（文本分类）任务，encoder和decoder部分都用相同的输入，将deocoder最后一个节点用于多类别线性分类器中。对于序列标注任务，同样是在decoder和encoder采用相同的文本输入，以decoder的隐藏节点输出用于预测每个节点的类别。由于BART的模型框架本身就采用了自回归方式，因而在finetune序列生成任务时，可直接在encoder部分输入原始文本，decoder部分用于预测待生成的文本。BART预训练模型同样也可用于将其它语言翻译为英文。\n2. UniLM通过引入attention mask实现了单向、双向和seq2seq语言模型，在双向LM模型中，没有任何mask，因为上下文信息都会被融入token中。在从左向右的单向LM模型中，矩阵上半三角会被mask掉，因为token只考虑左边的文本信息。在seq-to-seq模型中，句子s1的矩阵部分没有mask，s1因为s1融合上下文信息，句子s2矩阵的右上三角有mask，因为s2融合了s1的信息和左边的文本信息。使用完形填空和下一句预测作为预训练任务。\n3. T5从另一个角度实现了NLU和NLG的统一建模，靠着大力出奇迹，采用了Prompt的训练方式，将所有NLP任务都转化成Text-to-Text（文本到文本）任务。\n4. GPT-3是一种自回归模型，使用仅有解码器的体系结构。使用下一个单词预测目标进行训练。\n\n\n\n\n### Vision Transformer\nVision Transformer可以是看做是CV领域对NLP领域的抄作业，直接将Transformer的结构拿来，使用embedding+encode+MLP head的网络结构，主要区别在于embedding层的不同。对于标准的Transformer模块，要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]，对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。\n\n\n\n### UNITER、CLIP、UNIMO\n而在Transformer之前，多模态领域并未出现行之有效的预训练模型，图像和文本都各玩各的，毕竟图像的预训练是从imagenet带标签的数据上进行的，而NLP更加牛，它能自回归，能自己mask自己然后让模型去预测。与CV一样多模态预训练模型也是对NLP的抄作业。\n1. UNITER采用Transformer的encoder结构，主要是在embedding层有所不同，UNITER有两个Embedder，Image Embedder通过对Faster-RCNN的输出ROI feature以及其位置特征（7维，normalized top/left/bottom/right coordinates, width, height, and area.）进行融合建模，需要两个FC和一个LN操作完成。Text Embedder则参考BERT的输入，但是没有segment，然后直接接入Transformer进行双向建模，融合两种模态，从而达成目的，简单明了，不同于双流预训练模型，这两类模态共享同一个Encoder。预训练任务采用MLM，MRM(Mask Region Model, 预测图像，回归或分类，有三种变体)，ITM（Image Text Match, 图文是否一致）和WRA（Word Region Alignment, 字和图像的对齐任务）。\n2. CLIP用到了零样本学习（zero-shot learning）、自然语言理解和多模态学习等技术，来完成图像的理解。\n3. UNIMO采用类似的掩码预测自监督方法学习图像和文本的表示。同时，为了将文本和图像的表示映射到统一的语义空间，论文提出跨模态对比学习，基于图文对数据实现图像与文本的统一表示学习。\n\n","slug":"pretrained-model","published":1,"updated":"2022-06-13T12:58:35.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6t000dq4ux7tb487v4","content":"<p>自Google在2017年推出Transformer后，自然语言处理全面进入预训练大模型的时代，随之而来的BERT、GPT等大的预训练模型如雨后春笋般出现。带动了整个自然语言处理任务快速发展的同时，还影响了机器视觉和多模态技术的发展。本篇将介绍几个经典主流的预训练语言模型~</p>\n<span id=\"more\"></span>\n<h3 id=\"ELMo\"><a href=\"#ELMo\" class=\"headerlink\" title=\"ELMo\"></a>ELMo</h3><p>是一种动态词向量预训练模型，它使用双向的网络结构进行建模，使用语言模型，因此是双向的。他是用C卷积神经网络对词进行embedding，主体模型使用LSTM。它在多个下游任务上表现达到了SOTA，启发了后来的预训练语言模型。</p>\n<h3 id=\"BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\"><a href=\"#BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\" class=\"headerlink\" title=\"BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\"></a>BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT</h3><ol>\n<li>BETR采用了Transformer中encode部分。在输入中加入位置信息，使用了掩码语言模型（MLM）和预测下一个句子（NSP）进行预训练。因为MLM本身的建模就是双向的，所以不需要特殊的网络结构就可以实现双向的语义理解。</li>\n<li>RoBERTa是对BERT中的方法进行更加充分的发掘，增加了训练预料，使用基于字节的token，增加batch和step，并去掉了NSP预训练任务，同时对MLM任务进行优化，针对同一个本文有不同种的掩码方案。</li>\n<li>ALBERT是一个轻量级的BETR，主要是对词向量的维度进行压缩，所以模型的参数量会减少。之所以对词向量进行压缩是基于这样的假设，BERT中的词向量和输出向量的维度是一致的，但是在词级别时其实所包含的信息相对较少而且向量相对稀疏，而在上层的网络中会学习到更加复杂的语义表示，所以需要更大维度的向量表示。因为之前的试验认为NSP预训练任务过于简单，所以学习不到太多的语义，所以使用句子顺序预测任务，预测两个句子对的顺序。</li>\n<li>ELECTRA使用了生成对抗的网络结构，训练时使用两个BERT，一个做生成器，一个做判别器，使用替换词检测任务（RTD）进行预训练，预测时只是用判别器，所以生成器的模型结构可以适度减少，大概在判别器的四分之一到二分之一之间。同时由于判别器的输入没有[MASK]字符，所以与下游任务保持一致。</li>\n<li>XLNet使用了Transformer-XL作为主框架，相对传统的transformer拥有较好的性能，它使用了自回归语言模型结构，避免了[MASK]标记带来的上下游任务不一致，通过使用排列语言模型和双流自注意力实现了自编码语言模型中的双向上下文。</li>\n<li>MacBERT中应用了一种基于文本纠错的掩码语言模型，使用整词掩码和同义词替换，并采用了句子顺序预测任务进行预训练。它很好地解决了BERT使用[MASK]掩码带来的“预训练-精调”不一致问题。</li>\n</ol>\n<h3 id=\"DistilBERT、TinyBERT\"><a href=\"#DistilBERT、TinyBERT\" class=\"headerlink\" title=\"DistilBERT、TinyBERT\"></a>DistilBERT、TinyBERT</h3><p>这类模型主要是对预训练模型进行蒸馏。知识蒸馏是一种常用的知识迁移方法，通常由教师模型和学生模型组成，在用于压缩模型方面一般可以通过设置一个更小的学生模型对原始模型进行压缩。</p>\n<ol>\n<li><p>DistilBERT是基于Triple Loss的知识蒸馏方法，教师模型使用BERT-base，学生模型是六层的BERT，使用BERT的前六层进行初始化，预训练使用MLM，损失函数是MLM损失、蒸馏损失和词向量余弦损失之和。</p>\n</li>\n<li><p>TinyBERT主要使用了额外的词向量层蒸馏和中间层蒸馏来进一步提升蒸馏的效果，并使用了两段式蒸馏进一步提升下游任务的效果。</p>\n</li>\n</ol>\n<h3 id=\"Transformer-XL、Reformer、Longformer、BigBird\"><a href=\"#Transformer-XL、Reformer、Longformer、BigBird\" class=\"headerlink\" title=\"Transformer-XL、Reformer、Longformer、BigBird\"></a>Transformer-XL、Reformer、Longformer、BigBird</h3><p>Transformer因为采用自注意力机制使其空间和时间复杂度为$O(n^2)$，因此处理长文本的效率很低。虽然有一些处理长文本的trick，但是并不能解决问题本身。因此提出一些模型来解决上述问题：</p>\n<ol>\n<li>Transformer-XL提出了两种改进策略：状态复用的块级别循环和相对位置编码。</li>\n<li>Reformer引入了局部敏感哈希注意力和可逆Transformer技术，它可以帮助减少模型的内存占用，进而提高对长文本的处理能力。局部敏感哈希注意力的提出基于对Transformer中注意力机制观察的优化，首先通过实验可以看到单独计算查询和键与共享$QK$对整体性能没有太大的差异，同时全局注意力也没有必要，实际上注意力机制中更关心的是经过softmax函数激活的值，而激活的结果主要取决于数值较大的元素，因此在计算中只需要关注与当前查询关联度高的n个词，它可以极大地降低计算量。可以Transformer则是受到可以残差网络的启发，其主要思想是任意一层的激活值可以通过后续激活层进行还原。</li>\n<li>Longformer将输入文本序列的最大长度扩充到4096，提出了三种稀疏注意力模式来降低计算复杂度，分别是滑动窗口注意力，扩张滑动窗口注意力和全局注意力。</li>\n<li>BigBird使用了随机注意力、滑动窗口注意力和全局注意力三种稀疏注意力的方式对Transformer的长文本处理能力进一步进行优化，并通过理论分析证明了稀疏注意力的有效性。</li>\n</ol>\n<h3 id=\"BART、UniLM、T5、GPT-3\"><a href=\"#BART、UniLM、T5、GPT-3\" class=\"headerlink\" title=\"BART、UniLM、T5、GPT-3\"></a>BART、UniLM、T5、GPT-3</h3><p>前面的模型主要关注自然语言表示，而下面的模型更多的集中用于文本生成：</p>\n<ol>\n<li>BART使用了标准的seq2seq tranformer结构，BART的预训练是在于破坏原文档然后优化重构loss。BART采用了多种方式破坏原文档，即采用了多种Noise。对于序列分类（文本分类）任务，encoder和decoder部分都用相同的输入，将deocoder最后一个节点用于多类别线性分类器中。对于序列标注任务，同样是在decoder和encoder采用相同的文本输入，以decoder的隐藏节点输出用于预测每个节点的类别。由于BART的模型框架本身就采用了自回归方式，因而在finetune序列生成任务时，可直接在encoder部分输入原始文本，decoder部分用于预测待生成的文本。BART预训练模型同样也可用于将其它语言翻译为英文。</li>\n<li>UniLM通过引入attention mask实现了单向、双向和seq2seq语言模型，在双向LM模型中，没有任何mask，因为上下文信息都会被融入token中。在从左向右的单向LM模型中，矩阵上半三角会被mask掉，因为token只考虑左边的文本信息。在seq-to-seq模型中，句子s1的矩阵部分没有mask，s1因为s1融合上下文信息，句子s2矩阵的右上三角有mask，因为s2融合了s1的信息和左边的文本信息。使用完形填空和下一句预测作为预训练任务。</li>\n<li>T5从另一个角度实现了NLU和NLG的统一建模，靠着大力出奇迹，采用了Prompt的训练方式，将所有NLP任务都转化成Text-to-Text（文本到文本）任务。</li>\n<li>GPT-3是一种自回归模型，使用仅有解码器的体系结构。使用下一个单词预测目标进行训练。</li>\n</ol>\n<h3 id=\"Vision-Transformer\"><a href=\"#Vision-Transformer\" class=\"headerlink\" title=\"Vision Transformer\"></a>Vision Transformer</h3><p>Vision Transformer可以是看做是CV领域对NLP领域的抄作业，直接将Transformer的结构拿来，使用embedding+encode+MLP head的网络结构，主要区别在于embedding层的不同。对于标准的Transformer模块，要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]，对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。</p>\n<h3 id=\"UNITER、CLIP、UNIMO\"><a href=\"#UNITER、CLIP、UNIMO\" class=\"headerlink\" title=\"UNITER、CLIP、UNIMO\"></a>UNITER、CLIP、UNIMO</h3><p>而在Transformer之前，多模态领域并未出现行之有效的预训练模型，图像和文本都各玩各的，毕竟图像的预训练是从imagenet带标签的数据上进行的，而NLP更加牛，它能自回归，能自己mask自己然后让模型去预测。与CV一样多模态预训练模型也是对NLP的抄作业。</p>\n<ol>\n<li>UNITER采用Transformer的encoder结构，主要是在embedding层有所不同，UNITER有两个Embedder，Image Embedder通过对Faster-RCNN的输出ROI feature以及其位置特征（7维，normalized top/left/bottom/right coordinates, width, height, and area.）进行融合建模，需要两个FC和一个LN操作完成。Text Embedder则参考BERT的输入，但是没有segment，然后直接接入Transformer进行双向建模，融合两种模态，从而达成目的，简单明了，不同于双流预训练模型，这两类模态共享同一个Encoder。预训练任务采用MLM，MRM(Mask Region Model, 预测图像，回归或分类，有三种变体)，ITM（Image Text Match, 图文是否一致）和WRA（Word Region Alignment, 字和图像的对齐任务）。</li>\n<li>CLIP用到了零样本学习（zero-shot learning）、自然语言理解和多模态学习等技术，来完成图像的理解。</li>\n<li>UNIMO采用类似的掩码预测自监督方法学习图像和文本的表示。同时，为了将文本和图像的表示映射到统一的语义空间，论文提出跨模态对比学习，基于图文对数据实现图像与文本的统一表示学习。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>自Google在2017年推出Transformer后，自然语言处理全面进入预训练大模型的时代，随之而来的BERT、GPT等大的预训练模型如雨后春笋般出现。带动了整个自然语言处理任务快速发展的同时，还影响了机器视觉和多模态技术的发展。本篇将介绍几个经典主流的预训练语言模型~</p>","more":"<h3 id=\"ELMo\"><a href=\"#ELMo\" class=\"headerlink\" title=\"ELMo\"></a>ELMo</h3><p>是一种动态词向量预训练模型，它使用双向的网络结构进行建模，使用语言模型，因此是双向的。他是用C卷积神经网络对词进行embedding，主体模型使用LSTM。它在多个下游任务上表现达到了SOTA，启发了后来的预训练语言模型。</p>\n<h3 id=\"BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\"><a href=\"#BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\" class=\"headerlink\" title=\"BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT\"></a>BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT</h3><ol>\n<li>BETR采用了Transformer中encode部分。在输入中加入位置信息，使用了掩码语言模型（MLM）和预测下一个句子（NSP）进行预训练。因为MLM本身的建模就是双向的，所以不需要特殊的网络结构就可以实现双向的语义理解。</li>\n<li>RoBERTa是对BERT中的方法进行更加充分的发掘，增加了训练预料，使用基于字节的token，增加batch和step，并去掉了NSP预训练任务，同时对MLM任务进行优化，针对同一个本文有不同种的掩码方案。</li>\n<li>ALBERT是一个轻量级的BETR，主要是对词向量的维度进行压缩，所以模型的参数量会减少。之所以对词向量进行压缩是基于这样的假设，BERT中的词向量和输出向量的维度是一致的，但是在词级别时其实所包含的信息相对较少而且向量相对稀疏，而在上层的网络中会学习到更加复杂的语义表示，所以需要更大维度的向量表示。因为之前的试验认为NSP预训练任务过于简单，所以学习不到太多的语义，所以使用句子顺序预测任务，预测两个句子对的顺序。</li>\n<li>ELECTRA使用了生成对抗的网络结构，训练时使用两个BERT，一个做生成器，一个做判别器，使用替换词检测任务（RTD）进行预训练，预测时只是用判别器，所以生成器的模型结构可以适度减少，大概在判别器的四分之一到二分之一之间。同时由于判别器的输入没有[MASK]字符，所以与下游任务保持一致。</li>\n<li>XLNet使用了Transformer-XL作为主框架，相对传统的transformer拥有较好的性能，它使用了自回归语言模型结构，避免了[MASK]标记带来的上下游任务不一致，通过使用排列语言模型和双流自注意力实现了自编码语言模型中的双向上下文。</li>\n<li>MacBERT中应用了一种基于文本纠错的掩码语言模型，使用整词掩码和同义词替换，并采用了句子顺序预测任务进行预训练。它很好地解决了BERT使用[MASK]掩码带来的“预训练-精调”不一致问题。</li>\n</ol>\n<h3 id=\"DistilBERT、TinyBERT\"><a href=\"#DistilBERT、TinyBERT\" class=\"headerlink\" title=\"DistilBERT、TinyBERT\"></a>DistilBERT、TinyBERT</h3><p>这类模型主要是对预训练模型进行蒸馏。知识蒸馏是一种常用的知识迁移方法，通常由教师模型和学生模型组成，在用于压缩模型方面一般可以通过设置一个更小的学生模型对原始模型进行压缩。</p>\n<ol>\n<li><p>DistilBERT是基于Triple Loss的知识蒸馏方法，教师模型使用BERT-base，学生模型是六层的BERT，使用BERT的前六层进行初始化，预训练使用MLM，损失函数是MLM损失、蒸馏损失和词向量余弦损失之和。</p>\n</li>\n<li><p>TinyBERT主要使用了额外的词向量层蒸馏和中间层蒸馏来进一步提升蒸馏的效果，并使用了两段式蒸馏进一步提升下游任务的效果。</p>\n</li>\n</ol>\n<h3 id=\"Transformer-XL、Reformer、Longformer、BigBird\"><a href=\"#Transformer-XL、Reformer、Longformer、BigBird\" class=\"headerlink\" title=\"Transformer-XL、Reformer、Longformer、BigBird\"></a>Transformer-XL、Reformer、Longformer、BigBird</h3><p>Transformer因为采用自注意力机制使其空间和时间复杂度为$O(n^2)$，因此处理长文本的效率很低。虽然有一些处理长文本的trick，但是并不能解决问题本身。因此提出一些模型来解决上述问题：</p>\n<ol>\n<li>Transformer-XL提出了两种改进策略：状态复用的块级别循环和相对位置编码。</li>\n<li>Reformer引入了局部敏感哈希注意力和可逆Transformer技术，它可以帮助减少模型的内存占用，进而提高对长文本的处理能力。局部敏感哈希注意力的提出基于对Transformer中注意力机制观察的优化，首先通过实验可以看到单独计算查询和键与共享$QK$对整体性能没有太大的差异，同时全局注意力也没有必要，实际上注意力机制中更关心的是经过softmax函数激活的值，而激活的结果主要取决于数值较大的元素，因此在计算中只需要关注与当前查询关联度高的n个词，它可以极大地降低计算量。可以Transformer则是受到可以残差网络的启发，其主要思想是任意一层的激活值可以通过后续激活层进行还原。</li>\n<li>Longformer将输入文本序列的最大长度扩充到4096，提出了三种稀疏注意力模式来降低计算复杂度，分别是滑动窗口注意力，扩张滑动窗口注意力和全局注意力。</li>\n<li>BigBird使用了随机注意力、滑动窗口注意力和全局注意力三种稀疏注意力的方式对Transformer的长文本处理能力进一步进行优化，并通过理论分析证明了稀疏注意力的有效性。</li>\n</ol>\n<h3 id=\"BART、UniLM、T5、GPT-3\"><a href=\"#BART、UniLM、T5、GPT-3\" class=\"headerlink\" title=\"BART、UniLM、T5、GPT-3\"></a>BART、UniLM、T5、GPT-3</h3><p>前面的模型主要关注自然语言表示，而下面的模型更多的集中用于文本生成：</p>\n<ol>\n<li>BART使用了标准的seq2seq tranformer结构，BART的预训练是在于破坏原文档然后优化重构loss。BART采用了多种方式破坏原文档，即采用了多种Noise。对于序列分类（文本分类）任务，encoder和decoder部分都用相同的输入，将deocoder最后一个节点用于多类别线性分类器中。对于序列标注任务，同样是在decoder和encoder采用相同的文本输入，以decoder的隐藏节点输出用于预测每个节点的类别。由于BART的模型框架本身就采用了自回归方式，因而在finetune序列生成任务时，可直接在encoder部分输入原始文本，decoder部分用于预测待生成的文本。BART预训练模型同样也可用于将其它语言翻译为英文。</li>\n<li>UniLM通过引入attention mask实现了单向、双向和seq2seq语言模型，在双向LM模型中，没有任何mask，因为上下文信息都会被融入token中。在从左向右的单向LM模型中，矩阵上半三角会被mask掉，因为token只考虑左边的文本信息。在seq-to-seq模型中，句子s1的矩阵部分没有mask，s1因为s1融合上下文信息，句子s2矩阵的右上三角有mask，因为s2融合了s1的信息和左边的文本信息。使用完形填空和下一句预测作为预训练任务。</li>\n<li>T5从另一个角度实现了NLU和NLG的统一建模，靠着大力出奇迹，采用了Prompt的训练方式，将所有NLP任务都转化成Text-to-Text（文本到文本）任务。</li>\n<li>GPT-3是一种自回归模型，使用仅有解码器的体系结构。使用下一个单词预测目标进行训练。</li>\n</ol>\n<h3 id=\"Vision-Transformer\"><a href=\"#Vision-Transformer\" class=\"headerlink\" title=\"Vision Transformer\"></a>Vision Transformer</h3><p>Vision Transformer可以是看做是CV领域对NLP领域的抄作业，直接将Transformer的结构拿来，使用embedding+encode+MLP head的网络结构，主要区别在于embedding层的不同。对于标准的Transformer模块，要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]，对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。</p>\n<h3 id=\"UNITER、CLIP、UNIMO\"><a href=\"#UNITER、CLIP、UNIMO\" class=\"headerlink\" title=\"UNITER、CLIP、UNIMO\"></a>UNITER、CLIP、UNIMO</h3><p>而在Transformer之前，多模态领域并未出现行之有效的预训练模型，图像和文本都各玩各的，毕竟图像的预训练是从imagenet带标签的数据上进行的，而NLP更加牛，它能自回归，能自己mask自己然后让模型去预测。与CV一样多模态预训练模型也是对NLP的抄作业。</p>\n<ol>\n<li>UNITER采用Transformer的encoder结构，主要是在embedding层有所不同，UNITER有两个Embedder，Image Embedder通过对Faster-RCNN的输出ROI feature以及其位置特征（7维，normalized top/left/bottom/right coordinates, width, height, and area.）进行融合建模，需要两个FC和一个LN操作完成。Text Embedder则参考BERT的输入，但是没有segment，然后直接接入Transformer进行双向建模，融合两种模态，从而达成目的，简单明了，不同于双流预训练模型，这两类模态共享同一个Encoder。预训练任务采用MLM，MRM(Mask Region Model, 预测图像，回归或分类，有三种变体)，ITM（Image Text Match, 图文是否一致）和WRA（Word Region Alignment, 字和图像的对齐任务）。</li>\n<li>CLIP用到了零样本学习（zero-shot learning）、自然语言理解和多模态学习等技术，来完成图像的理解。</li>\n<li>UNIMO采用类似的掩码预测自监督方法学习图像和文本的表示。同时，为了将文本和图像的表示映射到统一的语义空间，论文提出跨模态对比学习，基于图文对数据实现图像与文本的统一表示学习。</li>\n</ol>"},{"title":"预训练模型RoFormer浅析","date":"2022-05-06T07:51:19.000Z","mathjax":true,"_content":"\n\n近期我司发布了自研预训练语言模型RoFormerV2[1]，RoFormer的核心是基于苏神的“旋转位置编码（Rotary Position Embedding，RoPE）”[2]。今天就来学习一下RoFormer的进化过程！\n<!--more-->\n\n说实话，看完苏神在科学空间的博客中的公式推导，内心经历十分跌宕从兴致勃勃到一脸懵逼到竟然这样又到顶礼膜拜。但是既然要学习一下，就硬拿起来啃一下~苏神原文中的公式推导已经很好了，我在这篇文章主要分享一下我的理解过程吧！\n\n\n### 追根溯源\n大家都知道Transformer[1]的编码除了有词id，还需要有词的位置编码，因为attention无法区分不同位置的词。transformer的做法是将句子中所有词的绝对位置硬编码融入到输入中去。那么这种硬编码有什么优劣呢？首先编码简单是不用说的，比如一般是设置最大长度512，按照512*dim（向量维度）初始化位置向量即可。那有什么缺点呢？其实我们可以直观的感受的这种方式比较自然容易想到，但是也比较固定，不够灵活，我们的语言是千变万化的，绝对位置的编码限制了语言的自由组合，所以研究人员开始提出各种各样的相对位置编码。那么什么是相对的位置编码呢？其实也很直观，比如有一个输入的句子特别长，其实完全考虑整个句子的绝对位置编码得出的信息并没有那么多。相反那些相对位置比较近的词句反而可以提取出更多的语义特征，因此研究人员通过对带绝对位置的attention进行公式转换，截断长句位置信息，保留与该词位置较近的词的位置信息，当然方法是层出不群的。那么RoFormer做了什么呢？RoFoermer采用的RoPE旋转位置编码是一种融合了绝对位置编码和相对位置编码的编码方式。既保留了绝对位置信息，又增强了相对位置的信息。\n\n\n### RoPE思想\nRoPE假设给attention中的q，k增加绝对位置m，n信息后的函数$$f(q,m)$$和$$f(k,n)$$做点积的结果是一个之和相对位置(m-n)有关的函数$$g(q,k,m-n)$，即：$<f(q,m), f(k,n)> = g(q,k,m-n)$$然后作者使用了复数形式的向量计算进行推导出含有绝对位置形式的函数形式：$$f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}$$。根据复数乘法的几何意义，该变换可以看做是向量的旋转，这也是RoPE旋转位置编码的由来。该公式可以写成矩阵相乘的形式进行计算。RoPE采用了Sinusoidal位置编码[3]的方案，因此RoPE对于位置的编码具有远程衰减的特性，如图可见。\n<img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\">\n\n\n因为RoPE是使用绝对位置编码来实现相对位置编码，所以它不需要对Attention矩阵进行操作，可以直接应用到线性attention中。RoFormer[4]论文中的实验也表明了使用RoPE编码的RoFormer对于长文本有很好的捕捉能力，它可以很好的处理任意长的文本。\n\n\n### RoFormerV2做了什么升级\n苏神在自己的博客的标题中[5]用到了《RoFormerV2：自然语言理解的极限探索》，可以看做是对RoformerV2的一个定位。这里的极限是指什么呢？与大多数预训练模型追求超大超深的网络不同，RoFormerV2探索的是预训练模型在相同参数数量下性能的极限。RoFormerV2的主要改动就是简化模型建构，使其成为GLUE榜上前五名中参数数量最少的模型。RoFormerV2去掉了模型中的所有bias项，并且加大了训练数据从30多G加到280G（深度学习果然是数据为王呀！）。并且RoFormerV2是从零开始训练，为了抵抗去掉参数带来的效果损失，还增加了监督的多任务进行训练。\n\n\n### 参考文献\n[1] https://github.com/ZhuiyiTechnology/roformer-v2\n[2] https://spaces.ac.cn/archives/8265\n[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.\n[5] https://kexue.fm/archives/8998","source":"_posts/roformer.md","raw":"---\ntitle: 预训练模型RoFormer浅析\ndate: 2022-05-06 15:51:19\ntags:\n- PLM\ncategories: \n- 预训练语言模型\nmathjax: true\n---\n\n\n近期我司发布了自研预训练语言模型RoFormerV2[1]，RoFormer的核心是基于苏神的“旋转位置编码（Rotary Position Embedding，RoPE）”[2]。今天就来学习一下RoFormer的进化过程！\n<!--more-->\n\n说实话，看完苏神在科学空间的博客中的公式推导，内心经历十分跌宕从兴致勃勃到一脸懵逼到竟然这样又到顶礼膜拜。但是既然要学习一下，就硬拿起来啃一下~苏神原文中的公式推导已经很好了，我在这篇文章主要分享一下我的理解过程吧！\n\n\n### 追根溯源\n大家都知道Transformer[1]的编码除了有词id，还需要有词的位置编码，因为attention无法区分不同位置的词。transformer的做法是将句子中所有词的绝对位置硬编码融入到输入中去。那么这种硬编码有什么优劣呢？首先编码简单是不用说的，比如一般是设置最大长度512，按照512*dim（向量维度）初始化位置向量即可。那有什么缺点呢？其实我们可以直观的感受的这种方式比较自然容易想到，但是也比较固定，不够灵活，我们的语言是千变万化的，绝对位置的编码限制了语言的自由组合，所以研究人员开始提出各种各样的相对位置编码。那么什么是相对的位置编码呢？其实也很直观，比如有一个输入的句子特别长，其实完全考虑整个句子的绝对位置编码得出的信息并没有那么多。相反那些相对位置比较近的词句反而可以提取出更多的语义特征，因此研究人员通过对带绝对位置的attention进行公式转换，截断长句位置信息，保留与该词位置较近的词的位置信息，当然方法是层出不群的。那么RoFormer做了什么呢？RoFoermer采用的RoPE旋转位置编码是一种融合了绝对位置编码和相对位置编码的编码方式。既保留了绝对位置信息，又增强了相对位置的信息。\n\n\n### RoPE思想\nRoPE假设给attention中的q，k增加绝对位置m，n信息后的函数$$f(q,m)$$和$$f(k,n)$$做点积的结果是一个之和相对位置(m-n)有关的函数$$g(q,k,m-n)$，即：$<f(q,m), f(k,n)> = g(q,k,m-n)$$然后作者使用了复数形式的向量计算进行推导出含有绝对位置形式的函数形式：$$f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}$$。根据复数乘法的几何意义，该变换可以看做是向量的旋转，这也是RoPE旋转位置编码的由来。该公式可以写成矩阵相乘的形式进行计算。RoPE采用了Sinusoidal位置编码[3]的方案，因此RoPE对于位置的编码具有远程衰减的特性，如图可见。\n<img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\">\n\n\n因为RoPE是使用绝对位置编码来实现相对位置编码，所以它不需要对Attention矩阵进行操作，可以直接应用到线性attention中。RoFormer[4]论文中的实验也表明了使用RoPE编码的RoFormer对于长文本有很好的捕捉能力，它可以很好的处理任意长的文本。\n\n\n### RoFormerV2做了什么升级\n苏神在自己的博客的标题中[5]用到了《RoFormerV2：自然语言理解的极限探索》，可以看做是对RoformerV2的一个定位。这里的极限是指什么呢？与大多数预训练模型追求超大超深的网络不同，RoFormerV2探索的是预训练模型在相同参数数量下性能的极限。RoFormerV2的主要改动就是简化模型建构，使其成为GLUE榜上前五名中参数数量最少的模型。RoFormerV2去掉了模型中的所有bias项，并且加大了训练数据从30多G加到280G（深度学习果然是数据为王呀！）。并且RoFormerV2是从零开始训练，为了抵抗去掉参数带来的效果损失，还增加了监督的多任务进行训练。\n\n\n### 参考文献\n[1] https://github.com/ZhuiyiTechnology/roformer-v2\n[2] https://spaces.ac.cn/archives/8265\n[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.\n[5] https://kexue.fm/archives/8998","slug":"roformer","published":1,"updated":"2022-06-24T09:38:19.291Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga6z000hq4uxafn90o86","content":"<p>近期我司发布了自研预训练语言模型RoFormerV2[1]，RoFormer的核心是基于苏神的“旋转位置编码（Rotary Position Embedding，RoPE）”[2]。今天就来学习一下RoFormer的进化过程！<br><span id=\"more\"></span></p>\n<p>说实话，看完苏神在科学空间的博客中的公式推导，内心经历十分跌宕从兴致勃勃到一脸懵逼到竟然这样又到顶礼膜拜。但是既然要学习一下，就硬拿起来啃一下~苏神原文中的公式推导已经很好了，我在这篇文章主要分享一下我的理解过程吧！</p>\n<h3 id=\"追根溯源\"><a href=\"#追根溯源\" class=\"headerlink\" title=\"追根溯源\"></a>追根溯源</h3><p>大家都知道Transformer[1]的编码除了有词id，还需要有词的位置编码，因为attention无法区分不同位置的词。transformer的做法是将句子中所有词的绝对位置硬编码融入到输入中去。那么这种硬编码有什么优劣呢？首先编码简单是不用说的，比如一般是设置最大长度512，按照512*dim（向量维度）初始化位置向量即可。那有什么缺点呢？其实我们可以直观的感受的这种方式比较自然容易想到，但是也比较固定，不够灵活，我们的语言是千变万化的，绝对位置的编码限制了语言的自由组合，所以研究人员开始提出各种各样的相对位置编码。那么什么是相对的位置编码呢？其实也很直观，比如有一个输入的句子特别长，其实完全考虑整个句子的绝对位置编码得出的信息并没有那么多。相反那些相对位置比较近的词句反而可以提取出更多的语义特征，因此研究人员通过对带绝对位置的attention进行公式转换，截断长句位置信息，保留与该词位置较近的词的位置信息，当然方法是层出不群的。那么RoFormer做了什么呢？RoFoermer采用的RoPE旋转位置编码是一种融合了绝对位置编码和相对位置编码的编码方式。既保留了绝对位置信息，又增强了相对位置的信息。</p>\n<h3 id=\"RoPE思想\"><a href=\"#RoPE思想\" class=\"headerlink\" title=\"RoPE思想\"></a>RoPE思想</h3><p>RoPE假设给attention中的q，k增加绝对位置m，n信息后的函数<script type=\"math/tex\">f(q,m)</script>和<script type=\"math/tex\">f(k,n)</script>做点积的结果是一个之和相对位置(m-n)有关的函数<script type=\"math/tex\">g(q,k,m-n)$，即：$<f(q,m), f(k,n)> = g(q,k,m-n)</script>然后作者使用了复数形式的向量计算进行推导出含有绝对位置形式的函数形式：<script type=\"math/tex\">f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}</script>。根据复数乘法的几何意义，该变换可以看做是向量的旋转，这也是RoPE旋转位置编码的由来。该公式可以写成矩阵相乘的形式进行计算。RoPE采用了Sinusoidal位置编码[3]的方案，因此RoPE对于位置的编码具有远程衰减的特性，如图可见。<br><img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\"></p>\n<p>因为RoPE是使用绝对位置编码来实现相对位置编码，所以它不需要对Attention矩阵进行操作，可以直接应用到线性attention中。RoFormer[4]论文中的实验也表明了使用RoPE编码的RoFormer对于长文本有很好的捕捉能力，它可以很好的处理任意长的文本。</p>\n<h3 id=\"RoFormerV2做了什么升级\"><a href=\"#RoFormerV2做了什么升级\" class=\"headerlink\" title=\"RoFormerV2做了什么升级\"></a>RoFormerV2做了什么升级</h3><p>苏神在自己的博客的标题中[5]用到了《RoFormerV2：自然语言理解的极限探索》，可以看做是对RoformerV2的一个定位。这里的极限是指什么呢？与大多数预训练模型追求超大超深的网络不同，RoFormerV2探索的是预训练模型在相同参数数量下性能的极限。RoFormerV2的主要改动就是简化模型建构，使其成为GLUE榜上前五名中参数数量最少的模型。RoFormerV2去掉了模型中的所有bias项，并且加大了训练数据从30多G加到280G（深度学习果然是数据为王呀！）。并且RoFormerV2是从零开始训练，为了抵抗去掉参数带来的效果损失，还增加了监督的多任务进行训练。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] <a href=\"https://github.com/ZhuiyiTechnology/roformer-v2\">https://github.com/ZhuiyiTechnology/roformer-v2</a><br>[2] <a href=\"https://spaces.ac.cn/archives/8265\">https://spaces.ac.cn/archives/8265</a><br>[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.<br>[5] <a href=\"https://kexue.fm/archives/8998\">https://kexue.fm/archives/8998</a></p>\n","site":{"data":{}},"excerpt":"<p>近期我司发布了自研预训练语言模型RoFormerV2[1]，RoFormer的核心是基于苏神的“旋转位置编码（Rotary Position Embedding，RoPE）”[2]。今天就来学习一下RoFormer的进化过程！<br>","more":"</p>\n<p>说实话，看完苏神在科学空间的博客中的公式推导，内心经历十分跌宕从兴致勃勃到一脸懵逼到竟然这样又到顶礼膜拜。但是既然要学习一下，就硬拿起来啃一下~苏神原文中的公式推导已经很好了，我在这篇文章主要分享一下我的理解过程吧！</p>\n<h3 id=\"追根溯源\"><a href=\"#追根溯源\" class=\"headerlink\" title=\"追根溯源\"></a>追根溯源</h3><p>大家都知道Transformer[1]的编码除了有词id，还需要有词的位置编码，因为attention无法区分不同位置的词。transformer的做法是将句子中所有词的绝对位置硬编码融入到输入中去。那么这种硬编码有什么优劣呢？首先编码简单是不用说的，比如一般是设置最大长度512，按照512*dim（向量维度）初始化位置向量即可。那有什么缺点呢？其实我们可以直观的感受的这种方式比较自然容易想到，但是也比较固定，不够灵活，我们的语言是千变万化的，绝对位置的编码限制了语言的自由组合，所以研究人员开始提出各种各样的相对位置编码。那么什么是相对的位置编码呢？其实也很直观，比如有一个输入的句子特别长，其实完全考虑整个句子的绝对位置编码得出的信息并没有那么多。相反那些相对位置比较近的词句反而可以提取出更多的语义特征，因此研究人员通过对带绝对位置的attention进行公式转换，截断长句位置信息，保留与该词位置较近的词的位置信息，当然方法是层出不群的。那么RoFormer做了什么呢？RoFoermer采用的RoPE旋转位置编码是一种融合了绝对位置编码和相对位置编码的编码方式。既保留了绝对位置信息，又增强了相对位置的信息。</p>\n<h3 id=\"RoPE思想\"><a href=\"#RoPE思想\" class=\"headerlink\" title=\"RoPE思想\"></a>RoPE思想</h3><p>RoPE假设给attention中的q，k增加绝对位置m，n信息后的函数<script type=\"math/tex\">f(q,m)</script>和<script type=\"math/tex\">f(k,n)</script>做点积的结果是一个之和相对位置(m-n)有关的函数<script type=\"math/tex\">g(q,k,m-n)$，即：$<f(q,m), f(k,n)> = g(q,k,m-n)</script>然后作者使用了复数形式的向量计算进行推导出含有绝对位置形式的函数形式：<script type=\"math/tex\">f(q,m) = ||q||e^{i(\\Theta (q) + m\\theta)} = qe^{im\\theta}</script>。根据复数乘法的几何意义，该变换可以看做是向量的旋转，这也是RoPE旋转位置编码的由来。该公式可以写成矩阵相乘的形式进行计算。RoPE采用了Sinusoidal位置编码[3]的方案，因此RoPE对于位置的编码具有远程衰减的特性，如图可见。<br><img src=\"https://github.com/Quelisa/picture/raw/eb694e51379595f70ff9544f07f3a8b28cd30b99/RoPE.png\" width=\"50%\" height=\"50%\"></p>\n<p>因为RoPE是使用绝对位置编码来实现相对位置编码，所以它不需要对Attention矩阵进行操作，可以直接应用到线性attention中。RoFormer[4]论文中的实验也表明了使用RoPE编码的RoFormer对于长文本有很好的捕捉能力，它可以很好的处理任意长的文本。</p>\n<h3 id=\"RoFormerV2做了什么升级\"><a href=\"#RoFormerV2做了什么升级\" class=\"headerlink\" title=\"RoFormerV2做了什么升级\"></a>RoFormerV2做了什么升级</h3><p>苏神在自己的博客的标题中[5]用到了《RoFormerV2：自然语言理解的极限探索》，可以看做是对RoformerV2的一个定位。这里的极限是指什么呢？与大多数预训练模型追求超大超深的网络不同，RoFormerV2探索的是预训练模型在相同参数数量下性能的极限。RoFormerV2的主要改动就是简化模型建构，使其成为GLUE榜上前五名中参数数量最少的模型。RoFormerV2去掉了模型中的所有bias项，并且加大了训练数据从30多G加到280G（深度学习果然是数据为王呀！）。并且RoFormerV2是从零开始训练，为了抵抗去掉参数带来的效果损失，还增加了监督的多任务进行训练。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] <a href=\"https://github.com/ZhuiyiTechnology/roformer-v2\">https://github.com/ZhuiyiTechnology/roformer-v2</a><br>[2] <a href=\"https://spaces.ac.cn/archives/8265\">https://spaces.ac.cn/archives/8265</a><br>[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[4] Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.<br>[5] <a href=\"https://kexue.fm/archives/8998\">https://kexue.fm/archives/8998</a></p>"},{"title":"有关句嵌入模型的整理","date":"2022-01-15T08:41:14.000Z","mathjax":true,"_content":"\n句向量嵌入也就是sentence embedding对于理解文本语义有很重要的作用，很多任务都需要用到sentence embedding，比如文本相似度的计算、文本检索、聚类、文本语义挖掘等。因此训练具有良好特性句向量嵌入的模型十分重要。本篇主要从sentence-transformer开源的句向量嵌入模型[1]选取几个有代表性的模型，进行分析，了解句向量发展的主流技术和面临的挑战。\n\n<!--more-->\n\n\n### 无监督句嵌入模型\n收到词向量的启发，自然地想到在自然语言处理中由于词的粒度较小，表达的信息比较离散，很多的场景需要用到句向量进行建模。而无论是将词映射的向量空间还是将句子映射到向量空间，在建模时我们都希望语义相似的句子在空间彼此靠近，而语义差别大的句子在空间中的距离彼此疏远，同时尽可能的保持空间向量分布的均匀性。一般对于BERT类型的语言模型，句向量的表示可以使用其[CLS]标志位上的向量进行表示，也可以对最后一层或者多层中所有输出采样（平均值采样、最大值采样等）后的结果进行表示。为什么采用这些数据向量而它们为什么具有语义表达的能力？它们在表示语义的时候又存在哪些问题？这就是主流的句嵌入模型研究的出发点。\n\n\n#### Sentence-BERT[2]\n对于计算语义文本相似度，BERT支持同时输入句子对进行计算，但是这样的结构在推理的时候，需要将所有句子的组合都经过网络计算，计算量非常大。Sentence-BERT也称SBERT采用双塔结构，分别对每个句子进行embedding，然后将embedding存起来，再进行单独的相似度计算，可以极大的提高推理速度。缺点是相对BERT的句子对计算会带来语义匹配度上的损失。\n\nSBERT的模型结构如图，图中左侧是一个训练的结构，图中右侧是一个模型的推理的结构，训练目标针对分类任务使用了交叉熵损失函数、针对相似度计算使用了均方差损失函数以及还是用了对比学习中的Triplet loss。\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\">\n\n#### BERT-flow[3] 和 BERT-whitening[4]\nBERT-flow是字节提出的基于BERT模型的改进。考虑到未经过fine-tune的BERT在语义表达上的效果还不如GloVe，为此探索BERT的语义表达能力瓶颈，是因为BERT没有学习到语义表达的能力还是BERT的语义表达能力没有被充分地发掘？根据以往的研究可以知道BERT的词嵌入表现出各向异性，空间分布呈现锥形，高频的词分布较为集中靠近锥尖，低频词距离较远原理锥尖。因此假设BERT在文本嵌入方面可能也存在类似的问题，于是作者提出了一种映射方式，将BERT的训练出的Embeddings通过一个可逆映射到均匀分布的空间，这个映射采用的flow模型，因此叫BERT-flow。\n\n\nBERT-whitening是追一科技提出的模型改进，他认为克服BERT本身的句嵌入空间向量集合各向异性不需要一个flow模型，只要一个线性变换就可以搞定。文本相似度计算是采用cosine计算的，cosine的计算公式是向量內积再做归一化，这个等式成立的前提是标准坐标基空间。可以通过统计学上假设，如果是标准正交基，那么它对应的向量应该表现出各向同性，而BERT句向量存在各向异性，所以认为它所属的坐标系不是标准正交基。作者指出flow模型的表达能力很弱而且模型参数很大，用一个flow模型有些浪费，而根据前面的假设推理，作者采用数据挖掘中的白化操作对向量进行线性变化，变化为一个标准的均值为0，协方差矩阵为单位阵的正态分布。\n\n\n#### Constrastive Tension[5]\nConstrastive Tension也称CT，这篇文章主要研究了对于STS任务transformer架构每一层的句嵌入向量的特点，然后针对句嵌入向量层敏感的特性采用对比方式进行改进。下面是几个主流模型每一层对于语义的敏感程度。\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\">\n可以看到在最后一层的时候语义表征反而降低了。为了对抗这种语义表达的损失，CT的核心是采用两个单独的模型，初始化参数相同，对最后一层的进行平均采用，采用对比损失，最大化相同句子的相似度并最小化不同句子的相似度。感觉本质上就是一种对比训练。\n\n\n#### SimCSE[6]\nSimCSE真的YYDS！完全没有花里胡哨的东西，还非常能打，最近一直在用SimCSE的方法进行对比训练，效果都有提升。SimCSE主要采用非对比学的方法，采用InfoNCE loss。创新点在于正例的选择，没有用各种奇奇怪怪的数据增强方式，仅用dropout，因为在dropout不为0时，同样的句子分别输入网络得到的句向量就是不同的。负例采用同一个batch中的其他句子。无监督学习，数据获取容易，效果好！\n\n\nSimCSE还做了一个监督的对比训练，其实主要是用类似于推理的数据，三个句子同时输入，前两个保持一致，最后一个句子和前面的矛盾，用来做强约束。效果比无监督的好，但是实际使用中感觉无监督更好用一些，毕竟构造高质量有监督的数据还是比较麻烦的。\n\n\n#### TSDAE[7]\nTSDAE是基于Transformer结构的序列噪声自编码模型，模型结构如下：\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\">\n它主要是在模型的输入中将个别词进行删除或者替换，通过encoder编码成一个固定长度的向量，然后通过decoder重建原来的输入。TSDAE在多个数据集上的测试效果都优于CT、SimCSE和BERT-flow等。\n\n\n\n#### LaBSE[8]\nLaBSE是跨语言的预训练模型，它支持109中语言，具有跨语言迁移的效果。它采用基于BERT的双塔模型架构。\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\">\n\n它使用对比学习进行。其loss是在InfoNCE loss的基础上进行了修改，加入了margin，可以更好地拉开正例和负例的距离。同时对比学习中负例的样本数越大，对比效果越好，所以LaBSE在训练是采用了交叉加速负采样（就是使用n个TPU，每个核中batch为8，从同一个batch采样负例的同时也从其他的核中采样负例），从而达到增加负例的效果。我感觉LaBSE另一个重大的贡献在于采用了大量而且丰富的多语种语料，才能达到如此优秀的跨语种迁移能力。\n\n\n### 方法总结\n目前主流的句向量模型都是在BERT或者BERT类型的预训练模型上，针对BERT预训练模型语义表达的痛点进行改进，例如通过空间映射到均匀空间解决BERT句向量空间各向异性的问题；还有采用对比学习的方法，通过拉近相似文本的空间向量表示，疏远不同样本的空间向量表示等。究其本质，都是默认把文本编码成句向量，希望空间的句向量表达在空间分布均匀，在语义理解上根据向量的距离进行识别。\n\n### 参考文献\n[1] https://www.sbert.net/\n[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.\n[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.\n[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.\n[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.\n[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.\n[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.","source":"_posts/sentence-transformers.md","raw":"---\ntitle: 有关句嵌入模型的整理\ndate: 2022-01-15 16:41:14\ntags:\n- 文本匹配\n- Representation Learning\ncategories: \n- Sentence Embedding\nmathjax: true\n---\n\n句向量嵌入也就是sentence embedding对于理解文本语义有很重要的作用，很多任务都需要用到sentence embedding，比如文本相似度的计算、文本检索、聚类、文本语义挖掘等。因此训练具有良好特性句向量嵌入的模型十分重要。本篇主要从sentence-transformer开源的句向量嵌入模型[1]选取几个有代表性的模型，进行分析，了解句向量发展的主流技术和面临的挑战。\n\n<!--more-->\n\n\n### 无监督句嵌入模型\n收到词向量的启发，自然地想到在自然语言处理中由于词的粒度较小，表达的信息比较离散，很多的场景需要用到句向量进行建模。而无论是将词映射的向量空间还是将句子映射到向量空间，在建模时我们都希望语义相似的句子在空间彼此靠近，而语义差别大的句子在空间中的距离彼此疏远，同时尽可能的保持空间向量分布的均匀性。一般对于BERT类型的语言模型，句向量的表示可以使用其[CLS]标志位上的向量进行表示，也可以对最后一层或者多层中所有输出采样（平均值采样、最大值采样等）后的结果进行表示。为什么采用这些数据向量而它们为什么具有语义表达的能力？它们在表示语义的时候又存在哪些问题？这就是主流的句嵌入模型研究的出发点。\n\n\n#### Sentence-BERT[2]\n对于计算语义文本相似度，BERT支持同时输入句子对进行计算，但是这样的结构在推理的时候，需要将所有句子的组合都经过网络计算，计算量非常大。Sentence-BERT也称SBERT采用双塔结构，分别对每个句子进行embedding，然后将embedding存起来，再进行单独的相似度计算，可以极大的提高推理速度。缺点是相对BERT的句子对计算会带来语义匹配度上的损失。\n\nSBERT的模型结构如图，图中左侧是一个训练的结构，图中右侧是一个模型的推理的结构，训练目标针对分类任务使用了交叉熵损失函数、针对相似度计算使用了均方差损失函数以及还是用了对比学习中的Triplet loss。\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\">\n\n#### BERT-flow[3] 和 BERT-whitening[4]\nBERT-flow是字节提出的基于BERT模型的改进。考虑到未经过fine-tune的BERT在语义表达上的效果还不如GloVe，为此探索BERT的语义表达能力瓶颈，是因为BERT没有学习到语义表达的能力还是BERT的语义表达能力没有被充分地发掘？根据以往的研究可以知道BERT的词嵌入表现出各向异性，空间分布呈现锥形，高频的词分布较为集中靠近锥尖，低频词距离较远原理锥尖。因此假设BERT在文本嵌入方面可能也存在类似的问题，于是作者提出了一种映射方式，将BERT的训练出的Embeddings通过一个可逆映射到均匀分布的空间，这个映射采用的flow模型，因此叫BERT-flow。\n\n\nBERT-whitening是追一科技提出的模型改进，他认为克服BERT本身的句嵌入空间向量集合各向异性不需要一个flow模型，只要一个线性变换就可以搞定。文本相似度计算是采用cosine计算的，cosine的计算公式是向量內积再做归一化，这个等式成立的前提是标准坐标基空间。可以通过统计学上假设，如果是标准正交基，那么它对应的向量应该表现出各向同性，而BERT句向量存在各向异性，所以认为它所属的坐标系不是标准正交基。作者指出flow模型的表达能力很弱而且模型参数很大，用一个flow模型有些浪费，而根据前面的假设推理，作者采用数据挖掘中的白化操作对向量进行线性变化，变化为一个标准的均值为0，协方差矩阵为单位阵的正态分布。\n\n\n#### Constrastive Tension[5]\nConstrastive Tension也称CT，这篇文章主要研究了对于STS任务transformer架构每一层的句嵌入向量的特点，然后针对句嵌入向量层敏感的特性采用对比方式进行改进。下面是几个主流模型每一层对于语义的敏感程度。\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\">\n可以看到在最后一层的时候语义表征反而降低了。为了对抗这种语义表达的损失，CT的核心是采用两个单独的模型，初始化参数相同，对最后一层的进行平均采用，采用对比损失，最大化相同句子的相似度并最小化不同句子的相似度。感觉本质上就是一种对比训练。\n\n\n#### SimCSE[6]\nSimCSE真的YYDS！完全没有花里胡哨的东西，还非常能打，最近一直在用SimCSE的方法进行对比训练，效果都有提升。SimCSE主要采用非对比学的方法，采用InfoNCE loss。创新点在于正例的选择，没有用各种奇奇怪怪的数据增强方式，仅用dropout，因为在dropout不为0时，同样的句子分别输入网络得到的句向量就是不同的。负例采用同一个batch中的其他句子。无监督学习，数据获取容易，效果好！\n\n\nSimCSE还做了一个监督的对比训练，其实主要是用类似于推理的数据，三个句子同时输入，前两个保持一致，最后一个句子和前面的矛盾，用来做强约束。效果比无监督的好，但是实际使用中感觉无监督更好用一些，毕竟构造高质量有监督的数据还是比较麻烦的。\n\n\n#### TSDAE[7]\nTSDAE是基于Transformer结构的序列噪声自编码模型，模型结构如下：\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\">\n它主要是在模型的输入中将个别词进行删除或者替换，通过encoder编码成一个固定长度的向量，然后通过decoder重建原来的输入。TSDAE在多个数据集上的测试效果都优于CT、SimCSE和BERT-flow等。\n\n\n\n#### LaBSE[8]\nLaBSE是跨语言的预训练模型，它支持109中语言，具有跨语言迁移的效果。它采用基于BERT的双塔模型架构。\n<img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\">\n\n它使用对比学习进行。其loss是在InfoNCE loss的基础上进行了修改，加入了margin，可以更好地拉开正例和负例的距离。同时对比学习中负例的样本数越大，对比效果越好，所以LaBSE在训练是采用了交叉加速负采样（就是使用n个TPU，每个核中batch为8，从同一个batch采样负例的同时也从其他的核中采样负例），从而达到增加负例的效果。我感觉LaBSE另一个重大的贡献在于采用了大量而且丰富的多语种语料，才能达到如此优秀的跨语种迁移能力。\n\n\n### 方法总结\n目前主流的句向量模型都是在BERT或者BERT类型的预训练模型上，针对BERT预训练模型语义表达的痛点进行改进，例如通过空间映射到均匀空间解决BERT句向量空间各向异性的问题；还有采用对比学习的方法，通过拉近相似文本的空间向量表示，疏远不同样本的空间向量表示等。究其本质，都是默认把文本编码成句向量，希望空间的句向量表达在空间分布均匀，在语义理解上根据向量的距离进行识别。\n\n### 参考文献\n[1] https://www.sbert.net/\n[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.\n[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.\n[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.\n[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.\n[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.\n[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.\n[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.","slug":"sentence-transformers","published":1,"updated":"2022-06-07T07:07:09.539Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga72000jq4ux7xq4fllg","content":"<p>句向量嵌入也就是sentence embedding对于理解文本语义有很重要的作用，很多任务都需要用到sentence embedding，比如文本相似度的计算、文本检索、聚类、文本语义挖掘等。因此训练具有良好特性句向量嵌入的模型十分重要。本篇主要从sentence-transformer开源的句向量嵌入模型[1]选取几个有代表性的模型，进行分析，了解句向量发展的主流技术和面临的挑战。</p>\n<span id=\"more\"></span>\n<h3 id=\"无监督句嵌入模型\"><a href=\"#无监督句嵌入模型\" class=\"headerlink\" title=\"无监督句嵌入模型\"></a>无监督句嵌入模型</h3><p>收到词向量的启发，自然地想到在自然语言处理中由于词的粒度较小，表达的信息比较离散，很多的场景需要用到句向量进行建模。而无论是将词映射的向量空间还是将句子映射到向量空间，在建模时我们都希望语义相似的句子在空间彼此靠近，而语义差别大的句子在空间中的距离彼此疏远，同时尽可能的保持空间向量分布的均匀性。一般对于BERT类型的语言模型，句向量的表示可以使用其[CLS]标志位上的向量进行表示，也可以对最后一层或者多层中所有输出采样（平均值采样、最大值采样等）后的结果进行表示。为什么采用这些数据向量而它们为什么具有语义表达的能力？它们在表示语义的时候又存在哪些问题？这就是主流的句嵌入模型研究的出发点。</p>\n<h4 id=\"Sentence-BERT-2\"><a href=\"#Sentence-BERT-2\" class=\"headerlink\" title=\"Sentence-BERT[2]\"></a>Sentence-BERT[2]</h4><p>对于计算语义文本相似度，BERT支持同时输入句子对进行计算，但是这样的结构在推理的时候，需要将所有句子的组合都经过网络计算，计算量非常大。Sentence-BERT也称SBERT采用双塔结构，分别对每个句子进行embedding，然后将embedding存起来，再进行单独的相似度计算，可以极大的提高推理速度。缺点是相对BERT的句子对计算会带来语义匹配度上的损失。</p>\n<p>SBERT的模型结构如图，图中左侧是一个训练的结构，图中右侧是一个模型的推理的结构，训练目标针对分类任务使用了交叉熵损失函数、针对相似度计算使用了均方差损失函数以及还是用了对比学习中的Triplet loss。<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\"></p>\n<h4 id=\"BERT-flow-3-和-BERT-whitening-4\"><a href=\"#BERT-flow-3-和-BERT-whitening-4\" class=\"headerlink\" title=\"BERT-flow[3] 和 BERT-whitening[4]\"></a>BERT-flow[3] 和 BERT-whitening[4]</h4><p>BERT-flow是字节提出的基于BERT模型的改进。考虑到未经过fine-tune的BERT在语义表达上的效果还不如GloVe，为此探索BERT的语义表达能力瓶颈，是因为BERT没有学习到语义表达的能力还是BERT的语义表达能力没有被充分地发掘？根据以往的研究可以知道BERT的词嵌入表现出各向异性，空间分布呈现锥形，高频的词分布较为集中靠近锥尖，低频词距离较远原理锥尖。因此假设BERT在文本嵌入方面可能也存在类似的问题，于是作者提出了一种映射方式，将BERT的训练出的Embeddings通过一个可逆映射到均匀分布的空间，这个映射采用的flow模型，因此叫BERT-flow。</p>\n<p>BERT-whitening是追一科技提出的模型改进，他认为克服BERT本身的句嵌入空间向量集合各向异性不需要一个flow模型，只要一个线性变换就可以搞定。文本相似度计算是采用cosine计算的，cosine的计算公式是向量內积再做归一化，这个等式成立的前提是标准坐标基空间。可以通过统计学上假设，如果是标准正交基，那么它对应的向量应该表现出各向同性，而BERT句向量存在各向异性，所以认为它所属的坐标系不是标准正交基。作者指出flow模型的表达能力很弱而且模型参数很大，用一个flow模型有些浪费，而根据前面的假设推理，作者采用数据挖掘中的白化操作对向量进行线性变化，变化为一个标准的均值为0，协方差矩阵为单位阵的正态分布。</p>\n<h4 id=\"Constrastive-Tension-5\"><a href=\"#Constrastive-Tension-5\" class=\"headerlink\" title=\"Constrastive Tension[5]\"></a>Constrastive Tension[5]</h4><p>Constrastive Tension也称CT，这篇文章主要研究了对于STS任务transformer架构每一层的句嵌入向量的特点，然后针对句嵌入向量层敏感的特性采用对比方式进行改进。下面是几个主流模型每一层对于语义的敏感程度。<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\"><br>可以看到在最后一层的时候语义表征反而降低了。为了对抗这种语义表达的损失，CT的核心是采用两个单独的模型，初始化参数相同，对最后一层的进行平均采用，采用对比损失，最大化相同句子的相似度并最小化不同句子的相似度。感觉本质上就是一种对比训练。</p>\n<h4 id=\"SimCSE-6\"><a href=\"#SimCSE-6\" class=\"headerlink\" title=\"SimCSE[6]\"></a>SimCSE[6]</h4><p>SimCSE真的YYDS！完全没有花里胡哨的东西，还非常能打，最近一直在用SimCSE的方法进行对比训练，效果都有提升。SimCSE主要采用非对比学的方法，采用InfoNCE loss。创新点在于正例的选择，没有用各种奇奇怪怪的数据增强方式，仅用dropout，因为在dropout不为0时，同样的句子分别输入网络得到的句向量就是不同的。负例采用同一个batch中的其他句子。无监督学习，数据获取容易，效果好！</p>\n<p>SimCSE还做了一个监督的对比训练，其实主要是用类似于推理的数据，三个句子同时输入，前两个保持一致，最后一个句子和前面的矛盾，用来做强约束。效果比无监督的好，但是实际使用中感觉无监督更好用一些，毕竟构造高质量有监督的数据还是比较麻烦的。</p>\n<h4 id=\"TSDAE-7\"><a href=\"#TSDAE-7\" class=\"headerlink\" title=\"TSDAE[7]\"></a>TSDAE[7]</h4><p>TSDAE是基于Transformer结构的序列噪声自编码模型，模型结构如下：<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\"><br>它主要是在模型的输入中将个别词进行删除或者替换，通过encoder编码成一个固定长度的向量，然后通过decoder重建原来的输入。TSDAE在多个数据集上的测试效果都优于CT、SimCSE和BERT-flow等。</p>\n<h4 id=\"LaBSE-8\"><a href=\"#LaBSE-8\" class=\"headerlink\" title=\"LaBSE[8]\"></a>LaBSE[8]</h4><p>LaBSE是跨语言的预训练模型，它支持109中语言，具有跨语言迁移的效果。它采用基于BERT的双塔模型架构。<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\"></p>\n<p>它使用对比学习进行。其loss是在InfoNCE loss的基础上进行了修改，加入了margin，可以更好地拉开正例和负例的距离。同时对比学习中负例的样本数越大，对比效果越好，所以LaBSE在训练是采用了交叉加速负采样（就是使用n个TPU，每个核中batch为8，从同一个batch采样负例的同时也从其他的核中采样负例），从而达到增加负例的效果。我感觉LaBSE另一个重大的贡献在于采用了大量而且丰富的多语种语料，才能达到如此优秀的跨语种迁移能力。</p>\n<h3 id=\"方法总结\"><a href=\"#方法总结\" class=\"headerlink\" title=\"方法总结\"></a>方法总结</h3><p>目前主流的句向量模型都是在BERT或者BERT类型的预训练模型上，针对BERT预训练模型语义表达的痛点进行改进，例如通过空间映射到均匀空间解决BERT句向量空间各向异性的问题；还有采用对比学习的方法，通过拉近相似文本的空间向量表示，疏远不同样本的空间向量表示等。究其本质，都是默认把文本编码成句向量，希望空间的句向量表达在空间分布均匀，在语义理解上根据向量的距离进行识别。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] <a href=\"https://www.sbert.net/\">https://www.sbert.net/</a><br>[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.<br>[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.<br>[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.<br>[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.<br>[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.<br>[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.</p>\n","site":{"data":{}},"excerpt":"<p>句向量嵌入也就是sentence embedding对于理解文本语义有很重要的作用，很多任务都需要用到sentence embedding，比如文本相似度的计算、文本检索、聚类、文本语义挖掘等。因此训练具有良好特性句向量嵌入的模型十分重要。本篇主要从sentence-transformer开源的句向量嵌入模型[1]选取几个有代表性的模型，进行分析，了解句向量发展的主流技术和面临的挑战。</p>","more":"<h3 id=\"无监督句嵌入模型\"><a href=\"#无监督句嵌入模型\" class=\"headerlink\" title=\"无监督句嵌入模型\"></a>无监督句嵌入模型</h3><p>收到词向量的启发，自然地想到在自然语言处理中由于词的粒度较小，表达的信息比较离散，很多的场景需要用到句向量进行建模。而无论是将词映射的向量空间还是将句子映射到向量空间，在建模时我们都希望语义相似的句子在空间彼此靠近，而语义差别大的句子在空间中的距离彼此疏远，同时尽可能的保持空间向量分布的均匀性。一般对于BERT类型的语言模型，句向量的表示可以使用其[CLS]标志位上的向量进行表示，也可以对最后一层或者多层中所有输出采样（平均值采样、最大值采样等）后的结果进行表示。为什么采用这些数据向量而它们为什么具有语义表达的能力？它们在表示语义的时候又存在哪些问题？这就是主流的句嵌入模型研究的出发点。</p>\n<h4 id=\"Sentence-BERT-2\"><a href=\"#Sentence-BERT-2\" class=\"headerlink\" title=\"Sentence-BERT[2]\"></a>Sentence-BERT[2]</h4><p>对于计算语义文本相似度，BERT支持同时输入句子对进行计算，但是这样的结构在推理的时候，需要将所有句子的组合都经过网络计算，计算量非常大。Sentence-BERT也称SBERT采用双塔结构，分别对每个句子进行embedding，然后将embedding存起来，再进行单独的相似度计算，可以极大的提高推理速度。缺点是相对BERT的句子对计算会带来语义匹配度上的损失。</p>\n<p>SBERT的模型结构如图，图中左侧是一个训练的结构，图中右侧是一个模型的推理的结构，训练目标针对分类任务使用了交叉熵损失函数、针对相似度计算使用了均方差损失函数以及还是用了对比学习中的Triplet loss。<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/SBERT.png\" width=\"60%\" height=\"50%\"></p>\n<h4 id=\"BERT-flow-3-和-BERT-whitening-4\"><a href=\"#BERT-flow-3-和-BERT-whitening-4\" class=\"headerlink\" title=\"BERT-flow[3] 和 BERT-whitening[4]\"></a>BERT-flow[3] 和 BERT-whitening[4]</h4><p>BERT-flow是字节提出的基于BERT模型的改进。考虑到未经过fine-tune的BERT在语义表达上的效果还不如GloVe，为此探索BERT的语义表达能力瓶颈，是因为BERT没有学习到语义表达的能力还是BERT的语义表达能力没有被充分地发掘？根据以往的研究可以知道BERT的词嵌入表现出各向异性，空间分布呈现锥形，高频的词分布较为集中靠近锥尖，低频词距离较远原理锥尖。因此假设BERT在文本嵌入方面可能也存在类似的问题，于是作者提出了一种映射方式，将BERT的训练出的Embeddings通过一个可逆映射到均匀分布的空间，这个映射采用的flow模型，因此叫BERT-flow。</p>\n<p>BERT-whitening是追一科技提出的模型改进，他认为克服BERT本身的句嵌入空间向量集合各向异性不需要一个flow模型，只要一个线性变换就可以搞定。文本相似度计算是采用cosine计算的，cosine的计算公式是向量內积再做归一化，这个等式成立的前提是标准坐标基空间。可以通过统计学上假设，如果是标准正交基，那么它对应的向量应该表现出各向同性，而BERT句向量存在各向异性，所以认为它所属的坐标系不是标准正交基。作者指出flow模型的表达能力很弱而且模型参数很大，用一个flow模型有些浪费，而根据前面的假设推理，作者采用数据挖掘中的白化操作对向量进行线性变化，变化为一个标准的均值为0，协方差矩阵为单位阵的正态分布。</p>\n<h4 id=\"Constrastive-Tension-5\"><a href=\"#Constrastive-Tension-5\" class=\"headerlink\" title=\"Constrastive Tension[5]\"></a>Constrastive Tension[5]</h4><p>Constrastive Tension也称CT，这篇文章主要研究了对于STS任务transformer架构每一层的句嵌入向量的特点，然后针对句嵌入向量层敏感的特性采用对比方式进行改进。下面是几个主流模型每一层对于语义的敏感程度。<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/transformer-layer-sts.png\" width=\"60%\" height=\"80%\"><br>可以看到在最后一层的时候语义表征反而降低了。为了对抗这种语义表达的损失，CT的核心是采用两个单独的模型，初始化参数相同，对最后一层的进行平均采用，采用对比损失，最大化相同句子的相似度并最小化不同句子的相似度。感觉本质上就是一种对比训练。</p>\n<h4 id=\"SimCSE-6\"><a href=\"#SimCSE-6\" class=\"headerlink\" title=\"SimCSE[6]\"></a>SimCSE[6]</h4><p>SimCSE真的YYDS！完全没有花里胡哨的东西，还非常能打，最近一直在用SimCSE的方法进行对比训练，效果都有提升。SimCSE主要采用非对比学的方法，采用InfoNCE loss。创新点在于正例的选择，没有用各种奇奇怪怪的数据增强方式，仅用dropout，因为在dropout不为0时，同样的句子分别输入网络得到的句向量就是不同的。负例采用同一个batch中的其他句子。无监督学习，数据获取容易，效果好！</p>\n<p>SimCSE还做了一个监督的对比训练，其实主要是用类似于推理的数据，三个句子同时输入，前两个保持一致，最后一个句子和前面的矛盾，用来做强约束。效果比无监督的好，但是实际使用中感觉无监督更好用一些，毕竟构造高质量有监督的数据还是比较麻烦的。</p>\n<h4 id=\"TSDAE-7\"><a href=\"#TSDAE-7\" class=\"headerlink\" title=\"TSDAE[7]\"></a>TSDAE[7]</h4><p>TSDAE是基于Transformer结构的序列噪声自编码模型，模型结构如下：<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/TSDAE.png\" width=\"30%\" height=\"40%\"><br>它主要是在模型的输入中将个别词进行删除或者替换，通过encoder编码成一个固定长度的向量，然后通过decoder重建原来的输入。TSDAE在多个数据集上的测试效果都优于CT、SimCSE和BERT-flow等。</p>\n<h4 id=\"LaBSE-8\"><a href=\"#LaBSE-8\" class=\"headerlink\" title=\"LaBSE[8]\"></a>LaBSE[8]</h4><p>LaBSE是跨语言的预训练模型，它支持109中语言，具有跨语言迁移的效果。它采用基于BERT的双塔模型架构。<br><img src=\"https://github.com/Quelisa/picture/raw/main/sentence-embedding/LaBSE.png\" width=\"50%\" height=\"40%\"></p>\n<p>它使用对比学习进行。其loss是在InfoNCE loss的基础上进行了修改，加入了margin，可以更好地拉开正例和负例的距离。同时对比学习中负例的样本数越大，对比效果越好，所以LaBSE在训练是采用了交叉加速负采样（就是使用n个TPU，每个核中batch为8，从同一个batch采样负例的同时也从其他的核中采样负例），从而达到增加负例的效果。我感觉LaBSE另一个重大的贡献在于采用了大量而且丰富的多语种语料，才能达到如此优秀的跨语种迁移能力。</p>\n<h3 id=\"方法总结\"><a href=\"#方法总结\" class=\"headerlink\" title=\"方法总结\"></a>方法总结</h3><p>目前主流的句向量模型都是在BERT或者BERT类型的预训练模型上，针对BERT预训练模型语义表达的痛点进行改进，例如通过空间映射到均匀空间解决BERT句向量空间各向异性的问题；还有采用对比学习的方法，通过拉近相似文本的空间向量表示，疏远不同样本的空间向量表示等。究其本质，都是默认把文本编码成句向量，希望空间的句向量表达在空间分布均匀，在语义理解上根据向量的距离进行识别。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] <a href=\"https://www.sbert.net/\">https://www.sbert.net/</a><br>[2] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.<br>[3] Li B, Zhou H, He J, et al. On the sentence embeddings from pre-trained language models[J]. arXiv preprint arXiv:2011.05864, 2020.<br>[4] Su J, Cao J, Liu W, et al. Whitening sentence representations for better semantics and faster retrieval[J]. arXiv preprint arXiv:2103.15316, 2021.<br>[5] Carlsson F, Gyllensten A C, Gogoulou E, et al. Semantic re-tuning with contrastive tension[C]//International Conference on Learning Representations. 2020.<br>[6] Gao T, Yao X, Chen D. Simcse: Simple contrastive learning of sentence embeddings[J]. arXiv preprint arXiv:2104.08821, 2021.<br>[7] Wang K, Reimers N, Gurevych I. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning[J]. arXiv preprint arXiv:2104.06979, 2021.<br>[8] Feng F, Yang Y, Cer D, et al. Language-agnostic bert sentence embedding[J]. arXiv preprint arXiv:2007.01852, 2020.</p>"},{"title":"我对时间实体mention的理解","date":"2021-01-06T08:40:04.000Z","_content":"\n任务型对话中会话的推进主要依靠意图的识别和实体信息的收集。而实体识别目前在工业落地主要使用规则模板的方式来保证准确率，而且实体类别众多，每一类实体的mention要考虑的情况特别多，实现起来相对复杂，因此具有有很大的提升空间。\n<!--more-->\n\n\n### 实体识别和mention的关系\n实体识别应用比较多的主要是关系抽取和对话理解。关系抽取主要是将抽取到的实体和关系进行匹配，所以主要对实体识别的准确率要求较高。而任务型对话除了要求高精度的实体识别外还要进行实体的mention。什么是实体mention呢？我自己的理解就是，比如用户访问：”我要明天到北京的机票“。这里的”明天“要被是被成一个时间的实体，这个实体的mention是具体的那一天比如明天是2021年5月12号，这里明天对应的mention就是2021年5月12号，这样对话机器人拿到这个实体的信息才是有意义的，它才可以通过查询拿到用户需要的信息并返回给用户。\n\n\n### 时间实体识别的分类\n目前主流的对于时间实体识别的分类主要分为date、time和duration。\n1. date是日期，例如2021年5月1日\n2. time是时间，例如12点58分\n3. duration是一个时间单位的大小，比如5年、半个月、2两小时、十秒等\n\n\n### 时间实体mention的挑战\n1. 时间分类的多样性，比如时间点、时间段、时间周期\n2. 时间的规则多样性，比如2月闰年29天，平年28天，节假日的规定不固定等，导致时间的校验困难\n3. 时间的颗粒度控制，比如用户关系的是小时范围发生的事还是一天范围发生的事\n4. 时间表达的多样性，比如3点15分可以表达为3点一刻，还比如昨天，过去两个小时等\n\n\n### 时间实体mention的范围\nmention的划分根据不同的维度有不同的分类，由于我们的场景的任务型问答场景，所以主要针对真实场景中遇到问题进行mention的划分，为后一步的mention抽取设计做好准备。下面可以举几个实际业务中的例子进行直观感受一下。\n\n(1) 举例1：准确的时间描述  \nuser：我想预定今天下午2点的《功夫熊猫》   \nbot： 好的，已为您预定2021年5月6号14:00:00的《功夫熊猫》  \n\n(2) 举例2：准确的日期描述  \nbot: 请问什么时候发货  \nuser: 后天  \nbot: 好的，发货日期为2021年5月8号  \n\n(3) 举例3：准确的时间段描述  \nbot: 请问预定什么时间段呢？  \nuser: 今天上午10点到11点半  \nbot: 已为您预定2021年5月6号10:00:00-2021年5月6号11:30:00的会议室  \n\n(4) 举例4：准确日期段的描述  \nbot: 请问举办日期段是？  \nuser: 2021年5月6号到2021年5月12号  \nbot: 已为您预定2021年5月6号-2021年5月12号的举办场地  \n\n(5) 举例5：通过推断的时间描述  \nuser: 15分钟后提醒我开会  \nbot: 好的，已为您设置2021年5月5号14:15:00开会的闹钟  \n\n(6) 举例6：通过推断的日期描述  \nbot: 请问什么时候送达  \nuser: 母情节那天  \nbot: 好的，我们将在2021年5月8号为您送达。  \n\n(7) 举例7：通过推断的时间段描述  \nbot：请问你需要订阅多久  \nuser：预定一个月  \nbot: 好的，已为您开通2021年5月6号00:00:00-2021年6月6号00:00:00的超级会员  \n\n(8) 举例8：通过推断的时间周期  \nbot: 请问要例会安排在什么时候  \nuser: 每天早上10点  \nbot: 已为您安排每天上午10:00:00的例会  \n\n(9) 举例9：通过推断的时间周期  \nbot: 请问要安排在哪几天  \nuser: 每周二和周三  \nbot: keep为您在每周二、周三安排训练  \n\n\n以上的mention主要将时间根据时间点和时间段进行一个大的分类，在时间点内根据时间颗粒度的不同又划分为日期和每日的时间。在这三个基本面上，根据时间的周期特性又可以延伸出周期时间点和周期时间段，根据人们日常表达中的口语特性又可以延伸出推测时间点和推测时间段。所以整体的mention类型划分如下：\n\n1. 准确时间mention：一天内时间（time）、日期（date）和时间段（duration）以及日期时间段\n2. 周期时间mention：每天的时间周期（cycle_time）、日期的周期（cycle_date）和时间段的周期（cycle_duration）\n3. 推测时间mention：准确时间点+时间长度的（shift_time）、准确日期点+日期长度的（shift_date）和准确时间段+时间长度的（shift_duration）\n4. 推测周期时间mention：每天的时间周期+时间长度的（cycle_shift_time）、日期的周期+日期长度的（cycle_shift_date）和时间段的周期+时间长度的（cycle_shift_duration）\n\n\n### 设计遐想\n\n在明确了mention分类后，其实已经有了整体的mention框架，mention的第一步无疑是进行时间实体的识别，识别的归类是date、time和duration，除了识别时间外还要识别。实体识别后要进行一步归一化处理，抽取出标准的表达，例如“2021年5月6日”识别为一个准确date的实体，具体归一化为一个标准的表达“2021年5月6号”。“明天”则识别为推测时间，在归一化时需要取得当前日期，在当前日期上进行时长运算得到标准的时间表达“2021年5月7日”。对于周期性的时间表达，如每周六等系统默认给出一个范围内的所有周六的计算方式。\n\n### TODO\n这是一个初步的设计的思路，后续有空补详细代码设计和代码实现。\n\n\n","source":"_posts/time-mention.md","raw":"---\ntitle: 我对时间实体mention的理解\ndate: 2021-01-06 16:40:04\ntags:\n- mention\n- 时间实体\ncategories: \n- 实体识别\n---\n\n任务型对话中会话的推进主要依靠意图的识别和实体信息的收集。而实体识别目前在工业落地主要使用规则模板的方式来保证准确率，而且实体类别众多，每一类实体的mention要考虑的情况特别多，实现起来相对复杂，因此具有有很大的提升空间。\n<!--more-->\n\n\n### 实体识别和mention的关系\n实体识别应用比较多的主要是关系抽取和对话理解。关系抽取主要是将抽取到的实体和关系进行匹配，所以主要对实体识别的准确率要求较高。而任务型对话除了要求高精度的实体识别外还要进行实体的mention。什么是实体mention呢？我自己的理解就是，比如用户访问：”我要明天到北京的机票“。这里的”明天“要被是被成一个时间的实体，这个实体的mention是具体的那一天比如明天是2021年5月12号，这里明天对应的mention就是2021年5月12号，这样对话机器人拿到这个实体的信息才是有意义的，它才可以通过查询拿到用户需要的信息并返回给用户。\n\n\n### 时间实体识别的分类\n目前主流的对于时间实体识别的分类主要分为date、time和duration。\n1. date是日期，例如2021年5月1日\n2. time是时间，例如12点58分\n3. duration是一个时间单位的大小，比如5年、半个月、2两小时、十秒等\n\n\n### 时间实体mention的挑战\n1. 时间分类的多样性，比如时间点、时间段、时间周期\n2. 时间的规则多样性，比如2月闰年29天，平年28天，节假日的规定不固定等，导致时间的校验困难\n3. 时间的颗粒度控制，比如用户关系的是小时范围发生的事还是一天范围发生的事\n4. 时间表达的多样性，比如3点15分可以表达为3点一刻，还比如昨天，过去两个小时等\n\n\n### 时间实体mention的范围\nmention的划分根据不同的维度有不同的分类，由于我们的场景的任务型问答场景，所以主要针对真实场景中遇到问题进行mention的划分，为后一步的mention抽取设计做好准备。下面可以举几个实际业务中的例子进行直观感受一下。\n\n(1) 举例1：准确的时间描述  \nuser：我想预定今天下午2点的《功夫熊猫》   \nbot： 好的，已为您预定2021年5月6号14:00:00的《功夫熊猫》  \n\n(2) 举例2：准确的日期描述  \nbot: 请问什么时候发货  \nuser: 后天  \nbot: 好的，发货日期为2021年5月8号  \n\n(3) 举例3：准确的时间段描述  \nbot: 请问预定什么时间段呢？  \nuser: 今天上午10点到11点半  \nbot: 已为您预定2021年5月6号10:00:00-2021年5月6号11:30:00的会议室  \n\n(4) 举例4：准确日期段的描述  \nbot: 请问举办日期段是？  \nuser: 2021年5月6号到2021年5月12号  \nbot: 已为您预定2021年5月6号-2021年5月12号的举办场地  \n\n(5) 举例5：通过推断的时间描述  \nuser: 15分钟后提醒我开会  \nbot: 好的，已为您设置2021年5月5号14:15:00开会的闹钟  \n\n(6) 举例6：通过推断的日期描述  \nbot: 请问什么时候送达  \nuser: 母情节那天  \nbot: 好的，我们将在2021年5月8号为您送达。  \n\n(7) 举例7：通过推断的时间段描述  \nbot：请问你需要订阅多久  \nuser：预定一个月  \nbot: 好的，已为您开通2021年5月6号00:00:00-2021年6月6号00:00:00的超级会员  \n\n(8) 举例8：通过推断的时间周期  \nbot: 请问要例会安排在什么时候  \nuser: 每天早上10点  \nbot: 已为您安排每天上午10:00:00的例会  \n\n(9) 举例9：通过推断的时间周期  \nbot: 请问要安排在哪几天  \nuser: 每周二和周三  \nbot: keep为您在每周二、周三安排训练  \n\n\n以上的mention主要将时间根据时间点和时间段进行一个大的分类，在时间点内根据时间颗粒度的不同又划分为日期和每日的时间。在这三个基本面上，根据时间的周期特性又可以延伸出周期时间点和周期时间段，根据人们日常表达中的口语特性又可以延伸出推测时间点和推测时间段。所以整体的mention类型划分如下：\n\n1. 准确时间mention：一天内时间（time）、日期（date）和时间段（duration）以及日期时间段\n2. 周期时间mention：每天的时间周期（cycle_time）、日期的周期（cycle_date）和时间段的周期（cycle_duration）\n3. 推测时间mention：准确时间点+时间长度的（shift_time）、准确日期点+日期长度的（shift_date）和准确时间段+时间长度的（shift_duration）\n4. 推测周期时间mention：每天的时间周期+时间长度的（cycle_shift_time）、日期的周期+日期长度的（cycle_shift_date）和时间段的周期+时间长度的（cycle_shift_duration）\n\n\n### 设计遐想\n\n在明确了mention分类后，其实已经有了整体的mention框架，mention的第一步无疑是进行时间实体的识别，识别的归类是date、time和duration，除了识别时间外还要识别。实体识别后要进行一步归一化处理，抽取出标准的表达，例如“2021年5月6日”识别为一个准确date的实体，具体归一化为一个标准的表达“2021年5月6号”。“明天”则识别为推测时间，在归一化时需要取得当前日期，在当前日期上进行时长运算得到标准的时间表达“2021年5月7日”。对于周期性的时间表达，如每周六等系统默认给出一个范围内的所有周六的计算方式。\n\n### TODO\n这是一个初步的设计的思路，后续有空补详细代码设计和代码实现。\n\n\n","slug":"time-mention","published":1,"updated":"2022-06-09T12:41:31.190Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga77000oq4ux8dnb8802","content":"<p>任务型对话中会话的推进主要依靠意图的识别和实体信息的收集。而实体识别目前在工业落地主要使用规则模板的方式来保证准确率，而且实体类别众多，每一类实体的mention要考虑的情况特别多，实现起来相对复杂，因此具有有很大的提升空间。<br><span id=\"more\"></span></p>\n<h3 id=\"实体识别和mention的关系\"><a href=\"#实体识别和mention的关系\" class=\"headerlink\" title=\"实体识别和mention的关系\"></a>实体识别和mention的关系</h3><p>实体识别应用比较多的主要是关系抽取和对话理解。关系抽取主要是将抽取到的实体和关系进行匹配，所以主要对实体识别的准确率要求较高。而任务型对话除了要求高精度的实体识别外还要进行实体的mention。什么是实体mention呢？我自己的理解就是，比如用户访问：”我要明天到北京的机票“。这里的”明天“要被是被成一个时间的实体，这个实体的mention是具体的那一天比如明天是2021年5月12号，这里明天对应的mention就是2021年5月12号，这样对话机器人拿到这个实体的信息才是有意义的，它才可以通过查询拿到用户需要的信息并返回给用户。</p>\n<h3 id=\"时间实体识别的分类\"><a href=\"#时间实体识别的分类\" class=\"headerlink\" title=\"时间实体识别的分类\"></a>时间实体识别的分类</h3><p>目前主流的对于时间实体识别的分类主要分为date、time和duration。</p>\n<ol>\n<li>date是日期，例如2021年5月1日</li>\n<li>time是时间，例如12点58分</li>\n<li>duration是一个时间单位的大小，比如5年、半个月、2两小时、十秒等</li>\n</ol>\n<h3 id=\"时间实体mention的挑战\"><a href=\"#时间实体mention的挑战\" class=\"headerlink\" title=\"时间实体mention的挑战\"></a>时间实体mention的挑战</h3><ol>\n<li>时间分类的多样性，比如时间点、时间段、时间周期</li>\n<li>时间的规则多样性，比如2月闰年29天，平年28天，节假日的规定不固定等，导致时间的校验困难</li>\n<li>时间的颗粒度控制，比如用户关系的是小时范围发生的事还是一天范围发生的事</li>\n<li>时间表达的多样性，比如3点15分可以表达为3点一刻，还比如昨天，过去两个小时等</li>\n</ol>\n<h3 id=\"时间实体mention的范围\"><a href=\"#时间实体mention的范围\" class=\"headerlink\" title=\"时间实体mention的范围\"></a>时间实体mention的范围</h3><p>mention的划分根据不同的维度有不同的分类，由于我们的场景的任务型问答场景，所以主要针对真实场景中遇到问题进行mention的划分，为后一步的mention抽取设计做好准备。下面可以举几个实际业务中的例子进行直观感受一下。</p>\n<p>(1) 举例1：准确的时间描述<br>user：我想预定今天下午2点的《功夫熊猫》<br>bot： 好的，已为您预定2021年5月6号14:00:00的《功夫熊猫》  </p>\n<p>(2) 举例2：准确的日期描述<br>bot: 请问什么时候发货<br>user: 后天<br>bot: 好的，发货日期为2021年5月8号  </p>\n<p>(3) 举例3：准确的时间段描述<br>bot: 请问预定什么时间段呢？<br>user: 今天上午10点到11点半<br>bot: 已为您预定2021年5月6号10:00:00-2021年5月6号11:30:00的会议室  </p>\n<p>(4) 举例4：准确日期段的描述<br>bot: 请问举办日期段是？<br>user: 2021年5月6号到2021年5月12号<br>bot: 已为您预定2021年5月6号-2021年5月12号的举办场地  </p>\n<p>(5) 举例5：通过推断的时间描述<br>user: 15分钟后提醒我开会<br>bot: 好的，已为您设置2021年5月5号14:15:00开会的闹钟  </p>\n<p>(6) 举例6：通过推断的日期描述<br>bot: 请问什么时候送达<br>user: 母情节那天<br>bot: 好的，我们将在2021年5月8号为您送达。  </p>\n<p>(7) 举例7：通过推断的时间段描述<br>bot：请问你需要订阅多久<br>user：预定一个月<br>bot: 好的，已为您开通2021年5月6号00:00:00-2021年6月6号00:00:00的超级会员  </p>\n<p>(8) 举例8：通过推断的时间周期<br>bot: 请问要例会安排在什么时候<br>user: 每天早上10点<br>bot: 已为您安排每天上午10:00:00的例会  </p>\n<p>(9) 举例9：通过推断的时间周期<br>bot: 请问要安排在哪几天<br>user: 每周二和周三<br>bot: keep为您在每周二、周三安排训练  </p>\n<p>以上的mention主要将时间根据时间点和时间段进行一个大的分类，在时间点内根据时间颗粒度的不同又划分为日期和每日的时间。在这三个基本面上，根据时间的周期特性又可以延伸出周期时间点和周期时间段，根据人们日常表达中的口语特性又可以延伸出推测时间点和推测时间段。所以整体的mention类型划分如下：</p>\n<ol>\n<li>准确时间mention：一天内时间（time）、日期（date）和时间段（duration）以及日期时间段</li>\n<li>周期时间mention：每天的时间周期（cycle_time）、日期的周期（cycle_date）和时间段的周期（cycle_duration）</li>\n<li>推测时间mention：准确时间点+时间长度的（shift_time）、准确日期点+日期长度的（shift_date）和准确时间段+时间长度的（shift_duration）</li>\n<li>推测周期时间mention：每天的时间周期+时间长度的（cycle_shift_time）、日期的周期+日期长度的（cycle_shift_date）和时间段的周期+时间长度的（cycle_shift_duration）</li>\n</ol>\n<h3 id=\"设计遐想\"><a href=\"#设计遐想\" class=\"headerlink\" title=\"设计遐想\"></a>设计遐想</h3><p>在明确了mention分类后，其实已经有了整体的mention框架，mention的第一步无疑是进行时间实体的识别，识别的归类是date、time和duration，除了识别时间外还要识别。实体识别后要进行一步归一化处理，抽取出标准的表达，例如“2021年5月6日”识别为一个准确date的实体，具体归一化为一个标准的表达“2021年5月6号”。“明天”则识别为推测时间，在归一化时需要取得当前日期，在当前日期上进行时长运算得到标准的时间表达“2021年5月7日”。对于周期性的时间表达，如每周六等系统默认给出一个范围内的所有周六的计算方式。</p>\n<h3 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h3><p>这是一个初步的设计的思路，后续有空补详细代码设计和代码实现。</p>\n","site":{"data":{}},"excerpt":"<p>任务型对话中会话的推进主要依靠意图的识别和实体信息的收集。而实体识别目前在工业落地主要使用规则模板的方式来保证准确率，而且实体类别众多，每一类实体的mention要考虑的情况特别多，实现起来相对复杂，因此具有有很大的提升空间。<br>","more":"</p>\n<h3 id=\"实体识别和mention的关系\"><a href=\"#实体识别和mention的关系\" class=\"headerlink\" title=\"实体识别和mention的关系\"></a>实体识别和mention的关系</h3><p>实体识别应用比较多的主要是关系抽取和对话理解。关系抽取主要是将抽取到的实体和关系进行匹配，所以主要对实体识别的准确率要求较高。而任务型对话除了要求高精度的实体识别外还要进行实体的mention。什么是实体mention呢？我自己的理解就是，比如用户访问：”我要明天到北京的机票“。这里的”明天“要被是被成一个时间的实体，这个实体的mention是具体的那一天比如明天是2021年5月12号，这里明天对应的mention就是2021年5月12号，这样对话机器人拿到这个实体的信息才是有意义的，它才可以通过查询拿到用户需要的信息并返回给用户。</p>\n<h3 id=\"时间实体识别的分类\"><a href=\"#时间实体识别的分类\" class=\"headerlink\" title=\"时间实体识别的分类\"></a>时间实体识别的分类</h3><p>目前主流的对于时间实体识别的分类主要分为date、time和duration。</p>\n<ol>\n<li>date是日期，例如2021年5月1日</li>\n<li>time是时间，例如12点58分</li>\n<li>duration是一个时间单位的大小，比如5年、半个月、2两小时、十秒等</li>\n</ol>\n<h3 id=\"时间实体mention的挑战\"><a href=\"#时间实体mention的挑战\" class=\"headerlink\" title=\"时间实体mention的挑战\"></a>时间实体mention的挑战</h3><ol>\n<li>时间分类的多样性，比如时间点、时间段、时间周期</li>\n<li>时间的规则多样性，比如2月闰年29天，平年28天，节假日的规定不固定等，导致时间的校验困难</li>\n<li>时间的颗粒度控制，比如用户关系的是小时范围发生的事还是一天范围发生的事</li>\n<li>时间表达的多样性，比如3点15分可以表达为3点一刻，还比如昨天，过去两个小时等</li>\n</ol>\n<h3 id=\"时间实体mention的范围\"><a href=\"#时间实体mention的范围\" class=\"headerlink\" title=\"时间实体mention的范围\"></a>时间实体mention的范围</h3><p>mention的划分根据不同的维度有不同的分类，由于我们的场景的任务型问答场景，所以主要针对真实场景中遇到问题进行mention的划分，为后一步的mention抽取设计做好准备。下面可以举几个实际业务中的例子进行直观感受一下。</p>\n<p>(1) 举例1：准确的时间描述<br>user：我想预定今天下午2点的《功夫熊猫》<br>bot： 好的，已为您预定2021年5月6号14:00:00的《功夫熊猫》  </p>\n<p>(2) 举例2：准确的日期描述<br>bot: 请问什么时候发货<br>user: 后天<br>bot: 好的，发货日期为2021年5月8号  </p>\n<p>(3) 举例3：准确的时间段描述<br>bot: 请问预定什么时间段呢？<br>user: 今天上午10点到11点半<br>bot: 已为您预定2021年5月6号10:00:00-2021年5月6号11:30:00的会议室  </p>\n<p>(4) 举例4：准确日期段的描述<br>bot: 请问举办日期段是？<br>user: 2021年5月6号到2021年5月12号<br>bot: 已为您预定2021年5月6号-2021年5月12号的举办场地  </p>\n<p>(5) 举例5：通过推断的时间描述<br>user: 15分钟后提醒我开会<br>bot: 好的，已为您设置2021年5月5号14:15:00开会的闹钟  </p>\n<p>(6) 举例6：通过推断的日期描述<br>bot: 请问什么时候送达<br>user: 母情节那天<br>bot: 好的，我们将在2021年5月8号为您送达。  </p>\n<p>(7) 举例7：通过推断的时间段描述<br>bot：请问你需要订阅多久<br>user：预定一个月<br>bot: 好的，已为您开通2021年5月6号00:00:00-2021年6月6号00:00:00的超级会员  </p>\n<p>(8) 举例8：通过推断的时间周期<br>bot: 请问要例会安排在什么时候<br>user: 每天早上10点<br>bot: 已为您安排每天上午10:00:00的例会  </p>\n<p>(9) 举例9：通过推断的时间周期<br>bot: 请问要安排在哪几天<br>user: 每周二和周三<br>bot: keep为您在每周二、周三安排训练  </p>\n<p>以上的mention主要将时间根据时间点和时间段进行一个大的分类，在时间点内根据时间颗粒度的不同又划分为日期和每日的时间。在这三个基本面上，根据时间的周期特性又可以延伸出周期时间点和周期时间段，根据人们日常表达中的口语特性又可以延伸出推测时间点和推测时间段。所以整体的mention类型划分如下：</p>\n<ol>\n<li>准确时间mention：一天内时间（time）、日期（date）和时间段（duration）以及日期时间段</li>\n<li>周期时间mention：每天的时间周期（cycle_time）、日期的周期（cycle_date）和时间段的周期（cycle_duration）</li>\n<li>推测时间mention：准确时间点+时间长度的（shift_time）、准确日期点+日期长度的（shift_date）和准确时间段+时间长度的（shift_duration）</li>\n<li>推测周期时间mention：每天的时间周期+时间长度的（cycle_shift_time）、日期的周期+日期长度的（cycle_shift_date）和时间段的周期+时间长度的（cycle_shift_duration）</li>\n</ol>\n<h3 id=\"设计遐想\"><a href=\"#设计遐想\" class=\"headerlink\" title=\"设计遐想\"></a>设计遐想</h3><p>在明确了mention分类后，其实已经有了整体的mention框架，mention的第一步无疑是进行时间实体的识别，识别的归类是date、time和duration，除了识别时间外还要识别。实体识别后要进行一步归一化处理，抽取出标准的表达，例如“2021年5月6日”识别为一个准确date的实体，具体归一化为一个标准的表达“2021年5月6号”。“明天”则识别为推测时间，在归一化时需要取得当前日期，在当前日期上进行时长运算得到标准的时间表达“2021年5月7日”。对于周期性的时间表达，如每周六等系统默认给出一个范围内的所有周六的计算方式。</p>\n<h3 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h3><p>这是一个初步的设计的思路，后续有空补详细代码设计和代码实现。</p>"},{"title":"重温经典Word2vec & GloVe","date":"2021-10-19T08:42:15.000Z","mathjax":true,"_content":"\nWord2vec由Mikolov在2013年提出，GloVe是对Word2vec的改进，它融合了全局统计信息。虽然随着BERT的出现，这种方法已经很少使用，但是其方法仍然值得思考借鉴。\n\n<!--more-->\n\n### 背景\n传统的语言模型在进行自然语言任务的时候首先要将词转化为向量，然后输入到模型中进行分类或者标注任务。如何将词转化为向量呢？比较简单的想法是one-hot编码，但是one-hot有一个缺点是数据太多稀疏，而且语义没有连续性。因为语言之间存在天然的上下文联系，这种关系对于单个词的表示有很大的影响，于是研究者开始想能不能用一个模型将所有的词映射到空间中，使得语义相似的词距离较近，而语义无关的距离疏远呢？于是就有了经典现代静态词向量算法Word2vec[1]和GloVe[2]。\n\n### Word2vec\nWord2vec基于一个自然语言中有意思的假设：一个词的语义可以用它的上下文表示。因此，word2vec可以用两个模型表示，即连续词袋模型（CBOW）和跳元模型（Skip-gram），模型结构如图：\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\">\n\n\n下面分别介绍一下这两模型。\n\n#### 连续词袋模型（CBOW）\n连续词袋模型假设中⼼词是基于其在⽂本序列中的周围上下⽂词⽣成的。例如，在⽂本序列“the”、“man”、“loves”、“his”、“son”中，在“loves”为中⼼词且上下⽂窗⼝为2的情况下，连续词袋模型考虑基于上下⽂词“the”、“man”、“him”、“son”⽣成中⼼词“loves”的条件概率，即：\n\n$$P(``love\"|``the\",``man\",``his\",``son\")$$\n\n\nCBOW模型可以分为输入层、词向量层和输出层。\n1. 在输入层中，假设上下文窗口大小为5，则在目标词$w_t$左右各取两个词作为模型的输入。输入层由四个维度为词表长度$\\mathbb{V}$的独热表示向量构成。\n2. 词向量层将输入层中每个词的独热表示向量经由矩阵$E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$映射到词向量空间：\n$$ \\pmb{v}_{w_i}=E \\pmb{e}_{w_i} $$\n设$$w_i$$对应的词向量即为矩阵$$E$$中相应位置的列向量，$$E$$为由所有向量组成的矩阵。令$$\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$表示$$w_t$$的上下文单词集合。对$$\\mathcal{c}_t$$中所有词向量取平均，就得到了$$w_t$$的上下文表示：\n$$\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m$$\n3. 输出层根据上下文表示对目标词进行预测（分类），令$$E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }$$为隐含层到输出层的权值矩阵，记$$\\pmb{v}_{w_i}^{'}$$为$$E'$$中与$$w_i$$对应的行向量，那么$$w_t$$的概率可以由下式计算：\n$$P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}$$\n\n\n\n在CBOW模型的参数中，矩阵$E$和矩阵$E'$均可作为词向量矩阵，他们分别描述了词表中的词作为条件上下文的或者目标词时的不同性质。\n\n\n\n#### 跳元模型（Skip-gram）\n跳元模型则适用当前词预测其上下文的词，某种程度上讲Skip-gram模型是CBOW的简化，它建立的是词与词之间的共现关系，即$P(w_{t+j}|w_t)$，其中$$j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}$$。以$k=2$为例，跳元模型也分为输入层、词向量层和输出层：\n1. 输入层是当前时刻$w_t$的独热编码，通过矩阵$E$投射到隐含层；\n2. 隐含层向量为$$w_t$$的词向量$$\\pmb{v}_{w_t}=E^T_{w_t}$$;\n3. 根据$$\\pmb{v}_{w_t}$$，输出层利用线性变换矩阵$$E'$$对上下文窗口内的词进行预测：\n$$P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }$$\n其中$$c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$。\n\n\n\n#### 负采样优化\n\n通过优化分类损失对CBOW和Skip-gram模型进行训练，需要估计的参数是$\\theta = \\{E, {E'}\\}$。给定长度为$T$的词序列$w_1 w_2 \\cdots w_T$，CBOW的负对数似然损失函数为：\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)$$\n\n\n式中$$\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}$$\n\nSkip-gram模型的负对数似然损失为：\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t) $$\n\n\n当词表规模较大时，这类模型的训练会受到输出层概率归一化计算效率的影响，负采样[3]技术提供了一种全新的视角，给定词与上下文，最大化两者的共现概率，这样一来，问题就被简化为一个二分类问题，从而避免了大词表上的归一化计算。令$P(D=1|w,c)$表示$c$与$w$共现的概率：\n\n$$P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\n那么，两者不共现的概率为：\n\n$$P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\n在CBOW模型中$$\\{w_t,c\\}$$是正样本，通过对$$w_t$$进行负采样和$$c$$构建负样本，构建二分类loss进行训练；在Skip-gram中，$$\\{w_t,w_{t+j}\\}$$为正样本，负样本取K个不出现在$w_t$上下文窗口内的词。\n\n\n\n### GloVe\n基于神经网络的词向量预训练方法本质上是利用文本中心词与局部上下文的共现信息作为自监督学习信号，另一类用于估计词向量的方法是基于矩阵分解的方法，这里方法主要是在整个预料上进行统计分析，获取全局统计信息“词上下文”共现矩阵，然后利用奇异值分解对矩阵进行降维。GloVe结合矩阵分解的思想来克服word2vec只关注局部特性的缺点。\n\n\nGlove的基本思想是利用词向量对“词-上下文”共现矩阵进行预测，从而实现隐式的矩阵分解。首先构建共现矩阵$M$，其中$M_{w,c}$表示词$w$与上下文$c$在受限窗口大小内的共现次数。GloVe模型在构建M的过程中进一步考虑了$w$和$c$的距离，认为距离远的$(w,c)$对于全局共现次数的贡献比较小，因此采用共现距离进行加权的计算方式：\n\n$$M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}$$\n\n式中，$d_{i}(w,c)$表示第$i$次共现发生时，$w$与$c$之间的距离。构建完共现矩阵$W$之后，就可以使用词向量对$M$中的元素进行回归拟合，具体形式：\n\n$$\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}$$\n\n其中$$\\pmb{v}_w^T$$和$$\\pmb{v}_c^{'}$$是$w$和$c$的向量表示，$b_w$和$b_c$是对应的偏置，对该回归问题进行求解即可获得词和上下文的向量表示。Glove模型的回归损失函数为：\n\n$$\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2$$\n\n\n其中，$f(M_{w,c})$表示每一个$(w,c)$样本的权重。样本权重与其共现次数相关，共现次数很少的样本通常被认为含有较大的噪声，所蕴含的有用信息相对频繁共现的样本也更少，因此给与较低的权重，同时也会避免给与过高的权重，因此Glove采用以下分段函数进行加权：\n\n$$f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}$$\n\n\n当$$M_{w,c}$$不超过阈值$$\\theta$$时，$$f(W_{w,c})$$的值随$$M_{w,c}$$递增且小等1，当超出阈值时恒等1，$$\\alpha$$用来控制增长速率。\n\n\n### 参考文献\n[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.\n[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.","source":"_posts/word2vec.md","raw":"---\ntitle: 重温经典Word2vec & GloVe\ndate: 2021-10-19 16:42:15\ntags:\n- 静态词向量\n- 负采样\ncategories: \n- 预训练语言模型\nmathjax: true\n---\n\nWord2vec由Mikolov在2013年提出，GloVe是对Word2vec的改进，它融合了全局统计信息。虽然随着BERT的出现，这种方法已经很少使用，但是其方法仍然值得思考借鉴。\n\n<!--more-->\n\n### 背景\n传统的语言模型在进行自然语言任务的时候首先要将词转化为向量，然后输入到模型中进行分类或者标注任务。如何将词转化为向量呢？比较简单的想法是one-hot编码，但是one-hot有一个缺点是数据太多稀疏，而且语义没有连续性。因为语言之间存在天然的上下文联系，这种关系对于单个词的表示有很大的影响，于是研究者开始想能不能用一个模型将所有的词映射到空间中，使得语义相似的词距离较近，而语义无关的距离疏远呢？于是就有了经典现代静态词向量算法Word2vec[1]和GloVe[2]。\n\n### Word2vec\nWord2vec基于一个自然语言中有意思的假设：一个词的语义可以用它的上下文表示。因此，word2vec可以用两个模型表示，即连续词袋模型（CBOW）和跳元模型（Skip-gram），模型结构如图：\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\">\n\n\n下面分别介绍一下这两模型。\n\n#### 连续词袋模型（CBOW）\n连续词袋模型假设中⼼词是基于其在⽂本序列中的周围上下⽂词⽣成的。例如，在⽂本序列“the”、“man”、“loves”、“his”、“son”中，在“loves”为中⼼词且上下⽂窗⼝为2的情况下，连续词袋模型考虑基于上下⽂词“the”、“man”、“him”、“son”⽣成中⼼词“loves”的条件概率，即：\n\n$$P(``love\"|``the\",``man\",``his\",``son\")$$\n\n\nCBOW模型可以分为输入层、词向量层和输出层。\n1. 在输入层中，假设上下文窗口大小为5，则在目标词$w_t$左右各取两个词作为模型的输入。输入层由四个维度为词表长度$\\mathbb{V}$的独热表示向量构成。\n2. 词向量层将输入层中每个词的独热表示向量经由矩阵$E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$映射到词向量空间：\n$$ \\pmb{v}_{w_i}=E \\pmb{e}_{w_i} $$\n设$$w_i$$对应的词向量即为矩阵$$E$$中相应位置的列向量，$$E$$为由所有向量组成的矩阵。令$$\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$表示$$w_t$$的上下文单词集合。对$$\\mathcal{c}_t$$中所有词向量取平均，就得到了$$w_t$$的上下文表示：\n$$\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m$$\n3. 输出层根据上下文表示对目标词进行预测（分类），令$$E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }$$为隐含层到输出层的权值矩阵，记$$\\pmb{v}_{w_i}^{'}$$为$$E'$$中与$$w_i$$对应的行向量，那么$$w_t$$的概率可以由下式计算：\n$$P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}$$\n\n\n\n在CBOW模型的参数中，矩阵$E$和矩阵$E'$均可作为词向量矩阵，他们分别描述了词表中的词作为条件上下文的或者目标词时的不同性质。\n\n\n\n#### 跳元模型（Skip-gram）\n跳元模型则适用当前词预测其上下文的词，某种程度上讲Skip-gram模型是CBOW的简化，它建立的是词与词之间的共现关系，即$P(w_{t+j}|w_t)$，其中$$j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}$$。以$k=2$为例，跳元模型也分为输入层、词向量层和输出层：\n1. 输入层是当前时刻$w_t$的独热编码，通过矩阵$E$投射到隐含层；\n2. 隐含层向量为$$w_t$$的词向量$$\\pmb{v}_{w_t}=E^T_{w_t}$$;\n3. 根据$$\\pmb{v}_{w_t}$$，输出层利用线性变换矩阵$$E'$$对上下文窗口内的词进行预测：\n$$P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }$$\n其中$$c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}$$。\n\n\n\n#### 负采样优化\n\n通过优化分类损失对CBOW和Skip-gram模型进行训练，需要估计的参数是$\\theta = \\{E, {E'}\\}$。给定长度为$T$的词序列$w_1 w_2 \\cdots w_T$，CBOW的负对数似然损失函数为：\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)$$\n\n\n式中$$\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}$$\n\nSkip-gram模型的负对数似然损失为：\n\n$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t) $$\n\n\n当词表规模较大时，这类模型的训练会受到输出层概率归一化计算效率的影响，负采样[3]技术提供了一种全新的视角，给定词与上下文，最大化两者的共现概率，这样一来，问题就被简化为一个二分类问题，从而避免了大词表上的归一化计算。令$P(D=1|w,c)$表示$c$与$w$共现的概率：\n\n$$P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\n那么，两者不共现的概率为：\n\n$$P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})$$\n\n在CBOW模型中$$\\{w_t,c\\}$$是正样本，通过对$$w_t$$进行负采样和$$c$$构建负样本，构建二分类loss进行训练；在Skip-gram中，$$\\{w_t,w_{t+j}\\}$$为正样本，负样本取K个不出现在$w_t$上下文窗口内的词。\n\n\n\n### GloVe\n基于神经网络的词向量预训练方法本质上是利用文本中心词与局部上下文的共现信息作为自监督学习信号，另一类用于估计词向量的方法是基于矩阵分解的方法，这里方法主要是在整个预料上进行统计分析，获取全局统计信息“词上下文”共现矩阵，然后利用奇异值分解对矩阵进行降维。GloVe结合矩阵分解的思想来克服word2vec只关注局部特性的缺点。\n\n\nGlove的基本思想是利用词向量对“词-上下文”共现矩阵进行预测，从而实现隐式的矩阵分解。首先构建共现矩阵$M$，其中$M_{w,c}$表示词$w$与上下文$c$在受限窗口大小内的共现次数。GloVe模型在构建M的过程中进一步考虑了$w$和$c$的距离，认为距离远的$(w,c)$对于全局共现次数的贡献比较小，因此采用共现距离进行加权的计算方式：\n\n$$M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}$$\n\n式中，$d_{i}(w,c)$表示第$i$次共现发生时，$w$与$c$之间的距离。构建完共现矩阵$W$之后，就可以使用词向量对$M$中的元素进行回归拟合，具体形式：\n\n$$\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}$$\n\n其中$$\\pmb{v}_w^T$$和$$\\pmb{v}_c^{'}$$是$w$和$c$的向量表示，$b_w$和$b_c$是对应的偏置，对该回归问题进行求解即可获得词和上下文的向量表示。Glove模型的回归损失函数为：\n\n$$\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2$$\n\n\n其中，$f(M_{w,c})$表示每一个$(w,c)$样本的权重。样本权重与其共现次数相关，共现次数很少的样本通常被认为含有较大的噪声，所蕴含的有用信息相对频繁共现的样本也更少，因此给与较低的权重，同时也会避免给与过高的权重，因此Glove采用以下分段函数进行加权：\n\n$$f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}$$\n\n\n当$$M_{w,c}$$不超过阈值$$\\theta$$时，$$f(W_{w,c})$$的值随$$M_{w,c}$$递增且小等1，当超出阈值时恒等1，$$\\alpha$$用来控制增长速率。\n\n\n### 参考文献\n[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.\n[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.","slug":"word2vec","published":1,"updated":"2022-06-09T12:39:28.711Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga7a000qq4ux64mg6y51","content":"<p>Word2vec由Mikolov在2013年提出，GloVe是对Word2vec的改进，它融合了全局统计信息。虽然随着BERT的出现，这种方法已经很少使用，但是其方法仍然值得思考借鉴。</p>\n<span id=\"more\"></span>\n<h3 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h3><p>传统的语言模型在进行自然语言任务的时候首先要将词转化为向量，然后输入到模型中进行分类或者标注任务。如何将词转化为向量呢？比较简单的想法是one-hot编码，但是one-hot有一个缺点是数据太多稀疏，而且语义没有连续性。因为语言之间存在天然的上下文联系，这种关系对于单个词的表示有很大的影响，于是研究者开始想能不能用一个模型将所有的词映射到空间中，使得语义相似的词距离较近，而语义无关的距离疏远呢？于是就有了经典现代静态词向量算法Word2vec[1]和GloVe[2]。</p>\n<h3 id=\"Word2vec\"><a href=\"#Word2vec\" class=\"headerlink\" title=\"Word2vec\"></a>Word2vec</h3><p>Word2vec基于一个自然语言中有意思的假设：一个词的语义可以用它的上下文表示。因此，word2vec可以用两个模型表示，即连续词袋模型（CBOW）和跳元模型（Skip-gram），模型结构如图：<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\"></p>\n<p>下面分别介绍一下这两模型。</p>\n<h4 id=\"连续词袋模型（CBOW）\"><a href=\"#连续词袋模型（CBOW）\" class=\"headerlink\" title=\"连续词袋模型（CBOW）\"></a>连续词袋模型（CBOW）</h4><p>连续词袋模型假设中⼼词是基于其在⽂本序列中的周围上下⽂词⽣成的。例如，在⽂本序列“the”、“man”、“loves”、“his”、“son”中，在“loves”为中⼼词且上下⽂窗⼝为2的情况下，连续词袋模型考虑基于上下⽂词“the”、“man”、“him”、“son”⽣成中⼼词“loves”的条件概率，即：</p>\n<script type=\"math/tex; mode=display\">P(``love\"|``the\",``man\",``his\",``son\")</script><p>CBOW模型可以分为输入层、词向量层和输出层。</p>\n<ol>\n<li>在输入层中，假设上下文窗口大小为5，则在目标词$w_t$左右各取两个词作为模型的输入。输入层由四个维度为词表长度$\\mathbb{V}$的独热表示向量构成。</li>\n<li>词向量层将输入层中每个词的独热表示向量经由矩阵$E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$映射到词向量空间：<script type=\"math/tex; mode=display\">\\pmb{v}_{w_i}=E \\pmb{e}_{w_i}</script>设<script type=\"math/tex\">w_i</script>对应的词向量即为矩阵<script type=\"math/tex\">E</script>中相应位置的列向量，<script type=\"math/tex\">E</script>为由所有向量组成的矩阵。令<script type=\"math/tex\">\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script>表示<script type=\"math/tex\">w_t</script>的上下文单词集合。对<script type=\"math/tex\">\\mathcal{c}_t</script>中所有词向量取平均，就得到了<script type=\"math/tex\">w_t</script>的上下文表示：<script type=\"math/tex; mode=display\">\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m</script></li>\n<li>输出层根据上下文表示对目标词进行预测（分类），令<script type=\"math/tex\">E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }</script>为隐含层到输出层的权值矩阵，记<script type=\"math/tex\">\\pmb{v}_{w_i}^{'}</script>为<script type=\"math/tex\">E'</script>中与<script type=\"math/tex\">w_i</script>对应的行向量，那么<script type=\"math/tex\">w_t</script>的概率可以由下式计算：<script type=\"math/tex; mode=display\">P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}</script></li>\n</ol>\n<p>在CBOW模型的参数中，矩阵$E$和矩阵$E’$均可作为词向量矩阵，他们分别描述了词表中的词作为条件上下文的或者目标词时的不同性质。</p>\n<h4 id=\"跳元模型（Skip-gram）\"><a href=\"#跳元模型（Skip-gram）\" class=\"headerlink\" title=\"跳元模型（Skip-gram）\"></a>跳元模型（Skip-gram）</h4><p>跳元模型则适用当前词预测其上下文的词，某种程度上讲Skip-gram模型是CBOW的简化，它建立的是词与词之间的共现关系，即$P(w_{t+j}|w_t)$，其中<script type=\"math/tex\">j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}</script>。以$k=2$为例，跳元模型也分为输入层、词向量层和输出层：</p>\n<ol>\n<li>输入层是当前时刻$w_t$的独热编码，通过矩阵$E$投射到隐含层；</li>\n<li>隐含层向量为<script type=\"math/tex\">w_t</script>的词向量<script type=\"math/tex\">\\pmb{v}_{w_t}=E^T_{w_t}</script>;</li>\n<li>根据<script type=\"math/tex\">\\pmb{v}_{w_t}</script>，输出层利用线性变换矩阵<script type=\"math/tex\">E'</script>对上下文窗口内的词进行预测：<script type=\"math/tex; mode=display\">P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }</script>其中<script type=\"math/tex\">c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script>。</li>\n</ol>\n<h4 id=\"负采样优化\"><a href=\"#负采样优化\" class=\"headerlink\" title=\"负采样优化\"></a>负采样优化</h4><p>通过优化分类损失对CBOW和Skip-gram模型进行训练，需要估计的参数是$\\theta = {E, {E’}}$。给定长度为$T$的词序列$w_1 w_2 \\cdots w_T$，CBOW的负对数似然损失函数为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)</script><p>式中<script type=\"math/tex\">\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}</script></p>\n<p>Skip-gram模型的负对数似然损失为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t)</script><p>当词表规模较大时，这类模型的训练会受到输出层概率归一化计算效率的影响，负采样[3]技术提供了一种全新的视角，给定词与上下文，最大化两者的共现概率，这样一来，问题就被简化为一个二分类问题，从而避免了大词表上的归一化计算。令$P(D=1|w,c)$表示$c$与$w$共现的概率：</p>\n<script type=\"math/tex; mode=display\">P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p>那么，两者不共现的概率为：</p>\n<script type=\"math/tex; mode=display\">P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p>在CBOW模型中<script type=\"math/tex\">\\{w_t,c\\}</script>是正样本，通过对<script type=\"math/tex\">w_t</script>进行负采样和<script type=\"math/tex\">c</script>构建负样本，构建二分类loss进行训练；在Skip-gram中，<script type=\"math/tex\">\\{w_t,w_{t+j}\\}</script>为正样本，负样本取K个不出现在$w_t$上下文窗口内的词。</p>\n<h3 id=\"GloVe\"><a href=\"#GloVe\" class=\"headerlink\" title=\"GloVe\"></a>GloVe</h3><p>基于神经网络的词向量预训练方法本质上是利用文本中心词与局部上下文的共现信息作为自监督学习信号，另一类用于估计词向量的方法是基于矩阵分解的方法，这里方法主要是在整个预料上进行统计分析，获取全局统计信息“词上下文”共现矩阵，然后利用奇异值分解对矩阵进行降维。GloVe结合矩阵分解的思想来克服word2vec只关注局部特性的缺点。</p>\n<p>Glove的基本思想是利用词向量对“词-上下文”共现矩阵进行预测，从而实现隐式的矩阵分解。首先构建共现矩阵$M$，其中$M_{w,c}$表示词$w$与上下文$c$在受限窗口大小内的共现次数。GloVe模型在构建M的过程中进一步考虑了$w$和$c$的距离，认为距离远的$(w,c)$对于全局共现次数的贡献比较小，因此采用共现距离进行加权的计算方式：</p>\n<script type=\"math/tex; mode=display\">M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}</script><p>式中，$d_{i}(w,c)$表示第$i$次共现发生时，$w$与$c$之间的距离。构建完共现矩阵$W$之后，就可以使用词向量对$M$中的元素进行回归拟合，具体形式：</p>\n<script type=\"math/tex; mode=display\">\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}</script><p>其中<script type=\"math/tex\">\\pmb{v}_w^T</script>和<script type=\"math/tex\">\\pmb{v}_c^{'}</script>是$w$和$c$的向量表示，$b_w$和$b_c$是对应的偏置，对该回归问题进行求解即可获得词和上下文的向量表示。Glove模型的回归损失函数为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2</script><p>其中，$f(M_{w,c})$表示每一个$(w,c)$样本的权重。样本权重与其共现次数相关，共现次数很少的样本通常被认为含有较大的噪声，所蕴含的有用信息相对频繁共现的样本也更少，因此给与较低的权重，同时也会避免给与过高的权重，因此Glove采用以下分段函数进行加权：</p>\n<script type=\"math/tex; mode=display\">f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}</script><p>当<script type=\"math/tex\">M_{w,c}</script>不超过阈值<script type=\"math/tex\">\\theta</script>时，<script type=\"math/tex\">f(W_{w,c})</script>的值随<script type=\"math/tex\">M_{w,c}</script>递增且小等1，当超出阈值时恒等1，<script type=\"math/tex\">\\alpha</script>用来控制增长速率。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.<br>[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.<br>[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.</p>\n","site":{"data":{}},"excerpt":"<p>Word2vec由Mikolov在2013年提出，GloVe是对Word2vec的改进，它融合了全局统计信息。虽然随着BERT的出现，这种方法已经很少使用，但是其方法仍然值得思考借鉴。</p>","more":"<h3 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h3><p>传统的语言模型在进行自然语言任务的时候首先要将词转化为向量，然后输入到模型中进行分类或者标注任务。如何将词转化为向量呢？比较简单的想法是one-hot编码，但是one-hot有一个缺点是数据太多稀疏，而且语义没有连续性。因为语言之间存在天然的上下文联系，这种关系对于单个词的表示有很大的影响，于是研究者开始想能不能用一个模型将所有的词映射到空间中，使得语义相似的词距离较近，而语义无关的距离疏远呢？于是就有了经典现代静态词向量算法Word2vec[1]和GloVe[2]。</p>\n<h3 id=\"Word2vec\"><a href=\"#Word2vec\" class=\"headerlink\" title=\"Word2vec\"></a>Word2vec</h3><p>Word2vec基于一个自然语言中有意思的假设：一个词的语义可以用它的上下文表示。因此，word2vec可以用两个模型表示，即连续词袋模型（CBOW）和跳元模型（Skip-gram），模型结构如图：<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/word2vec.png\" width=\"60%\" height=\"40%\"></p>\n<p>下面分别介绍一下这两模型。</p>\n<h4 id=\"连续词袋模型（CBOW）\"><a href=\"#连续词袋模型（CBOW）\" class=\"headerlink\" title=\"连续词袋模型（CBOW）\"></a>连续词袋模型（CBOW）</h4><p>连续词袋模型假设中⼼词是基于其在⽂本序列中的周围上下⽂词⽣成的。例如，在⽂本序列“the”、“man”、“loves”、“his”、“son”中，在“loves”为中⼼词且上下⽂窗⼝为2的情况下，连续词袋模型考虑基于上下⽂词“the”、“man”、“him”、“son”⽣成中⼼词“loves”的条件概率，即：</p>\n<script type=\"math/tex; mode=display\">P(``love\"|``the\",``man\",``his\",``son\")</script><p>CBOW模型可以分为输入层、词向量层和输出层。</p>\n<ol>\n<li>在输入层中，假设上下文窗口大小为5，则在目标词$w_t$左右各取两个词作为模型的输入。输入层由四个维度为词表长度$\\mathbb{V}$的独热表示向量构成。</li>\n<li>词向量层将输入层中每个词的独热表示向量经由矩阵$E\\in \\mathbb{R}^{d \\times |\\mathbb{V}| }$映射到词向量空间：<script type=\"math/tex; mode=display\">\\pmb{v}_{w_i}=E \\pmb{e}_{w_i}</script>设<script type=\"math/tex\">w_i</script>对应的词向量即为矩阵<script type=\"math/tex\">E</script>中相应位置的列向量，<script type=\"math/tex\">E</script>为由所有向量组成的矩阵。令<script type=\"math/tex\">\\mathcal{c}_t = \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script>表示<script type=\"math/tex\">w_t</script>的上下文单词集合。对<script type=\"math/tex\">\\mathcal{c}_t</script>中所有词向量取平均，就得到了<script type=\"math/tex\">w_t</script>的上下文表示：<script type=\"math/tex; mode=display\">\\pmb{v}_{\\mathcal{C}_t} = \\frac{1}{ |\\mathcal{C}_t| } \\sum_{w\\in \\mathcal{C}_t} \\pmb{v}_m</script></li>\n<li>输出层根据上下文表示对目标词进行预测（分类），令<script type=\"math/tex\">E' \\in \\mathbb{R}^{ |\\mathbb{V} \\times d | }</script>为隐含层到输出层的权值矩阵，记<script type=\"math/tex\">\\pmb{v}_{w_i}^{'}</script>为<script type=\"math/tex\">E'</script>中与<script type=\"math/tex\">w_i</script>对应的行向量，那么<script type=\"math/tex\">w_t</script>的概率可以由下式计算：<script type=\"math/tex; mode=display\">P(w_t|\\mathcal{C}_t) = \\frac{exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w_t}^{'})}{\\sum_{w' \\in \\mathbb{V}}exp(\\pmb{v}_{\\mathcal{C}_t} \\cdot \\pmb{v}_{w'}^{'})}</script></li>\n</ol>\n<p>在CBOW模型的参数中，矩阵$E$和矩阵$E’$均可作为词向量矩阵，他们分别描述了词表中的词作为条件上下文的或者目标词时的不同性质。</p>\n<h4 id=\"跳元模型（Skip-gram）\"><a href=\"#跳元模型（Skip-gram）\" class=\"headerlink\" title=\"跳元模型（Skip-gram）\"></a>跳元模型（Skip-gram）</h4><p>跳元模型则适用当前词预测其上下文的词，某种程度上讲Skip-gram模型是CBOW的简化，它建立的是词与词之间的共现关系，即$P(w_{t+j}|w_t)$，其中<script type=\"math/tex\">j\\in \\{\\pm{1}, \\cdots, \\pm{k}\\}</script>。以$k=2$为例，跳元模型也分为输入层、词向量层和输出层：</p>\n<ol>\n<li>输入层是当前时刻$w_t$的独热编码，通过矩阵$E$投射到隐含层；</li>\n<li>隐含层向量为<script type=\"math/tex\">w_t</script>的词向量<script type=\"math/tex\">\\pmb{v}_{w_t}=E^T_{w_t}</script>;</li>\n<li>根据<script type=\"math/tex\">\\pmb{v}_{w_t}</script>，输出层利用线性变换矩阵<script type=\"math/tex\">E'</script>对上下文窗口内的词进行预测：<script type=\"math/tex; mode=display\">P(c|w_t) = \\frac{exp(\\pmb{v}_{w_t} \\cdot \\pmb{v}^{'}_c)}{\\sum_{w' \\in \\mathbb{V}}{exp(\\pmb{v}_{w_t}\\cdot \\pmb{v}^{'}_{w'})} }</script>其中<script type=\"math/tex\">c\\in \\{ w_{t-k}, \\cdots, w_{t-1}, w_{t+1},  \\cdots, w_{t+k} \\}</script>。</li>\n</ol>\n<h4 id=\"负采样优化\"><a href=\"#负采样优化\" class=\"headerlink\" title=\"负采样优化\"></a>负采样优化</h4><p>通过优化分类损失对CBOW和Skip-gram模型进行训练，需要估计的参数是$\\theta = {E, {E’}}$。给定长度为$T$的词序列$w_1 w_2 \\cdots w_T$，CBOW的负对数似然损失函数为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T logP(w_t|\\mathcal{C}_t)</script><p>式中<script type=\"math/tex\">\\mathcal{C}_t=\\{w_{t-k}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+k}\\}</script></p>\n<p>Skip-gram模型的负对数似然损失为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}(\\theta) = -\\sum_{t=1}^T\\sum_{-k\\leq j\\leq k,j \\neq 0} logP(w_{t+j}|w_t)</script><p>当词表规模较大时，这类模型的训练会受到输出层概率归一化计算效率的影响，负采样[3]技术提供了一种全新的视角，给定词与上下文，最大化两者的共现概率，这样一来，问题就被简化为一个二分类问题，从而避免了大词表上的归一化计算。令$P(D=1|w,c)$表示$c$与$w$共现的概率：</p>\n<script type=\"math/tex; mode=display\">P(D=1|w,c) = \\sigma(\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p>那么，两者不共现的概率为：</p>\n<script type=\"math/tex; mode=display\">P(D=0|w,c) = 1- P(D|w,c) = \\sigma(-\\pmb{v}_w \\cdot \\pmb{v}_{c}^{'})</script><p>在CBOW模型中<script type=\"math/tex\">\\{w_t,c\\}</script>是正样本，通过对<script type=\"math/tex\">w_t</script>进行负采样和<script type=\"math/tex\">c</script>构建负样本，构建二分类loss进行训练；在Skip-gram中，<script type=\"math/tex\">\\{w_t,w_{t+j}\\}</script>为正样本，负样本取K个不出现在$w_t$上下文窗口内的词。</p>\n<h3 id=\"GloVe\"><a href=\"#GloVe\" class=\"headerlink\" title=\"GloVe\"></a>GloVe</h3><p>基于神经网络的词向量预训练方法本质上是利用文本中心词与局部上下文的共现信息作为自监督学习信号，另一类用于估计词向量的方法是基于矩阵分解的方法，这里方法主要是在整个预料上进行统计分析，获取全局统计信息“词上下文”共现矩阵，然后利用奇异值分解对矩阵进行降维。GloVe结合矩阵分解的思想来克服word2vec只关注局部特性的缺点。</p>\n<p>Glove的基本思想是利用词向量对“词-上下文”共现矩阵进行预测，从而实现隐式的矩阵分解。首先构建共现矩阵$M$，其中$M_{w,c}$表示词$w$与上下文$c$在受限窗口大小内的共现次数。GloVe模型在构建M的过程中进一步考虑了$w$和$c$的距离，认为距离远的$(w,c)$对于全局共现次数的贡献比较小，因此采用共现距离进行加权的计算方式：</p>\n<script type=\"math/tex; mode=display\">M_{w,c} = \\sum_{i}\\frac{1}{d_{i}(w,c)}</script><p>式中，$d_{i}(w,c)$表示第$i$次共现发生时，$w$与$c$之间的距离。构建完共现矩阵$W$之后，就可以使用词向量对$M$中的元素进行回归拟合，具体形式：</p>\n<script type=\"math/tex; mode=display\">\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} = logM_{w,c}</script><p>其中<script type=\"math/tex\">\\pmb{v}_w^T</script>和<script type=\"math/tex\">\\pmb{v}_c^{'}</script>是$w$和$c$的向量表示，$b_w$和$b_c$是对应的偏置，对该回归问题进行求解即可获得词和上下文的向量表示。Glove模型的回归损失函数为：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}({E,E',b,b'};M) = \\sum_{(w,c)\\in \\mathbb{D}} f(M_{w,c})(\\pmb{v}_w^T \\pmb{v}_c^{'} + b_w + b_c^{'} - logM_{w,c})^2</script><p>其中，$f(M_{w,c})$表示每一个$(w,c)$样本的权重。样本权重与其共现次数相关，共现次数很少的样本通常被认为含有较大的噪声，所蕴含的有用信息相对频繁共现的样本也更少，因此给与较低的权重，同时也会避免给与过高的权重，因此Glove采用以下分段函数进行加权：</p>\n<script type=\"math/tex; mode=display\">f(M_{w,c}) = \\begin{cases}\n(M_{w,c}/\\theta)^{\\alpha}, & M_{w,c} \\leq \\theta \\\\\n1, & others\n\\end{cases}</script><p>当<script type=\"math/tex\">M_{w,c}</script>不超过阈值<script type=\"math/tex\">\\theta</script>时，<script type=\"math/tex\">f(W_{w,c})</script>的值随<script type=\"math/tex\">M_{w,c}</script>递增且小等1，当超出阈值时恒等1，<script type=\"math/tex\">\\alpha</script>用来控制增长速率。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.<br>[2] Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.<br>[3] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.</p>"},{"title":"BERT 知多少","date":"2021-03-02T08:40:29.000Z","mathjax":true,"_content":"\nBERT已经提出三四年了，但是提到NLP首先想到的还是Transformer[1]、BERT[2]。像经典的CNN、RNN一样BERT也可以说Transformer也一定在会在NLP和人工智能的舞台上留下浓墨重彩的一笔，如此经典的神经网络模型，当然值得不断深入的思考和学习啦！\n<!--more-->\n\n\n### Transformer和CNN、RNN的区别是什么，本质是什么？\n这三个模型其实是代表了三种典型的神经网络。RNN代表的是循环神经网络，CNN代表的是卷积神经网络，而transformer代表的是注意力机制下的神经网络。所以要理解他们之间的区别就要了解这三种神经网络的特性。理解这三种神经网络的特点首先要了解神经网络的发展过程。\n\n\n最初的神经网络发展自单层感知机，由于单层感知机只是将特征加权后输出，所以不具备处理非线性问题的能力，也导致神经网络发展停滞，直到出现多层神经网络加激活函数形式的多层感知机，神经网络才再度出现在人们的视野。研究人员开始研究多层感知机的优化，首先是各种各样的激活函数的出现，随着计算机硬件能力的不断提高，通过加深网络，和提高参数量，神经网络获得了很大的进展。但是，多层感知机的全连接网络由于设计简单，对于所有的特征提取没有区分度，所以在处理复杂的数据例如图像数据、序列数据时就会显得力不从心。随着计算机视觉和自然语言处理的发展，深度学习和对应的领域进行融合，于是发展出了专为图像数据而设计的卷积神经网络CNN以及专为文本序列处理而设计的循环卷积网络RNN。在这两种模式的神经网络取得成功后，研究人员迅速改进了RNN和CNN发展出了基于这两种神经网络的各种现代网络，但是核心思想是不变的。之后，研究人员还在神经网络中引入了注意力机制。而Transformer的出现是神经网络发展中另一个具有里程碑意义的模型结构。Transformer完全抛弃了CNN和RNN，采用多头注意力机制（multi-head attention）和自注意力（self-attention），Transformer的大获成功在很大程度上诠释了基于自注意力机制神经网络的超强能力。下面就深入细节，具体感受一下各种神经网络的不同。\n\n\n#### (1) 卷积神经网络CNN[3]\n卷积神经网络的核心思想在于它考虑了数据的局部空间特性，以及数据的平移不变性，在此基础上进行特征提取。因此，CNN引入了一个重要的概念“卷积”，在网络上结构中主要体现在引入了卷积层，卷积层的权重就是卷积核。引入卷积层的神经网络较多层感知机可以极大地减少参数。下图是一个典型卷积神经网络的代表。可以看到网络中主要包括卷积层和汇聚层也称池化层，最后的输出是一个全连接层。\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\">\n\n\n要想深入地了解CNN，就需要深入了解卷积层的操作，下面来介绍卷积相关的概念和操作。首先了解什么卷积？在数学中，两个函数（$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $）之间的卷积被定义为：\n\n$$ (f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)$$\n\n也就是说，卷积是把一个函数“翻转”并移位$ \\bf{x} $时，测量$f$和$g$之前的重叠。当为离散对象时，积分就变成求和：\n\n$$ (f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2) $$\n\n对于二维张量，则$f$的索引为$(a,b)$和$g$的索引$(i-a, j-b)$上的对应加和:\n\n$$ (f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3) $$\n\n严格来说，卷积层做的操作不是卷积运算而是互相关运算，卷积层对输⼊和卷积核权重进⾏互相关运算，并在添加标量偏置之后产⽣输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。就像我们之前随机初始化全连接层⼀样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。\n\n\n影响卷积输出的大小主要是填充（padding）和步幅（stride）。⽐如，⼀个240 × 240像素的图像，经过10层5 × 5的卷积后，将减少到200 × 200像素。如此⼀来，原始图像的边界丢失了许多有⽤信息。⽽填充是解决此问题最有效的⽅法。有时，我们可能希望⼤幅降低图像的宽度和⾼度。例如，如果我们发现原始的输⼊分辨率⼗分冗余。步幅则可以在这类情况下提供帮助。卷积神经网络中还有一个重要的概念是通道，通道的作用我理解就是不同的特征提取器，拿图像举例，图像中一个像素点需要三个维度的数据来表示（RGB），那么其中每一个维度就需要一个通道，一个通道就对应一个卷积层。当然，随着CNN的发展也出现了各种各样的卷积操作，其中包括转置(Transposed)卷积，可分离卷积，扩张/空洞(Dilated/Atrous)卷积以及可变形(Deformable)卷积等。\n\n\n除了最核心的卷积层外，CNN还有一个关键的网络层汇聚层，一般的神经网络在每一次卷积操作之后都会接一个汇聚层，对前一层提取的特征进行采样。因为卷积层更多的关注空间的局部特征，所以加入汇聚层可以更好的捕捉空间的整体特性。汇聚层主要有最⼤汇聚层和平均汇聚层等。\n\n\n比较有代表性的卷积神经网络有LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet以及DenseNet等。\n\n#### (2) 循环神经网络RNN[3]\n如果说卷积神经⽹络可以有效地处理空间信息，那么循环神经⽹络则可以更好地处理序列信息。循环神经⽹络通过引⼊状态变量存储过去的信息和当前的输⼊，从⽽可以确定当前的输出。循环神经网络的设计启发来自于序列模型，在文本处理中常使用n元语法模型，但是n元语法模型中单词$x_t$在时间步$t$的条件概率仅取决于前$n-1$个单词，对于更长序列的捕捉，需要更大的$n$,然⽽模型参数的数量也会随之呈指数增⻓。所以不如含使用隐变量的模型：\n$$ P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4) $$\n\n\n其中$$h_{t-1}$$是隐状态，也称为隐变量，它存储了到时间步$$t-1$$的序列信息。通常，我们可以基于当前输入$$x_t$$和先前的隐状态$$h_{t-1}$$来计算时间步$$t$$处的任何时间的隐状态：\n$$ h_{t} = f(x_t, h_{t-1}) \\qquad (5) $$\n\n\n循环神经⽹络RNN是具有隐状态的神经⽹络，当然包括隐状态的模型还有隐马尔可夫连、条件随机场等。下面来感受一下含有隐状态的神经网络的建模过程：\n假设在时间步$t$有⼩批量输⼊$$X_t \\in \\mathbb{R}^{n \\times d}$$，用$$H_t \\in \\mathbb{R}^{n \\times h}$$表示时间步$t$的隐变量。当前步保存前一个时间步的隐变量$$H_{t-1}$$，引入新的权重参数$$W_{hh} \\in \\mathbb{R}^{h \\times n}$$，来描述如何在当前时间步中使用前一个时间步的隐变量。即当前时间隐藏变量由当前时间步的输入与前一个时间步的隐变量一起计算得出：\n\n\n$$H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6) $$\n\n由于在当前时间步中，隐状态使⽤的定义与前⼀个时间步中使⽤的定义相同，因此（5）的计算是循环的。于是基于循环计算的隐状态神经⽹络被命名为循环神经⽹络。对于时间步$t$，输出层的输出类似于多层感知机中的计算：\n\n$$O_t = H_t W_{hq} + b_q \\qquad (7) $$\n\n\n循环神经⽹络的参数包括隐藏层的权重$$W_{xh} \\in \\mathbb{R}^{d \\times h}$$，$$W_{hh} \\in \\mathbb{R}^{h \\times h}$$和偏置$$b_h \\in \\mathbb{R}^{1 \\times h}$$，以及输出层的权重$$W_{qh} \\in \\mathbb{R}^{q \\times h}$$和偏置$$b_q \\in \\mathbb{R}^{1 \\times q}$$。值得⼀提的是，即使在不同的时间步，循环神经⽹络也总是使⽤这些模型参数。因此，循环神经⽹络的参数开销不会随着时间步的增加⽽增加。下图是一个RNN的网络层结构：\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\">\n\n\n随着对RNN的深入研究，研究员发明了控制循环门单元、长短记忆网络以及双向RNN等来改善朴素的循环神经⽹络，也使得现代循环神经网络变得越来越复杂。\n\n\n#### (3) 注意力机制和Transformer\n我不知道注意力机制最初的设计灵感来源于哪里，但是看到有关注意力的一段描述，我觉得可以很好的解释为什么我们需要注意力机制。它的原文是这样的：\n> 注意⼒是稀缺的，⽽环境中的⼲扰注意⼒的信息却并不少。⽐如我们的视觉神经系统⼤约每秒收到108位的信息，这远远超过了⼤脑能够完全处理的⽔平。幸运的是，我们的祖先已经从经验（也称为数据）中认识到“并⾮感官的所有输⼊都是⼀样的”。在整个⼈类历史中，这种只将注意⼒引向感兴趣的⼀⼩部分信息的能⼒，使我们的⼤脑能够更明智地分配资源来⽣存、成⻓和社交，例如发现天敌、找寻⻝物和伴侣。[3]\n\n通过学习人类的注意力，我们把注意力分为自主性注意力和非自主性注意力。对于简单的感官传入到我们的大脑的刺激是非自主性注意力，对于我们主动选择要查询搜索的刺激是自主性注意力。那么如何用神经网络来设计注意力机制的框架呢？\n\n\n对于非自主性注意力，可以简单地使⽤参数化的全连接层，甚⾄是⾮参数化的最⼤汇聚层或平均汇聚层。因此，“是否包含⾃主性提⽰”将注意⼒机制与全连接层或汇聚层区别开来。在注意⼒机制的背景下，我们将⾃主性提⽰称为查询（query），非自主查询称为键（key）。注意力机制的设计如下图所示，它展示了一个查询（自助查询）和所有键（非自主查询）通过注意力汇聚产生的结果和感觉输入：值（value）结合作为中间特征输入到神经网络中的过程。\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\">\n\n实际应用中，假设有一个查询$\\pmb{q} \\in \\mathbb{R}^q$和$m$个“键-值”对$$(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k$$，其中$$\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v$$，注意力汇聚函数$f$通常被表⽰成值的加权和：\n\n$$ f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7) $$\n\n其中查询$\\pmb{q}$和键$\\pmb{k}_i$的注意⼒权重（标量）是通过注意⼒评分函数$\\alpha$将两个向量映射成标量，再经过softmax运算得到的：\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8) $$\n\n\n选择不同的注意⼒评分函数a会导致不同的注意⼒汇聚操作，注意力的不同主要在其评分函数的不同，常用的注意力主要是加性注意力（additive attention）和缩放点积注意⼒（scaled dot-product attention）。⼀般来说，当查询和键是不同⻓度的⽮量时，我们可以使⽤加性注意⼒作为评分函数。给定查询q ∈ Rq和 键k ∈ Rk，加性注意⼒的评分函数为\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9) $$\n\n其中可学习的参数是$$\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}$$、$$\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}$$和$$\\pmb{W}_v \\in \\mathbb{R}_h$$。如公式（9）所示，将查询和键连结起来后输入到一个多层感知机中，感知机包含一个隐含层，其隐含但愿是一个超参数$h$。通过使用tanh作为激活函数，并禁止使用偏置项。\n\n使⽤点积可以得到计算效率更⾼的评分函数，但是点积操作要求查询和键具有相同的⻓度$d$。假设查询和键的所有元素都是独⽴的随机变量，并且都满⾜零均值和单位⽅差，那么两个向量的点积的均值为0，⽅差为$d$。为确保⽆论向量⻓度如何，点积的⽅差在不考虑向量⻓度的情况下仍然是1，我们将点积除以$\\sqrt{d}$，则缩放点积注意⼒评分函数为：\n\n$$ \\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10) $$\n\n在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键-值对的计算注意力，其中查询和键的长度为$d$，值得长度为$v$。查询$$\\pmb{Q} \\in \\mathbb{R}^{n \\times d}$$、键$$\\pmb{K} \\in \\mathbb{R}^{m \\times d}$$和值$$\\pmb{V} \\in \\mathbb{R}^{m \\times v}$$的缩放点积注意力是：\n\n$$ softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11) $$\n\n\n了解了注意力机制就来看看Transformer是怎么使用这些机制设计神经网络架构的。Transformer使用的是缩放点积的自注意⼒机制，并且是多头的。下面来看一下Transformer的网络结构：\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\">\n\nTransformer的采用的是encoder-decoder的架构，bert主要使用了encoder部分。在encoder部分，输入是加上位置编码的embedding，然后输入一个N=6层的encode层，每一个encode层包括两个子层，一个是多头自注意力机制，另一个是基于位置的全连接前馈神经网络，每一部分网络的输出连接一个残差网络和一个归一化层。在decoder部分包含一个N=6层的decode层，每一个encode层包括三个子层，除了包含encode中的两个子层外，还增加了一个mask多头注意力。这里的mask可以遮盖预测位置之后的输出，来确保之后的信息不被泄露。\n\n\n这里值得一提的是，因为Transformer完全采用注意力机制，不像是CNN或RNN都在网络中包含了位置信息，所以在Transformer的输入中要结合一个位置编码，Transformer使用了$sine$和$cosine$函数作为位置编码：\n\n$$ PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12) $$\n$$ PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13) $$\n\n其中，$pos$是位置，$i$是维度，$d_{model}$是模型隐含层的维度。也就是说，位置编码的每个维度对应于一个正弦曲线，曲线从$2\\pi$到$10000\\cdot 2\\pi$。有关位置编码的故事可以查看苏神的博客：[让研究人员绞尽脑汁的Transformer位置编码](https://spaces.ac.cn/archives/8130)和[Sinusoidal位置编码追根溯源](https://spaces.ac.cn/archives/8231)。\n\n### 多头自注意力有什么好处\n因为自注意力机制是互斥的，也就是说，对于一个语句，一个自注意力机制只能从某个角度出发学习所关注的词的权重，但是可能存在多种关注关系，这时就需要用到多个自注意力头。多头自注意力机制有点类似于CNN中的多卷积核，直观上讲，有助于网络从不同模式中铺捉到更加丰富的特征。\n\n\n\n### 从Transformer到BERT\nBERT采用了Transformer架构的encode部分，并且在输入的embedding中加入了toke_type。BERT支持同时输入两个句子，并用特殊符号进行拼接开头是[CLS]，两句的中间和最后添加[SEP]。有关[CLS]的解释是可以用作句嵌入的embedding，将CLS上的向量在下游fine-tune后可用作分类或者检索任务等。[SEP]是为了区分输入的两个句子，这个是在BERT预训练的任务中用到的。\n\n\n由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种Masked Language Model的预训练方式，即随机从输入预料上mask掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。因此BERT相较Transformer是双向的。仅仅一个MLM任务是不足以让BERT解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即Next Sequence Prediction。具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文。对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。\n\n\n### BERT的局限在哪里？\n- BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务\n- 由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）\n- 适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)\n- BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计\n\n\n### 参考文献\n[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] https://github.com/d2l-ai/d2l-zh","source":"_posts/bert-3w.md","raw":"---\ntitle: BERT 知多少\ndate: 2021-03-02 16:40:29\ntags:\n- BERT\ncategories: \n- 预训练语言模型\nmathjax: true\n---\n\nBERT已经提出三四年了，但是提到NLP首先想到的还是Transformer[1]、BERT[2]。像经典的CNN、RNN一样BERT也可以说Transformer也一定在会在NLP和人工智能的舞台上留下浓墨重彩的一笔，如此经典的神经网络模型，当然值得不断深入的思考和学习啦！\n<!--more-->\n\n\n### Transformer和CNN、RNN的区别是什么，本质是什么？\n这三个模型其实是代表了三种典型的神经网络。RNN代表的是循环神经网络，CNN代表的是卷积神经网络，而transformer代表的是注意力机制下的神经网络。所以要理解他们之间的区别就要了解这三种神经网络的特性。理解这三种神经网络的特点首先要了解神经网络的发展过程。\n\n\n最初的神经网络发展自单层感知机，由于单层感知机只是将特征加权后输出，所以不具备处理非线性问题的能力，也导致神经网络发展停滞，直到出现多层神经网络加激活函数形式的多层感知机，神经网络才再度出现在人们的视野。研究人员开始研究多层感知机的优化，首先是各种各样的激活函数的出现，随着计算机硬件能力的不断提高，通过加深网络，和提高参数量，神经网络获得了很大的进展。但是，多层感知机的全连接网络由于设计简单，对于所有的特征提取没有区分度，所以在处理复杂的数据例如图像数据、序列数据时就会显得力不从心。随着计算机视觉和自然语言处理的发展，深度学习和对应的领域进行融合，于是发展出了专为图像数据而设计的卷积神经网络CNN以及专为文本序列处理而设计的循环卷积网络RNN。在这两种模式的神经网络取得成功后，研究人员迅速改进了RNN和CNN发展出了基于这两种神经网络的各种现代网络，但是核心思想是不变的。之后，研究人员还在神经网络中引入了注意力机制。而Transformer的出现是神经网络发展中另一个具有里程碑意义的模型结构。Transformer完全抛弃了CNN和RNN，采用多头注意力机制（multi-head attention）和自注意力（self-attention），Transformer的大获成功在很大程度上诠释了基于自注意力机制神经网络的超强能力。下面就深入细节，具体感受一下各种神经网络的不同。\n\n\n#### (1) 卷积神经网络CNN[3]\n卷积神经网络的核心思想在于它考虑了数据的局部空间特性，以及数据的平移不变性，在此基础上进行特征提取。因此，CNN引入了一个重要的概念“卷积”，在网络上结构中主要体现在引入了卷积层，卷积层的权重就是卷积核。引入卷积层的神经网络较多层感知机可以极大地减少参数。下图是一个典型卷积神经网络的代表。可以看到网络中主要包括卷积层和汇聚层也称池化层，最后的输出是一个全连接层。\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\">\n\n\n要想深入地了解CNN，就需要深入了解卷积层的操作，下面来介绍卷积相关的概念和操作。首先了解什么卷积？在数学中，两个函数（$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $）之间的卷积被定义为：\n\n$$ (f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)$$\n\n也就是说，卷积是把一个函数“翻转”并移位$ \\bf{x} $时，测量$f$和$g$之前的重叠。当为离散对象时，积分就变成求和：\n\n$$ (f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2) $$\n\n对于二维张量，则$f$的索引为$(a,b)$和$g$的索引$(i-a, j-b)$上的对应加和:\n\n$$ (f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3) $$\n\n严格来说，卷积层做的操作不是卷积运算而是互相关运算，卷积层对输⼊和卷积核权重进⾏互相关运算，并在添加标量偏置之后产⽣输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。就像我们之前随机初始化全连接层⼀样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。\n\n\n影响卷积输出的大小主要是填充（padding）和步幅（stride）。⽐如，⼀个240 × 240像素的图像，经过10层5 × 5的卷积后，将减少到200 × 200像素。如此⼀来，原始图像的边界丢失了许多有⽤信息。⽽填充是解决此问题最有效的⽅法。有时，我们可能希望⼤幅降低图像的宽度和⾼度。例如，如果我们发现原始的输⼊分辨率⼗分冗余。步幅则可以在这类情况下提供帮助。卷积神经网络中还有一个重要的概念是通道，通道的作用我理解就是不同的特征提取器，拿图像举例，图像中一个像素点需要三个维度的数据来表示（RGB），那么其中每一个维度就需要一个通道，一个通道就对应一个卷积层。当然，随着CNN的发展也出现了各种各样的卷积操作，其中包括转置(Transposed)卷积，可分离卷积，扩张/空洞(Dilated/Atrous)卷积以及可变形(Deformable)卷积等。\n\n\n除了最核心的卷积层外，CNN还有一个关键的网络层汇聚层，一般的神经网络在每一次卷积操作之后都会接一个汇聚层，对前一层提取的特征进行采样。因为卷积层更多的关注空间的局部特征，所以加入汇聚层可以更好的捕捉空间的整体特性。汇聚层主要有最⼤汇聚层和平均汇聚层等。\n\n\n比较有代表性的卷积神经网络有LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet以及DenseNet等。\n\n#### (2) 循环神经网络RNN[3]\n如果说卷积神经⽹络可以有效地处理空间信息，那么循环神经⽹络则可以更好地处理序列信息。循环神经⽹络通过引⼊状态变量存储过去的信息和当前的输⼊，从⽽可以确定当前的输出。循环神经网络的设计启发来自于序列模型，在文本处理中常使用n元语法模型，但是n元语法模型中单词$x_t$在时间步$t$的条件概率仅取决于前$n-1$个单词，对于更长序列的捕捉，需要更大的$n$,然⽽模型参数的数量也会随之呈指数增⻓。所以不如含使用隐变量的模型：\n$$ P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4) $$\n\n\n其中$$h_{t-1}$$是隐状态，也称为隐变量，它存储了到时间步$$t-1$$的序列信息。通常，我们可以基于当前输入$$x_t$$和先前的隐状态$$h_{t-1}$$来计算时间步$$t$$处的任何时间的隐状态：\n$$ h_{t} = f(x_t, h_{t-1}) \\qquad (5) $$\n\n\n循环神经⽹络RNN是具有隐状态的神经⽹络，当然包括隐状态的模型还有隐马尔可夫连、条件随机场等。下面来感受一下含有隐状态的神经网络的建模过程：\n假设在时间步$t$有⼩批量输⼊$$X_t \\in \\mathbb{R}^{n \\times d}$$，用$$H_t \\in \\mathbb{R}^{n \\times h}$$表示时间步$t$的隐变量。当前步保存前一个时间步的隐变量$$H_{t-1}$$，引入新的权重参数$$W_{hh} \\in \\mathbb{R}^{h \\times n}$$，来描述如何在当前时间步中使用前一个时间步的隐变量。即当前时间隐藏变量由当前时间步的输入与前一个时间步的隐变量一起计算得出：\n\n\n$$H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6) $$\n\n由于在当前时间步中，隐状态使⽤的定义与前⼀个时间步中使⽤的定义相同，因此（5）的计算是循环的。于是基于循环计算的隐状态神经⽹络被命名为循环神经⽹络。对于时间步$t$，输出层的输出类似于多层感知机中的计算：\n\n$$O_t = H_t W_{hq} + b_q \\qquad (7) $$\n\n\n循环神经⽹络的参数包括隐藏层的权重$$W_{xh} \\in \\mathbb{R}^{d \\times h}$$，$$W_{hh} \\in \\mathbb{R}^{h \\times h}$$和偏置$$b_h \\in \\mathbb{R}^{1 \\times h}$$，以及输出层的权重$$W_{qh} \\in \\mathbb{R}^{q \\times h}$$和偏置$$b_q \\in \\mathbb{R}^{1 \\times q}$$。值得⼀提的是，即使在不同的时间步，循环神经⽹络也总是使⽤这些模型参数。因此，循环神经⽹络的参数开销不会随着时间步的增加⽽增加。下图是一个RNN的网络层结构：\n\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\">\n\n\n随着对RNN的深入研究，研究员发明了控制循环门单元、长短记忆网络以及双向RNN等来改善朴素的循环神经⽹络，也使得现代循环神经网络变得越来越复杂。\n\n\n#### (3) 注意力机制和Transformer\n我不知道注意力机制最初的设计灵感来源于哪里，但是看到有关注意力的一段描述，我觉得可以很好的解释为什么我们需要注意力机制。它的原文是这样的：\n> 注意⼒是稀缺的，⽽环境中的⼲扰注意⼒的信息却并不少。⽐如我们的视觉神经系统⼤约每秒收到108位的信息，这远远超过了⼤脑能够完全处理的⽔平。幸运的是，我们的祖先已经从经验（也称为数据）中认识到“并⾮感官的所有输⼊都是⼀样的”。在整个⼈类历史中，这种只将注意⼒引向感兴趣的⼀⼩部分信息的能⼒，使我们的⼤脑能够更明智地分配资源来⽣存、成⻓和社交，例如发现天敌、找寻⻝物和伴侣。[3]\n\n通过学习人类的注意力，我们把注意力分为自主性注意力和非自主性注意力。对于简单的感官传入到我们的大脑的刺激是非自主性注意力，对于我们主动选择要查询搜索的刺激是自主性注意力。那么如何用神经网络来设计注意力机制的框架呢？\n\n\n对于非自主性注意力，可以简单地使⽤参数化的全连接层，甚⾄是⾮参数化的最⼤汇聚层或平均汇聚层。因此，“是否包含⾃主性提⽰”将注意⼒机制与全连接层或汇聚层区别开来。在注意⼒机制的背景下，我们将⾃主性提⽰称为查询（query），非自主查询称为键（key）。注意力机制的设计如下图所示，它展示了一个查询（自助查询）和所有键（非自主查询）通过注意力汇聚产生的结果和感觉输入：值（value）结合作为中间特征输入到神经网络中的过程。\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\">\n\n实际应用中，假设有一个查询$\\pmb{q} \\in \\mathbb{R}^q$和$m$个“键-值”对$$(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k$$，其中$$\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v$$，注意力汇聚函数$f$通常被表⽰成值的加权和：\n\n$$ f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7) $$\n\n其中查询$\\pmb{q}$和键$\\pmb{k}_i$的注意⼒权重（标量）是通过注意⼒评分函数$\\alpha$将两个向量映射成标量，再经过softmax运算得到的：\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8) $$\n\n\n选择不同的注意⼒评分函数a会导致不同的注意⼒汇聚操作，注意力的不同主要在其评分函数的不同，常用的注意力主要是加性注意力（additive attention）和缩放点积注意⼒（scaled dot-product attention）。⼀般来说，当查询和键是不同⻓度的⽮量时，我们可以使⽤加性注意⼒作为评分函数。给定查询q ∈ Rq和 键k ∈ Rk，加性注意⼒的评分函数为\n\n$$ \\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9) $$\n\n其中可学习的参数是$$\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}$$、$$\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}$$和$$\\pmb{W}_v \\in \\mathbb{R}_h$$。如公式（9）所示，将查询和键连结起来后输入到一个多层感知机中，感知机包含一个隐含层，其隐含但愿是一个超参数$h$。通过使用tanh作为激活函数，并禁止使用偏置项。\n\n使⽤点积可以得到计算效率更⾼的评分函数，但是点积操作要求查询和键具有相同的⻓度$d$。假设查询和键的所有元素都是独⽴的随机变量，并且都满⾜零均值和单位⽅差，那么两个向量的点积的均值为0，⽅差为$d$。为确保⽆论向量⻓度如何，点积的⽅差在不考虑向量⻓度的情况下仍然是1，我们将点积除以$\\sqrt{d}$，则缩放点积注意⼒评分函数为：\n\n$$ \\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10) $$\n\n在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键-值对的计算注意力，其中查询和键的长度为$d$，值得长度为$v$。查询$$\\pmb{Q} \\in \\mathbb{R}^{n \\times d}$$、键$$\\pmb{K} \\in \\mathbb{R}^{m \\times d}$$和值$$\\pmb{V} \\in \\mathbb{R}^{m \\times v}$$的缩放点积注意力是：\n\n$$ softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11) $$\n\n\n了解了注意力机制就来看看Transformer是怎么使用这些机制设计神经网络架构的。Transformer使用的是缩放点积的自注意⼒机制，并且是多头的。下面来看一下Transformer的网络结构：\n<img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\">\n\nTransformer的采用的是encoder-decoder的架构，bert主要使用了encoder部分。在encoder部分，输入是加上位置编码的embedding，然后输入一个N=6层的encode层，每一个encode层包括两个子层，一个是多头自注意力机制，另一个是基于位置的全连接前馈神经网络，每一部分网络的输出连接一个残差网络和一个归一化层。在decoder部分包含一个N=6层的decode层，每一个encode层包括三个子层，除了包含encode中的两个子层外，还增加了一个mask多头注意力。这里的mask可以遮盖预测位置之后的输出，来确保之后的信息不被泄露。\n\n\n这里值得一提的是，因为Transformer完全采用注意力机制，不像是CNN或RNN都在网络中包含了位置信息，所以在Transformer的输入中要结合一个位置编码，Transformer使用了$sine$和$cosine$函数作为位置编码：\n\n$$ PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12) $$\n$$ PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13) $$\n\n其中，$pos$是位置，$i$是维度，$d_{model}$是模型隐含层的维度。也就是说，位置编码的每个维度对应于一个正弦曲线，曲线从$2\\pi$到$10000\\cdot 2\\pi$。有关位置编码的故事可以查看苏神的博客：[让研究人员绞尽脑汁的Transformer位置编码](https://spaces.ac.cn/archives/8130)和[Sinusoidal位置编码追根溯源](https://spaces.ac.cn/archives/8231)。\n\n### 多头自注意力有什么好处\n因为自注意力机制是互斥的，也就是说，对于一个语句，一个自注意力机制只能从某个角度出发学习所关注的词的权重，但是可能存在多种关注关系，这时就需要用到多个自注意力头。多头自注意力机制有点类似于CNN中的多卷积核，直观上讲，有助于网络从不同模式中铺捉到更加丰富的特征。\n\n\n\n### 从Transformer到BERT\nBERT采用了Transformer架构的encode部分，并且在输入的embedding中加入了toke_type。BERT支持同时输入两个句子，并用特殊符号进行拼接开头是[CLS]，两句的中间和最后添加[SEP]。有关[CLS]的解释是可以用作句嵌入的embedding，将CLS上的向量在下游fine-tune后可用作分类或者检索任务等。[SEP]是为了区分输入的两个句子，这个是在BERT预训练的任务中用到的。\n\n\n由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种Masked Language Model的预训练方式，即随机从输入预料上mask掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。因此BERT相较Transformer是双向的。仅仅一个MLM任务是不足以让BERT解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即Next Sequence Prediction。具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文。对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。\n\n\n### BERT的局限在哪里？\n- BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务\n- 由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）\n- 适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)\n- BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计\n\n\n### 参考文献\n[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.\n[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] https://github.com/d2l-ai/d2l-zh","slug":"bert-3w","published":1,"updated":"2022-06-07T07:06:27.506Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4xlga7w0020q4uxcn91gosx","content":"<p>BERT已经提出三四年了，但是提到NLP首先想到的还是Transformer[1]、BERT[2]。像经典的CNN、RNN一样BERT也可以说Transformer也一定在会在NLP和人工智能的舞台上留下浓墨重彩的一笔，如此经典的神经网络模型，当然值得不断深入的思考和学习啦！<br><span id=\"more\"></span></p>\n<h3 id=\"Transformer和CNN、RNN的区别是什么，本质是什么？\"><a href=\"#Transformer和CNN、RNN的区别是什么，本质是什么？\" class=\"headerlink\" title=\"Transformer和CNN、RNN的区别是什么，本质是什么？\"></a>Transformer和CNN、RNN的区别是什么，本质是什么？</h3><p>这三个模型其实是代表了三种典型的神经网络。RNN代表的是循环神经网络，CNN代表的是卷积神经网络，而transformer代表的是注意力机制下的神经网络。所以要理解他们之间的区别就要了解这三种神经网络的特性。理解这三种神经网络的特点首先要了解神经网络的发展过程。</p>\n<p>最初的神经网络发展自单层感知机，由于单层感知机只是将特征加权后输出，所以不具备处理非线性问题的能力，也导致神经网络发展停滞，直到出现多层神经网络加激活函数形式的多层感知机，神经网络才再度出现在人们的视野。研究人员开始研究多层感知机的优化，首先是各种各样的激活函数的出现，随着计算机硬件能力的不断提高，通过加深网络，和提高参数量，神经网络获得了很大的进展。但是，多层感知机的全连接网络由于设计简单，对于所有的特征提取没有区分度，所以在处理复杂的数据例如图像数据、序列数据时就会显得力不从心。随着计算机视觉和自然语言处理的发展，深度学习和对应的领域进行融合，于是发展出了专为图像数据而设计的卷积神经网络CNN以及专为文本序列处理而设计的循环卷积网络RNN。在这两种模式的神经网络取得成功后，研究人员迅速改进了RNN和CNN发展出了基于这两种神经网络的各种现代网络，但是核心思想是不变的。之后，研究人员还在神经网络中引入了注意力机制。而Transformer的出现是神经网络发展中另一个具有里程碑意义的模型结构。Transformer完全抛弃了CNN和RNN，采用多头注意力机制（multi-head attention）和自注意力（self-attention），Transformer的大获成功在很大程度上诠释了基于自注意力机制神经网络的超强能力。下面就深入细节，具体感受一下各种神经网络的不同。</p>\n<h4 id=\"1-卷积神经网络CNN-3\"><a href=\"#1-卷积神经网络CNN-3\" class=\"headerlink\" title=\"(1) 卷积神经网络CNN[3]\"></a>(1) 卷积神经网络CNN[3]</h4><p>卷积神经网络的核心思想在于它考虑了数据的局部空间特性，以及数据的平移不变性，在此基础上进行特征提取。因此，CNN引入了一个重要的概念“卷积”，在网络上结构中主要体现在引入了卷积层，卷积层的权重就是卷积核。引入卷积层的神经网络较多层感知机可以极大地减少参数。下图是一个典型卷积神经网络的代表。可以看到网络中主要包括卷积层和汇聚层也称池化层，最后的输出是一个全连接层。<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\"></p>\n<p>要想深入地了解CNN，就需要深入了解卷积层的操作，下面来介绍卷积相关的概念和操作。首先了解什么卷积？在数学中，两个函数（$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $）之间的卷积被定义为：</p>\n<script type=\"math/tex; mode=display\">(f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)</script><p>也就是说，卷积是把一个函数“翻转”并移位$ \\bf{x} $时，测量$f$和$g$之前的重叠。当为离散对象时，积分就变成求和：</p>\n<script type=\"math/tex; mode=display\">(f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2)</script><p>对于二维张量，则$f$的索引为$(a,b)$和$g$的索引$(i-a, j-b)$上的对应加和:</p>\n<script type=\"math/tex; mode=display\">(f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3)</script><p>严格来说，卷积层做的操作不是卷积运算而是互相关运算，卷积层对输⼊和卷积核权重进⾏互相关运算，并在添加标量偏置之后产⽣输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。就像我们之前随机初始化全连接层⼀样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。</p>\n<p>影响卷积输出的大小主要是填充（padding）和步幅（stride）。⽐如，⼀个240 × 240像素的图像，经过10层5 × 5的卷积后，将减少到200 × 200像素。如此⼀来，原始图像的边界丢失了许多有⽤信息。⽽填充是解决此问题最有效的⽅法。有时，我们可能希望⼤幅降低图像的宽度和⾼度。例如，如果我们发现原始的输⼊分辨率⼗分冗余。步幅则可以在这类情况下提供帮助。卷积神经网络中还有一个重要的概念是通道，通道的作用我理解就是不同的特征提取器，拿图像举例，图像中一个像素点需要三个维度的数据来表示（RGB），那么其中每一个维度就需要一个通道，一个通道就对应一个卷积层。当然，随着CNN的发展也出现了各种各样的卷积操作，其中包括转置(Transposed)卷积，可分离卷积，扩张/空洞(Dilated/Atrous)卷积以及可变形(Deformable)卷积等。</p>\n<p>除了最核心的卷积层外，CNN还有一个关键的网络层汇聚层，一般的神经网络在每一次卷积操作之后都会接一个汇聚层，对前一层提取的特征进行采样。因为卷积层更多的关注空间的局部特征，所以加入汇聚层可以更好的捕捉空间的整体特性。汇聚层主要有最⼤汇聚层和平均汇聚层等。</p>\n<p>比较有代表性的卷积神经网络有LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet以及DenseNet等。</p>\n<h4 id=\"2-循环神经网络RNN-3\"><a href=\"#2-循环神经网络RNN-3\" class=\"headerlink\" title=\"(2) 循环神经网络RNN[3]\"></a>(2) 循环神经网络RNN[3]</h4><p>如果说卷积神经⽹络可以有效地处理空间信息，那么循环神经⽹络则可以更好地处理序列信息。循环神经⽹络通过引⼊状态变量存储过去的信息和当前的输⼊，从⽽可以确定当前的输出。循环神经网络的设计启发来自于序列模型，在文本处理中常使用n元语法模型，但是n元语法模型中单词$x_t$在时间步$t$的条件概率仅取决于前$n-1$个单词，对于更长序列的捕捉，需要更大的$n$,然⽽模型参数的数量也会随之呈指数增⻓。所以不如含使用隐变量的模型：</p>\n<script type=\"math/tex; mode=display\">P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4)</script><p>其中<script type=\"math/tex\">h_{t-1}</script>是隐状态，也称为隐变量，它存储了到时间步<script type=\"math/tex\">t-1</script>的序列信息。通常，我们可以基于当前输入<script type=\"math/tex\">x_t</script>和先前的隐状态<script type=\"math/tex\">h_{t-1}</script>来计算时间步<script type=\"math/tex\">t</script>处的任何时间的隐状态：</p>\n<script type=\"math/tex; mode=display\">h_{t} = f(x_t, h_{t-1}) \\qquad (5)</script><p>循环神经⽹络RNN是具有隐状态的神经⽹络，当然包括隐状态的模型还有隐马尔可夫连、条件随机场等。下面来感受一下含有隐状态的神经网络的建模过程：<br>假设在时间步$t$有⼩批量输⼊<script type=\"math/tex\">X_t \\in \\mathbb{R}^{n \\times d}</script>，用<script type=\"math/tex\">H_t \\in \\mathbb{R}^{n \\times h}</script>表示时间步$t$的隐变量。当前步保存前一个时间步的隐变量<script type=\"math/tex\">H_{t-1}</script>，引入新的权重参数<script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times n}</script>，来描述如何在当前时间步中使用前一个时间步的隐变量。即当前时间隐藏变量由当前时间步的输入与前一个时间步的隐变量一起计算得出：</p>\n<script type=\"math/tex; mode=display\">H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6)</script><p>由于在当前时间步中，隐状态使⽤的定义与前⼀个时间步中使⽤的定义相同，因此（5）的计算是循环的。于是基于循环计算的隐状态神经⽹络被命名为循环神经⽹络。对于时间步$t$，输出层的输出类似于多层感知机中的计算：</p>\n<script type=\"math/tex; mode=display\">O_t = H_t W_{hq} + b_q \\qquad (7)</script><p>循环神经⽹络的参数包括隐藏层的权重<script type=\"math/tex\">W_{xh} \\in \\mathbb{R}^{d \\times h}</script>，<script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times h}</script>和偏置<script type=\"math/tex\">b_h \\in \\mathbb{R}^{1 \\times h}</script>，以及输出层的权重<script type=\"math/tex\">W_{qh} \\in \\mathbb{R}^{q \\times h}</script>和偏置<script type=\"math/tex\">b_q \\in \\mathbb{R}^{1 \\times q}</script>。值得⼀提的是，即使在不同的时间步，循环神经⽹络也总是使⽤这些模型参数。因此，循环神经⽹络的参数开销不会随着时间步的增加⽽增加。下图是一个RNN的网络层结构：</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\"></p>\n<p>随着对RNN的深入研究，研究员发明了控制循环门单元、长短记忆网络以及双向RNN等来改善朴素的循环神经⽹络，也使得现代循环神经网络变得越来越复杂。</p>\n<h4 id=\"3-注意力机制和Transformer\"><a href=\"#3-注意力机制和Transformer\" class=\"headerlink\" title=\"(3) 注意力机制和Transformer\"></a>(3) 注意力机制和Transformer</h4><p>我不知道注意力机制最初的设计灵感来源于哪里，但是看到有关注意力的一段描述，我觉得可以很好的解释为什么我们需要注意力机制。它的原文是这样的：</p>\n<blockquote>\n<p>注意⼒是稀缺的，⽽环境中的⼲扰注意⼒的信息却并不少。⽐如我们的视觉神经系统⼤约每秒收到108位的信息，这远远超过了⼤脑能够完全处理的⽔平。幸运的是，我们的祖先已经从经验（也称为数据）中认识到“并⾮感官的所有输⼊都是⼀样的”。在整个⼈类历史中，这种只将注意⼒引向感兴趣的⼀⼩部分信息的能⼒，使我们的⼤脑能够更明智地分配资源来⽣存、成⻓和社交，例如发现天敌、找寻⻝物和伴侣。[3]</p>\n</blockquote>\n<p>通过学习人类的注意力，我们把注意力分为自主性注意力和非自主性注意力。对于简单的感官传入到我们的大脑的刺激是非自主性注意力，对于我们主动选择要查询搜索的刺激是自主性注意力。那么如何用神经网络来设计注意力机制的框架呢？</p>\n<p>对于非自主性注意力，可以简单地使⽤参数化的全连接层，甚⾄是⾮参数化的最⼤汇聚层或平均汇聚层。因此，“是否包含⾃主性提⽰”将注意⼒机制与全连接层或汇聚层区别开来。在注意⼒机制的背景下，我们将⾃主性提⽰称为查询（query），非自主查询称为键（key）。注意力机制的设计如下图所示，它展示了一个查询（自助查询）和所有键（非自主查询）通过注意力汇聚产生的结果和感觉输入：值（value）结合作为中间特征输入到神经网络中的过程。<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\"></p>\n<p>实际应用中，假设有一个查询$\\pmb{q} \\in \\mathbb{R}^q$和$m$个“键-值”对<script type=\"math/tex\">(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k</script>，其中<script type=\"math/tex\">\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v</script>，注意力汇聚函数$f$通常被表⽰成值的加权和：</p>\n<script type=\"math/tex; mode=display\">f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7)</script><p>其中查询$\\pmb{q}$和键$\\pmb{k}_i$的注意⼒权重（标量）是通过注意⼒评分函数$\\alpha$将两个向量映射成标量，再经过softmax运算得到的：</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8)</script><p>选择不同的注意⼒评分函数a会导致不同的注意⼒汇聚操作，注意力的不同主要在其评分函数的不同，常用的注意力主要是加性注意力（additive attention）和缩放点积注意⼒（scaled dot-product attention）。⼀般来说，当查询和键是不同⻓度的⽮量时，我们可以使⽤加性注意⼒作为评分函数。给定查询q ∈ Rq和 键k ∈ Rk，加性注意⼒的评分函数为</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9)</script><p>其中可学习的参数是<script type=\"math/tex\">\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}</script>、<script type=\"math/tex\">\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}</script>和<script type=\"math/tex\">\\pmb{W}_v \\in \\mathbb{R}_h</script>。如公式（9）所示，将查询和键连结起来后输入到一个多层感知机中，感知机包含一个隐含层，其隐含但愿是一个超参数$h$。通过使用tanh作为激活函数，并禁止使用偏置项。</p>\n<p>使⽤点积可以得到计算效率更⾼的评分函数，但是点积操作要求查询和键具有相同的⻓度$d$。假设查询和键的所有元素都是独⽴的随机变量，并且都满⾜零均值和单位⽅差，那么两个向量的点积的均值为0，⽅差为$d$。为确保⽆论向量⻓度如何，点积的⽅差在不考虑向量⻓度的情况下仍然是1，我们将点积除以$\\sqrt{d}$，则缩放点积注意⼒评分函数为：</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10)</script><p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键-值对的计算注意力，其中查询和键的长度为$d$，值得长度为$v$。查询<script type=\"math/tex\">\\pmb{Q} \\in \\mathbb{R}^{n \\times d}</script>、键<script type=\"math/tex\">\\pmb{K} \\in \\mathbb{R}^{m \\times d}</script>和值<script type=\"math/tex\">\\pmb{V} \\in \\mathbb{R}^{m \\times v}</script>的缩放点积注意力是：</p>\n<script type=\"math/tex; mode=display\">softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11)</script><p>了解了注意力机制就来看看Transformer是怎么使用这些机制设计神经网络架构的。Transformer使用的是缩放点积的自注意⼒机制，并且是多头的。下面来看一下Transformer的网络结构：<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\"></p>\n<p>Transformer的采用的是encoder-decoder的架构，bert主要使用了encoder部分。在encoder部分，输入是加上位置编码的embedding，然后输入一个N=6层的encode层，每一个encode层包括两个子层，一个是多头自注意力机制，另一个是基于位置的全连接前馈神经网络，每一部分网络的输出连接一个残差网络和一个归一化层。在decoder部分包含一个N=6层的decode层，每一个encode层包括三个子层，除了包含encode中的两个子层外，还增加了一个mask多头注意力。这里的mask可以遮盖预测位置之后的输出，来确保之后的信息不被泄露。</p>\n<p>这里值得一提的是，因为Transformer完全采用注意力机制，不像是CNN或RNN都在网络中包含了位置信息，所以在Transformer的输入中要结合一个位置编码，Transformer使用了$sine$和$cosine$函数作为位置编码：</p>\n<script type=\"math/tex; mode=display\">PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12)</script><script type=\"math/tex; mode=display\">PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13)</script><p>其中，$pos$是位置，$i$是维度，$d_{model}$是模型隐含层的维度。也就是说，位置编码的每个维度对应于一个正弦曲线，曲线从$2\\pi$到$10000\\cdot 2\\pi$。有关位置编码的故事可以查看苏神的博客：<a href=\"https://spaces.ac.cn/archives/8130\">让研究人员绞尽脑汁的Transformer位置编码</a>和<a href=\"https://spaces.ac.cn/archives/8231\">Sinusoidal位置编码追根溯源</a>。</p>\n<h3 id=\"多头自注意力有什么好处\"><a href=\"#多头自注意力有什么好处\" class=\"headerlink\" title=\"多头自注意力有什么好处\"></a>多头自注意力有什么好处</h3><p>因为自注意力机制是互斥的，也就是说，对于一个语句，一个自注意力机制只能从某个角度出发学习所关注的词的权重，但是可能存在多种关注关系，这时就需要用到多个自注意力头。多头自注意力机制有点类似于CNN中的多卷积核，直观上讲，有助于网络从不同模式中铺捉到更加丰富的特征。</p>\n<h3 id=\"从Transformer到BERT\"><a href=\"#从Transformer到BERT\" class=\"headerlink\" title=\"从Transformer到BERT\"></a>从Transformer到BERT</h3><p>BERT采用了Transformer架构的encode部分，并且在输入的embedding中加入了toke_type。BERT支持同时输入两个句子，并用特殊符号进行拼接开头是[CLS]，两句的中间和最后添加[SEP]。有关[CLS]的解释是可以用作句嵌入的embedding，将CLS上的向量在下游fine-tune后可用作分类或者检索任务等。[SEP]是为了区分输入的两个句子，这个是在BERT预训练的任务中用到的。</p>\n<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种Masked Language Model的预训练方式，即随机从输入预料上mask掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。因此BERT相较Transformer是双向的。仅仅一个MLM任务是不足以让BERT解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即Next Sequence Prediction。具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文。对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。</p>\n<h3 id=\"BERT的局限在哪里？\"><a href=\"#BERT的局限在哪里？\" class=\"headerlink\" title=\"BERT的局限在哪里？\"></a>BERT的局限在哪里？</h3><ul>\n<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>\n<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>\n<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>\n<li>BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>\n</ul>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.<br>[3] <a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></p>\n","site":{"data":{}},"excerpt":"<p>BERT已经提出三四年了，但是提到NLP首先想到的还是Transformer[1]、BERT[2]。像经典的CNN、RNN一样BERT也可以说Transformer也一定在会在NLP和人工智能的舞台上留下浓墨重彩的一笔，如此经典的神经网络模型，当然值得不断深入的思考和学习啦！<br>","more":"</p>\n<h3 id=\"Transformer和CNN、RNN的区别是什么，本质是什么？\"><a href=\"#Transformer和CNN、RNN的区别是什么，本质是什么？\" class=\"headerlink\" title=\"Transformer和CNN、RNN的区别是什么，本质是什么？\"></a>Transformer和CNN、RNN的区别是什么，本质是什么？</h3><p>这三个模型其实是代表了三种典型的神经网络。RNN代表的是循环神经网络，CNN代表的是卷积神经网络，而transformer代表的是注意力机制下的神经网络。所以要理解他们之间的区别就要了解这三种神经网络的特性。理解这三种神经网络的特点首先要了解神经网络的发展过程。</p>\n<p>最初的神经网络发展自单层感知机，由于单层感知机只是将特征加权后输出，所以不具备处理非线性问题的能力，也导致神经网络发展停滞，直到出现多层神经网络加激活函数形式的多层感知机，神经网络才再度出现在人们的视野。研究人员开始研究多层感知机的优化，首先是各种各样的激活函数的出现，随着计算机硬件能力的不断提高，通过加深网络，和提高参数量，神经网络获得了很大的进展。但是，多层感知机的全连接网络由于设计简单，对于所有的特征提取没有区分度，所以在处理复杂的数据例如图像数据、序列数据时就会显得力不从心。随着计算机视觉和自然语言处理的发展，深度学习和对应的领域进行融合，于是发展出了专为图像数据而设计的卷积神经网络CNN以及专为文本序列处理而设计的循环卷积网络RNN。在这两种模式的神经网络取得成功后，研究人员迅速改进了RNN和CNN发展出了基于这两种神经网络的各种现代网络，但是核心思想是不变的。之后，研究人员还在神经网络中引入了注意力机制。而Transformer的出现是神经网络发展中另一个具有里程碑意义的模型结构。Transformer完全抛弃了CNN和RNN，采用多头注意力机制（multi-head attention）和自注意力（self-attention），Transformer的大获成功在很大程度上诠释了基于自注意力机制神经网络的超强能力。下面就深入细节，具体感受一下各种神经网络的不同。</p>\n<h4 id=\"1-卷积神经网络CNN-3\"><a href=\"#1-卷积神经网络CNN-3\" class=\"headerlink\" title=\"(1) 卷积神经网络CNN[3]\"></a>(1) 卷积神经网络CNN[3]</h4><p>卷积神经网络的核心思想在于它考虑了数据的局部空间特性，以及数据的平移不变性，在此基础上进行特征提取。因此，CNN引入了一个重要的概念“卷积”，在网络上结构中主要体现在引入了卷积层，卷积层的权重就是卷积核。引入卷积层的神经网络较多层感知机可以极大地减少参数。下图是一个典型卷积神经网络的代表。可以看到网络中主要包括卷积层和汇聚层也称池化层，最后的输出是一个全连接层。<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/LeNet.png\" width=\"70%\" height=\"50%\"></p>\n<p>要想深入地了解CNN，就需要深入了解卷积层的操作，下面来介绍卷积相关的概念和操作。首先了解什么卷积？在数学中，两个函数（$ f,g:\\mathbb{R}^d \\to \\mathbb{R} $）之间的卷积被定义为：</p>\n<script type=\"math/tex; mode=display\">(f*g)(\\pmb{x}) = \\int {f(\\pmb{z})g(\\pmb{x}-\\pmb{z})dz} \\qquad (1)</script><p>也就是说，卷积是把一个函数“翻转”并移位$ \\bf{x} $时，测量$f$和$g$之前的重叠。当为离散对象时，积分就变成求和：</p>\n<script type=\"math/tex; mode=display\">(f*g)(i) = \\sum_a {f(a)g(i-a)} \\qquad (2)</script><p>对于二维张量，则$f$的索引为$(a,b)$和$g$的索引$(i-a, j-b)$上的对应加和:</p>\n<script type=\"math/tex; mode=display\">(f*g)(i,j) = \\sum_a{\\sum_b {f(a,b)g(i-a,i-b)}} \\qquad (3)</script><p>严格来说，卷积层做的操作不是卷积运算而是互相关运算，卷积层对输⼊和卷积核权重进⾏互相关运算，并在添加标量偏置之后产⽣输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。就像我们之前随机初始化全连接层⼀样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。</p>\n<p>影响卷积输出的大小主要是填充（padding）和步幅（stride）。⽐如，⼀个240 × 240像素的图像，经过10层5 × 5的卷积后，将减少到200 × 200像素。如此⼀来，原始图像的边界丢失了许多有⽤信息。⽽填充是解决此问题最有效的⽅法。有时，我们可能希望⼤幅降低图像的宽度和⾼度。例如，如果我们发现原始的输⼊分辨率⼗分冗余。步幅则可以在这类情况下提供帮助。卷积神经网络中还有一个重要的概念是通道，通道的作用我理解就是不同的特征提取器，拿图像举例，图像中一个像素点需要三个维度的数据来表示（RGB），那么其中每一个维度就需要一个通道，一个通道就对应一个卷积层。当然，随着CNN的发展也出现了各种各样的卷积操作，其中包括转置(Transposed)卷积，可分离卷积，扩张/空洞(Dilated/Atrous)卷积以及可变形(Deformable)卷积等。</p>\n<p>除了最核心的卷积层外，CNN还有一个关键的网络层汇聚层，一般的神经网络在每一次卷积操作之后都会接一个汇聚层，对前一层提取的特征进行采样。因为卷积层更多的关注空间的局部特征，所以加入汇聚层可以更好的捕捉空间的整体特性。汇聚层主要有最⼤汇聚层和平均汇聚层等。</p>\n<p>比较有代表性的卷积神经网络有LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet以及DenseNet等。</p>\n<h4 id=\"2-循环神经网络RNN-3\"><a href=\"#2-循环神经网络RNN-3\" class=\"headerlink\" title=\"(2) 循环神经网络RNN[3]\"></a>(2) 循环神经网络RNN[3]</h4><p>如果说卷积神经⽹络可以有效地处理空间信息，那么循环神经⽹络则可以更好地处理序列信息。循环神经⽹络通过引⼊状态变量存储过去的信息和当前的输⼊，从⽽可以确定当前的输出。循环神经网络的设计启发来自于序列模型，在文本处理中常使用n元语法模型，但是n元语法模型中单词$x_t$在时间步$t$的条件概率仅取决于前$n-1$个单词，对于更长序列的捕捉，需要更大的$n$,然⽽模型参数的数量也会随之呈指数增⻓。所以不如含使用隐变量的模型：</p>\n<script type=\"math/tex; mode=display\">P(x_t|x_{t-1},...,x_1) \\approx P(x_t|h_{t-1}) \\qquad (4)</script><p>其中<script type=\"math/tex\">h_{t-1}</script>是隐状态，也称为隐变量，它存储了到时间步<script type=\"math/tex\">t-1</script>的序列信息。通常，我们可以基于当前输入<script type=\"math/tex\">x_t</script>和先前的隐状态<script type=\"math/tex\">h_{t-1}</script>来计算时间步<script type=\"math/tex\">t</script>处的任何时间的隐状态：</p>\n<script type=\"math/tex; mode=display\">h_{t} = f(x_t, h_{t-1}) \\qquad (5)</script><p>循环神经⽹络RNN是具有隐状态的神经⽹络，当然包括隐状态的模型还有隐马尔可夫连、条件随机场等。下面来感受一下含有隐状态的神经网络的建模过程：<br>假设在时间步$t$有⼩批量输⼊<script type=\"math/tex\">X_t \\in \\mathbb{R}^{n \\times d}</script>，用<script type=\"math/tex\">H_t \\in \\mathbb{R}^{n \\times h}</script>表示时间步$t$的隐变量。当前步保存前一个时间步的隐变量<script type=\"math/tex\">H_{t-1}</script>，引入新的权重参数<script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times n}</script>，来描述如何在当前时间步中使用前一个时间步的隐变量。即当前时间隐藏变量由当前时间步的输入与前一个时间步的隐变量一起计算得出：</p>\n<script type=\"math/tex; mode=display\">H_t = \\phi (X_t W_{xh} + H_{t-1}W_{hh} + b_h) \\qquad (6)</script><p>由于在当前时间步中，隐状态使⽤的定义与前⼀个时间步中使⽤的定义相同，因此（5）的计算是循环的。于是基于循环计算的隐状态神经⽹络被命名为循环神经⽹络。对于时间步$t$，输出层的输出类似于多层感知机中的计算：</p>\n<script type=\"math/tex; mode=display\">O_t = H_t W_{hq} + b_q \\qquad (7)</script><p>循环神经⽹络的参数包括隐藏层的权重<script type=\"math/tex\">W_{xh} \\in \\mathbb{R}^{d \\times h}</script>，<script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h \\times h}</script>和偏置<script type=\"math/tex\">b_h \\in \\mathbb{R}^{1 \\times h}</script>，以及输出层的权重<script type=\"math/tex\">W_{qh} \\in \\mathbb{R}^{q \\times h}</script>和偏置<script type=\"math/tex\">b_q \\in \\mathbb{R}^{1 \\times q}</script>。值得⼀提的是，即使在不同的时间步，循环神经⽹络也总是使⽤这些模型参数。因此，循环神经⽹络的参数开销不会随着时间步的增加⽽增加。下图是一个RNN的网络层结构：</p>\n<p><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/RNN.png\" width=\"60%\" height=\"50%\"></p>\n<p>随着对RNN的深入研究，研究员发明了控制循环门单元、长短记忆网络以及双向RNN等来改善朴素的循环神经⽹络，也使得现代循环神经网络变得越来越复杂。</p>\n<h4 id=\"3-注意力机制和Transformer\"><a href=\"#3-注意力机制和Transformer\" class=\"headerlink\" title=\"(3) 注意力机制和Transformer\"></a>(3) 注意力机制和Transformer</h4><p>我不知道注意力机制最初的设计灵感来源于哪里，但是看到有关注意力的一段描述，我觉得可以很好的解释为什么我们需要注意力机制。它的原文是这样的：</p>\n<blockquote>\n<p>注意⼒是稀缺的，⽽环境中的⼲扰注意⼒的信息却并不少。⽐如我们的视觉神经系统⼤约每秒收到108位的信息，这远远超过了⼤脑能够完全处理的⽔平。幸运的是，我们的祖先已经从经验（也称为数据）中认识到“并⾮感官的所有输⼊都是⼀样的”。在整个⼈类历史中，这种只将注意⼒引向感兴趣的⼀⼩部分信息的能⼒，使我们的⼤脑能够更明智地分配资源来⽣存、成⻓和社交，例如发现天敌、找寻⻝物和伴侣。[3]</p>\n</blockquote>\n<p>通过学习人类的注意力，我们把注意力分为自主性注意力和非自主性注意力。对于简单的感官传入到我们的大脑的刺激是非自主性注意力，对于我们主动选择要查询搜索的刺激是自主性注意力。那么如何用神经网络来设计注意力机制的框架呢？</p>\n<p>对于非自主性注意力，可以简单地使⽤参数化的全连接层，甚⾄是⾮参数化的最⼤汇聚层或平均汇聚层。因此，“是否包含⾃主性提⽰”将注意⼒机制与全连接层或汇聚层区别开来。在注意⼒机制的背景下，我们将⾃主性提⽰称为查询（query），非自主查询称为键（key）。注意力机制的设计如下图所示，它展示了一个查询（自助查询）和所有键（非自主查询）通过注意力汇聚产生的结果和感觉输入：值（value）结合作为中间特征输入到神经网络中的过程。<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/attention.png\" width=\"60%\" height=\"50%\"></p>\n<p>实际应用中，假设有一个查询$\\pmb{q} \\in \\mathbb{R}^q$和$m$个“键-值”对<script type=\"math/tex\">(\\pmb{k}_i, \\pmb{v}_i), 1 \\leq i \\leq k</script>，其中<script type=\"math/tex\">\\pmb{k}_i \\in \\mathbb{R}^k, \\pmb{v}_i \\in \\mathbb{R}^v</script>，注意力汇聚函数$f$通常被表⽰成值的加权和：</p>\n<script type=\"math/tex; mode=display\">f(\\pmb{q}, (\\pmb{k}_1 , \\pmb{v}_1),...,(\\pmb{k}_m, \\pmb{v}_m)) = \\sum_{i=1}^{m} \\alpha(\\pmb{q}, \\pmb{k}_i)\\pmb{v}_i \\, \\in \\mathbb{R}^v \\qquad (7)</script><p>其中查询$\\pmb{q}$和键$\\pmb{k}_i$的注意⼒权重（标量）是通过注意⼒评分函数$\\alpha$将两个向量映射成标量，再经过softmax运算得到的：</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}_i) = softmax(\\alpha (\\pmb{q}, \\pmb{k}_i)) = \\frac{exp(\\alpha (\\pmb{q}, \\pmb{k}_i))}{\\sum_{j=1}^m{exp(\\alpha (\\pmb{q}, \\pmb{k}_j))}} \\qquad (8)</script><p>选择不同的注意⼒评分函数a会导致不同的注意⼒汇聚操作，注意力的不同主要在其评分函数的不同，常用的注意力主要是加性注意力（additive attention）和缩放点积注意⼒（scaled dot-product attention）。⼀般来说，当查询和键是不同⻓度的⽮量时，我们可以使⽤加性注意⼒作为评分函数。给定查询q ∈ Rq和 键k ∈ Rk，加性注意⼒的评分函数为</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q}, \\pmb{k}) = \\pmb{w}_v^T tanh(\\pmb{W}_q \\pmb{q} + \\pmb{W}_k \\pmb{k}) \\qquad (9)</script><p>其中可学习的参数是<script type=\"math/tex\">\\pmb{W}_q \\in \\mathbb{R}_{h \\times q}</script>、<script type=\"math/tex\">\\pmb{W}_k \\in \\mathbb{R}_{h \\times k}</script>和<script type=\"math/tex\">\\pmb{W}_v \\in \\mathbb{R}_h</script>。如公式（9）所示，将查询和键连结起来后输入到一个多层感知机中，感知机包含一个隐含层，其隐含但愿是一个超参数$h$。通过使用tanh作为激活函数，并禁止使用偏置项。</p>\n<p>使⽤点积可以得到计算效率更⾼的评分函数，但是点积操作要求查询和键具有相同的⻓度$d$。假设查询和键的所有元素都是独⽴的随机变量，并且都满⾜零均值和单位⽅差，那么两个向量的点积的均值为0，⽅差为$d$。为确保⽆论向量⻓度如何，点积的⽅差在不考虑向量⻓度的情况下仍然是1，我们将点积除以$\\sqrt{d}$，则缩放点积注意⼒评分函数为：</p>\n<script type=\"math/tex; mode=display\">\\alpha(\\pmb{q},\\pmb{k}) = \\pmb{q}^T \\pmb{k}/{\\sqrt{d}} \\qquad (10)</script><p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键-值对的计算注意力，其中查询和键的长度为$d$，值得长度为$v$。查询<script type=\"math/tex\">\\pmb{Q} \\in \\mathbb{R}^{n \\times d}</script>、键<script type=\"math/tex\">\\pmb{K} \\in \\mathbb{R}^{m \\times d}</script>和值<script type=\"math/tex\">\\pmb{V} \\in \\mathbb{R}^{m \\times v}</script>的缩放点积注意力是：</p>\n<script type=\"math/tex; mode=display\">softmax(\\frac{\\pmb{Q}\\pmb{K}^T}{\\sqrt{d}})\\pmb{V} \\, \\in \\mathbb{R}^{n \\times v} \\qquad (11)</script><p>了解了注意力机制就来看看Transformer是怎么使用这些机制设计神经网络架构的。Transformer使用的是缩放点积的自注意⼒机制，并且是多头的。下面来看一下Transformer的网络结构：<br><img src=\"https://github.com/Quelisa/picture/raw/main/DNN/transformer.png\" width=\"40%\" height=\"70%\"></p>\n<p>Transformer的采用的是encoder-decoder的架构，bert主要使用了encoder部分。在encoder部分，输入是加上位置编码的embedding，然后输入一个N=6层的encode层，每一个encode层包括两个子层，一个是多头自注意力机制，另一个是基于位置的全连接前馈神经网络，每一部分网络的输出连接一个残差网络和一个归一化层。在decoder部分包含一个N=6层的decode层，每一个encode层包括三个子层，除了包含encode中的两个子层外，还增加了一个mask多头注意力。这里的mask可以遮盖预测位置之后的输出，来确保之后的信息不被泄露。</p>\n<p>这里值得一提的是，因为Transformer完全采用注意力机制，不像是CNN或RNN都在网络中包含了位置信息，所以在Transformer的输入中要结合一个位置编码，Transformer使用了$sine$和$cosine$函数作为位置编码：</p>\n<script type=\"math/tex; mode=display\">PE(pos,2i) = sin(pos/10000^{2i/d_model} \\qquad (12)</script><script type=\"math/tex; mode=display\">PE(pos,2i+1) = sin(pos/10000^{2i/d_model} \\qquad (13)</script><p>其中，$pos$是位置，$i$是维度，$d_{model}$是模型隐含层的维度。也就是说，位置编码的每个维度对应于一个正弦曲线，曲线从$2\\pi$到$10000\\cdot 2\\pi$。有关位置编码的故事可以查看苏神的博客：<a href=\"https://spaces.ac.cn/archives/8130\">让研究人员绞尽脑汁的Transformer位置编码</a>和<a href=\"https://spaces.ac.cn/archives/8231\">Sinusoidal位置编码追根溯源</a>。</p>\n<h3 id=\"多头自注意力有什么好处\"><a href=\"#多头自注意力有什么好处\" class=\"headerlink\" title=\"多头自注意力有什么好处\"></a>多头自注意力有什么好处</h3><p>因为自注意力机制是互斥的，也就是说，对于一个语句，一个自注意力机制只能从某个角度出发学习所关注的词的权重，但是可能存在多种关注关系，这时就需要用到多个自注意力头。多头自注意力机制有点类似于CNN中的多卷积核，直观上讲，有助于网络从不同模式中铺捉到更加丰富的特征。</p>\n<h3 id=\"从Transformer到BERT\"><a href=\"#从Transformer到BERT\" class=\"headerlink\" title=\"从Transformer到BERT\"></a>从Transformer到BERT</h3><p>BERT采用了Transformer架构的encode部分，并且在输入的embedding中加入了toke_type。BERT支持同时输入两个句子，并用特殊符号进行拼接开头是[CLS]，两句的中间和最后添加[SEP]。有关[CLS]的解释是可以用作句嵌入的embedding，将CLS上的向量在下游fine-tune后可用作分类或者检索任务等。[SEP]是为了区分输入的两个句子，这个是在BERT预训练的任务中用到的。</p>\n<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种Masked Language Model的预训练方式，即随机从输入预料上mask掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。因此BERT相较Transformer是双向的。仅仅一个MLM任务是不足以让BERT解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即Next Sequence Prediction。具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文。对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。</p>\n<h3 id=\"BERT的局限在哪里？\"><a href=\"#BERT的局限在哪里？\" class=\"headerlink\" title=\"BERT的局限在哪里？\"></a>BERT的局限在哪里？</h3><ul>\n<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>\n<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>\n<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>\n<li>BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>\n</ul>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p>[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.<br>[2] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.<br>[3] <a href=\"https://github.com/d2l-ai/d2l-zh\">https://github.com/d2l-ai/d2l-zh</a></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cl4xlga6g0001q4uxdduzdc52","category_id":"cl4xlga6m0004q4uxacivhgvn","_id":"cl4xlga6u000eq4ux0w4a8owe"},{"post_id":"cl4xlga6l0003q4uxaami25xn","category_id":"cl4xlga6r000aq4uxbsjsh36j","_id":"cl4xlga76000lq4uxh4pl1za7"},{"post_id":"cl4xlga6p0007q4ux5z0jezoz","category_id":"cl4xlga6v000fq4ux6og8arn4","_id":"cl4xlga7c000rq4ux8l9hac2e"},{"post_id":"cl4xlga6q0008q4uxfjuh6b38","category_id":"cl4xlga76000mq4ux0ptt2z56","_id":"cl4xlga7e000vq4ux8btu3n41"},{"post_id":"cl4xlga6r0009q4ux270ib8fz","category_id":"cl4xlga7c000sq4uxdhd22kov","_id":"cl4xlga7g000zq4uxeb768l9f"},{"post_id":"cl4xlga6s000cq4uxh0vy08dp","category_id":"cl4xlga76000mq4ux0ptt2z56","_id":"cl4xlga7h0012q4ux528o6cyv"},{"post_id":"cl4xlga6t000dq4ux7tb487v4","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7i0017q4ux9dzm3a7b"},{"post_id":"cl4xlga6z000hq4uxafn90o86","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7j001aq4ux8za11yqr"},{"post_id":"cl4xlga72000jq4ux7xq4fllg","category_id":"cl4xlga7i0016q4uxh06bbuyv","_id":"cl4xlga7k001eq4ux1qbahc5a"},{"post_id":"cl4xlga77000oq4ux8dnb8802","category_id":"cl4xlga7j001bq4ux7dey6v00","_id":"cl4xlga7l001iq4ux5de14b5g"},{"post_id":"cl4xlga7a000qq4ux64mg6y51","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7m001mq4ux21m64pro"},{"post_id":"cl4xlga7w0020q4uxcn91gosx","category_id":"cl4xlga7g000yq4uxbx27ds6x","_id":"cl4xlga7x0022q4ux10sacm4j"}],"PostTag":[{"post_id":"cl4xlga6g0001q4uxdduzdc52","tag_id":"cl4xlga6n0005q4uxchwl2jgt","_id":"cl4xlga71000iq4ux9gwp7p5d"},{"post_id":"cl4xlga6g0001q4uxdduzdc52","tag_id":"cl4xlga6r000bq4ux9ndl1eob","_id":"cl4xlga75000kq4uxale95skx"},{"post_id":"cl4xlga6l0003q4uxaami25xn","tag_id":"cl4xlga6x000gq4ux48x4ccu2","_id":"cl4xlga79000pq4ux96yk06fi"},{"post_id":"cl4xlga6p0007q4ux5z0jezoz","tag_id":"cl4xlga76000nq4ux5p7hc675","_id":"cl4xlga7d000uq4ux2k3r8brf"},{"post_id":"cl4xlga6q0008q4uxfjuh6b38","tag_id":"cl4xlga7c000tq4ux519k162k","_id":"cl4xlga7h0011q4uxcv0lcf1e"},{"post_id":"cl4xlga6q0008q4uxfjuh6b38","tag_id":"cl4xlga7e000xq4uxgrvlc14f","_id":"cl4xlga7h0013q4uxbcf37pdc"},{"post_id":"cl4xlga6r0009q4ux270ib8fz","tag_id":"cl4xlga7g0010q4ux7xb8f7iz","_id":"cl4xlga7j0019q4uxfwgmcca1"},{"post_id":"cl4xlga6r0009q4ux270ib8fz","tag_id":"cl4xlga7h0015q4ux1g56b8g4","_id":"cl4xlga7j001cq4ux0br019sc"},{"post_id":"cl4xlga6s000cq4uxh0vy08dp","tag_id":"cl4xlga7j0018q4uxfj98304o","_id":"cl4xlga7l001hq4ux6pa09x0n"},{"post_id":"cl4xlga6s000cq4uxh0vy08dp","tag_id":"cl4xlga7j001dq4uxbune1x1u","_id":"cl4xlga7l001jq4uxh204bt82"},{"post_id":"cl4xlga6t000dq4ux7tb487v4","tag_id":"cl4xlga7k001gq4uxd6y4dxaa","_id":"cl4xlga7l001lq4uxhbhz87zk"},{"post_id":"cl4xlga6z000hq4uxafn90o86","tag_id":"cl4xlga7c000tq4ux519k162k","_id":"cl4xlga7m001oq4ux294tftt2"},{"post_id":"cl4xlga72000jq4ux7xq4fllg","tag_id":"cl4xlga7m001nq4uxggtugnif","_id":"cl4xlga7n001rq4ux2ndofp60"},{"post_id":"cl4xlga72000jq4ux7xq4fllg","tag_id":"cl4xlga76000nq4ux5p7hc675","_id":"cl4xlga7n001sq4ux7zfy0zzq"},{"post_id":"cl4xlga77000oq4ux8dnb8802","tag_id":"cl4xlga7n001qq4uxa2t0fcmo","_id":"cl4xlga7o001vq4uxcslyfdmf"},{"post_id":"cl4xlga77000oq4ux8dnb8802","tag_id":"cl4xlga7n001tq4uxfq304bvl","_id":"cl4xlga7o001wq4ux1hw8eo65"},{"post_id":"cl4xlga7a000qq4ux64mg6y51","tag_id":"cl4xlga7n001uq4ux6mgxa7rz","_id":"cl4xlga7s001yq4ux1ws56dqy"},{"post_id":"cl4xlga7a000qq4ux64mg6y51","tag_id":"cl4xlga7o001xq4ux3obr814n","_id":"cl4xlga7t001zq4uxbocu8gt9"},{"post_id":"cl4xlga7w0020q4uxcn91gosx","tag_id":"cl4xlga7x0021q4uxajtdgkxn","_id":"cl4xlga7x0023q4uxa8c974a4"}],"Tag":[{"name":"Information Extraction","_id":"cl4xlga6n0005q4uxchwl2jgt"},{"name":"SOTA","_id":"cl4xlga6r000bq4ux9ndl1eob"},{"name":"学习心得","_id":"cl4xlga6x000gq4ux48x4ccu2"},{"name":"Representation Learning","_id":"cl4xlga76000nq4ux5p7hc675"},{"name":"PLM","_id":"cl4xlga7c000tq4ux519k162k"},{"name":"cross-lingual","_id":"cl4xlga7e000xq4uxgrvlc14f"},{"name":"PCM","_id":"cl4xlga7g0010q4ux7xb8f7iz"},{"name":"Dialog","_id":"cl4xlga7h0015q4ux1g56b8g4"},{"name":"粤语","_id":"cl4xlga7j0018q4uxfj98304o"},{"name":"数据收集与清洗","_id":"cl4xlga7j001dq4uxbune1x1u"},{"name":"PM","_id":"cl4xlga7k001gq4uxd6y4dxaa"},{"name":"文本匹配","_id":"cl4xlga7m001nq4uxggtugnif"},{"name":"mention","_id":"cl4xlga7n001qq4uxa2t0fcmo"},{"name":"时间实体","_id":"cl4xlga7n001tq4uxfq304bvl"},{"name":"静态词向量","_id":"cl4xlga7n001uq4ux6mgxa7rz"},{"name":"负采样","_id":"cl4xlga7o001xq4ux3obr814n"},{"name":"BERT","_id":"cl4xlga7x0021q4uxajtdgkxn"}]}}