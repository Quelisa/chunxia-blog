---
title: 还有人不了解这些预训练模型？
date: 2021-11-15 16:40:46
tags:
- PM
categories: 
- 预训练语言模型
mathjax: true
---

自Google在2017年推出Transformer后，自然语言处理全面进入预训练大模型的时代，随之而来的BERT、GPT等大的预训练模型如雨后春笋般出现。带动了整个自然语言处理任务快速发展的同时，还影响了机器视觉和多模态技术的发展。本篇将介绍几个经典主流的预训练语言模型~


<!--more-->


### ELMo
是一种动态词向量预训练模型，它使用双向的网络结构进行建模，使用语言模型，因此是双向的。他是用C卷积神经网络对词进行embedding，主体模型使用LSTM。它在多个下游任务上表现达到了SOTA，启发了后来的预训练语言模型。


### BERT、RoBERTa、ALBERT、ELECTRA、XLNet、MacBERT
1. BETR采用了Transformer中encode部分。在输入中加入位置信息，使用了掩码语言模型（MLM）和预测下一个句子（NSP）进行预训练。因为MLM本身的建模就是双向的，所以不需要特殊的网络结构就可以实现双向的语义理解。
2. RoBERTa是对BERT中的方法进行更加充分的发掘，增加了训练预料，使用基于字节的token，增加batch和step，并去掉了NSP预训练任务，同时对MLM任务进行优化，针对同一个本文有不同种的掩码方案。
3. ALBERT是一个轻量级的BETR，主要是对词向量的维度进行压缩，所以模型的参数量会减少。之所以对词向量进行压缩是基于这样的假设，BERT中的词向量和输出向量的维度是一致的，但是在词级别时其实所包含的信息相对较少而且向量相对稀疏，而在上层的网络中会学习到更加复杂的语义表示，所以需要更大维度的向量表示。因为之前的试验认为NSP预训练任务过于简单，所以学习不到太多的语义，所以使用句子顺序预测任务，预测两个句子对的顺序。
4. ELECTRA使用了生成对抗的网络结构，训练时使用两个BERT，一个做生成器，一个做判别器，使用替换词检测任务（RTD）进行预训练，预测时只是用判别器，所以生成器的模型结构可以适度减少，大概在判别器的四分之一到二分之一之间。同时由于判别器的输入没有[MASK]字符，所以与下游任务保持一致。
5. XLNet使用了Transformer-XL作为主框架，相对传统的transformer拥有较好的性能，它使用了自回归语言模型结构，避免了[MASK]标记带来的上下游任务不一致，通过使用排列语言模型和双流自注意力实现了自编码语言模型中的双向上下文。
6. MacBERT中应用了一种基于文本纠错的掩码语言模型，使用整词掩码和同义词替换，并采用了句子顺序预测任务进行预训练。它很好地解决了BERT使用[MASK]掩码带来的“预训练-精调”不一致问题。



### DistilBERT、TinyBERT

这类模型主要是对预训练模型进行蒸馏。知识蒸馏是一种常用的知识迁移方法，通常由教师模型和学生模型组成，在用于压缩模型方面一般可以通过设置一个更小的学生模型对原始模型进行压缩。


1. DistilBERT是基于Triple Loss的知识蒸馏方法，教师模型使用BERT-base，学生模型是六层的BERT，使用BERT的前六层进行初始化，预训练使用MLM，损失函数是MLM损失、蒸馏损失和词向量余弦损失之和。

2. TinyBERT主要使用了额外的词向量层蒸馏和中间层蒸馏来进一步提升蒸馏的效果，并使用了两段式蒸馏进一步提升下游任务的效果。



### Transformer-XL、Reformer、Longformer、BigBird
Transformer因为采用自注意力机制使其空间和时间复杂度为$O(n^2)$，因此处理长文本的效率很低。虽然有一些处理长文本的trick，但是并不能解决问题本身。因此提出一些模型来解决上述问题：


1. Transformer-XL提出了两种改进策略：状态复用的块级别循环和相对位置编码。
2. Reformer引入了局部敏感哈希注意力和可逆Transformer技术，它可以帮助减少模型的内存占用，进而提高对长文本的处理能力。局部敏感哈希注意力的提出基于对Transformer中注意力机制观察的优化，首先通过实验可以看到单独计算查询和键与共享$QK$对整体性能没有太大的差异，同时全局注意力也没有必要，实际上注意力机制中更关心的是经过softmax函数激活的值，而激活的结果主要取决于数值较大的元素，因此在计算中只需要关注与当前查询关联度高的n个词，它可以极大地降低计算量。可以Transformer则是受到可以残差网络的启发，其主要思想是任意一层的激活值可以通过后续激活层进行还原。
3. Longformer将输入文本序列的最大长度扩充到4096，提出了三种稀疏注意力模式来降低计算复杂度，分别是滑动窗口注意力，扩张滑动窗口注意力和全局注意力。
4. BigBird使用了随机注意力、滑动窗口注意力和全局注意力三种稀疏注意力的方式对Transformer的长文本处理能力进一步进行优化，并通过理论分析证明了稀疏注意力的有效性。



### BART、UniLM、T5、GPT-3

前面的模型主要关注自然语言表示，而下面的模型更多的集中用于文本生成：

1. BART使用了标准的seq2seq tranformer结构，BART的预训练是在于破坏原文档然后优化重构loss。BART采用了多种方式破坏原文档，即采用了多种Noise。对于序列分类（文本分类）任务，encoder和decoder部分都用相同的输入，将deocoder最后一个节点用于多类别线性分类器中。对于序列标注任务，同样是在decoder和encoder采用相同的文本输入，以decoder的隐藏节点输出用于预测每个节点的类别。由于BART的模型框架本身就采用了自回归方式，因而在finetune序列生成任务时，可直接在encoder部分输入原始文本，decoder部分用于预测待生成的文本。BART预训练模型同样也可用于将其它语言翻译为英文。
2. UniLM通过引入attention mask实现了单向、双向和seq2seq语言模型，在双向LM模型中，没有任何mask，因为上下文信息都会被融入token中。在从左向右的单向LM模型中，矩阵上半三角会被mask掉，因为token只考虑左边的文本信息。在seq-to-seq模型中，句子s1的矩阵部分没有mask，s1因为s1融合上下文信息，句子s2矩阵的右上三角有mask，因为s2融合了s1的信息和左边的文本信息。使用完形填空和下一句预测作为预训练任务。
3. T5从另一个角度实现了NLU和NLG的统一建模，靠着大力出奇迹，采用了Prompt的训练方式，将所有NLP任务都转化成Text-to-Text（文本到文本）任务。
4. GPT-3是一种自回归模型，使用仅有解码器的体系结构。使用下一个单词预测目标进行训练。




### Vision Transformer
Vision Transformer可以是看做是CV领域对NLP领域的抄作业，直接将Transformer的结构拿来，使用embedding+encode+MLP head的网络结构，主要区别在于embedding层的不同。对于标准的Transformer模块，要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]，对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。



### UNITER、CLIP、UNIMO
而在Transformer之前，多模态领域并未出现行之有效的预训练模型，图像和文本都各玩各的，毕竟图像的预训练是从imagenet带标签的数据上进行的，而NLP更加牛，它能自回归，能自己mask自己然后让模型去预测。与CV一样多模态预训练模型也是对NLP的抄作业。
1. UNITER采用Transformer的encoder结构，主要是在embedding层有所不同，UNITER有两个Embedder，Image Embedder通过对Faster-RCNN的输出ROI feature以及其位置特征（7维，normalized top/left/bottom/right coordinates, width, height, and area.）进行融合建模，需要两个FC和一个LN操作完成。Text Embedder则参考BERT的输入，但是没有segment，然后直接接入Transformer进行双向建模，融合两种模态，从而达成目的，简单明了，不同于双流预训练模型，这两类模态共享同一个Encoder。预训练任务采用MLM，MRM(Mask Region Model, 预测图像，回归或分类，有三种变体)，ITM（Image Text Match, 图文是否一致）和WRA（Word Region Alignment, 字和图像的对齐任务）。
2. CLIP用到了零样本学习（zero-shot learning）、自然语言理解和多模态学习等技术，来完成图像的理解。
3. UNIMO采用类似的掩码预测自监督方法学习图像和文本的表示。同时，为了将文本和图像的表示映射到统一的语义空间，论文提出跨模态对比学习，基于图文对数据实现图像与文本的统一表示学习。

